[{"title":"Nginx配置文件详解","url":"/2023/12/12/nginx-configuration-files-explaination/","content":"\nNginx配置文件 nginx.conf 和default.conf 讲解\n\n<!--more-->\n\n**nginx.conf**\n\n/etc/nginx/nginx.conf\n\n```\n######Nginx配置文件nginx.conf中文详解#####\n \n#定义Nginx运行的用户和用户组\nuser www www;\n \n#nginx进程数，建议设置为等于CPU总核心数。\nworker_processes 8;\n  \n#全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]\nerror_log /usr/local/nginx/logs/error.log info;\n \n#进程pid文件\npid /usr/local/nginx/logs/nginx.pid;\n \n#指定进程可以打开的最大描述符：数目\n#工作模式与连接数上限\n#这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。\n#现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。\n#这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。\nworker_rlimit_nofile 65535;\n \nevents\n{\n    #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型\n    #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。\n    #补充说明：\n    #与apache相类，nginx针对不同的操作系统，有不同的事件模型\n    #A）标准事件模型\n    #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll\n    #B）高效事件模型\n    #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。\n    #Epoll：使用于Linux内核2.6版本及以后的系统。\n    #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。\n    #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。\n    use epoll;\n \n    #单个进程最大连接数（最大连接数=连接数*进程数）\n    #根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。\n    worker_connections 65535;\n \n    #keepalive超时时间。\n    keepalive_timeout 60;\n \n    #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。\n    #分页大小可以用命令getconf PAGESIZE 取得。\n    #[root@web001 ~]# getconf PAGESIZE\n    #4096\n    #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。\n    client_header_buffer_size 4k;\n \n    #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。\n    open_file_cache max=65535 inactive=60s;\n \n    #这个是指多长时间检查一次缓存的有效信息。\n    #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息.\n    open_file_cache_valid 80s;\n \n    #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。\n    #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location  这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态.\n    open_file_cache_min_uses 1;\n     \n    #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件是记录cache错误.\n    open_file_cache_errors on;\n}\n  \n  \n  \n#设定http服务器，利用它的反向代理功能提供负载均衡支持\nhttp\n{\n    #文件扩展名与文件类型映射表\n    include /etc/nginx/mime.types;\n \n    #默认文件类型\n    default_type application/octet-stream;\n \n    #默认编码\n    #charset utf-8;\n \n    #服务器名字的hash表大小\n    #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小.\n    server_names_hash_bucket_size 128;\n \n    #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。\n    client_header_buffer_size 32k;\n \n    #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。\n    large_client_header_buffers 4 64k;\n \n    #设定通过nginx上传文件的大小\n    client_max_body_size 8m;\n \n    #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。\n    #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。\n    sendfile on;\n \n    #开启目录列表访问，合适下载服务器，默认关闭。\n    autoindex on;\n \n    #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用\n    tcp_nopush on;\n      \n    tcp_nodelay on;\n \n    #长连接超时时间，单位是秒\n    keepalive_timeout 120;\n \n    #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。\n    fastcgi_connect_timeout 300;\n    fastcgi_send_timeout 300;\n    fastcgi_read_timeout 300;\n    fastcgi_buffer_size 64k;\n    fastcgi_buffers 4 64k;\n    fastcgi_busy_buffers_size 128k;\n    fastcgi_temp_file_write_size 128k;\n \n    #gzip模块设置\n    gzip on; #开启gzip压缩输出\n    gzip_min_length 1k;    #最小压缩文件大小\n    gzip_buffers 4 16k;    #压缩缓冲区\n    gzip_http_version 1.0;    #压缩版本（默认1.1，前端如果是squid2.5请使用1.0）\n    gzip_comp_level 2;    #压缩等级\n    gzip_types text/plain application/x-javascript text/css application/xml;    #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。\n    gzip_vary on;\n \n    #开启限制IP连接数的时候需要使用\n    #limit_zone crawler $binary_remote_addr 10m;\n \n \n \n    #负载均衡配置\n    upstream piao.jd.com {\n      \n        #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。\n        server 192.168.80.121:80 weight=3;\n        server 192.168.80.122:80 weight=2;\n        server 192.168.80.123:80 weight=3;\n \n        #nginx的upstream目前支持4种方式的分配\n        #1、轮询（默认）\n        #每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。\n        #2、weight\n        #指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。\n        #例如：\n        #upstream bakend {\n        #    server 192.168.0.14 weight=10;\n        #    server 192.168.0.15 weight=10;\n        #}\n        #2、ip_hash\n        #每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。\n        #例如：\n        #upstream bakend {\n        #    ip_hash;\n        #    server 192.168.0.14:88;\n        #    server 192.168.0.15:80;\n        #}\n        #3、fair（第三方）\n        #按后端服务器的响应时间来分配请求，响应时间短的优先分配。\n        #upstream backend {\n        #    server server1;\n        #    server server2;\n        #    fair;\n        #}\n        #4、url_hash（第三方）\n        #按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。\n        #例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法\n        #upstream backend {\n        #    server squid1:3128;\n        #    server squid2:3128;\n        #    hash $request_uri;\n        #    hash_method crc32;\n        #}\n \n        #tips:\n        #upstream bakend{#定义负载均衡设备的Ip及设备状态}{\n        #    ip_hash;\n        #    server 127.0.0.1:9090 down;\n        #    server 127.0.0.1:8080 weight=2;\n        #    server 127.0.0.1:6060;\n        #    server 127.0.0.1:7070 backup;\n        #}\n        #在需要使用负载均衡的server中增加 proxy_pass http://bakend/;\n \n        #每个设备的状态设置为:\n        #1.down表示单前的server暂时不参与负载\n        #2.weight为weight越大，负载的权重就越大。\n        #3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误\n        #4.fail_timeout:max_fails次失败后，暂停的时间。\n        #5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。\n \n        #nginx支持同时设置多组的负载均衡，用来给不用的server来使用。\n        #client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug\n        #client_body_temp_path设置记录文件的目录 可以设置最多3层目录\n        #location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡\n    }\n```\n\n　　\n\n**default.conf**\n\n/etc/nginx/conf.d/default.conf\n\n```\nserver\n    {\n        #监听端口\n        listen 80;\n \n        #域名可以有多个，用空格隔开\n        server_name www.jd.com jd.com;\n        index index.html index.htm index.php;\n        root /data/www/jd;\n \n        #对******进行负载均衡\n        location ~ .*.(php|php5)?$\n        {\n            fastcgi_pass 127.0.0.1:9000;\n            fastcgi_index index.php;\n            include fastcgi.conf;\n        }\n          \n        #图片缓存时间设置\n        location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$\n        {\n            expires 10d;\n        }\n          \n        #JS和CSS缓存时间设置\n        location ~ .*.(js|css)?$\n        {\n            expires 1h;\n        }\n          \n        #日志格式设定\n        #$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址；\n        #$remote_user：用来记录客户端用户名称；\n        #$time_local： 用来记录访问时间与时区；\n        #$request： 用来记录请求的url与http协议；\n        #$status： 用来记录请求状态；成功是200，\n        #$body_bytes_sent ：记录发送给客户端文件主体内容大小；\n        #$http_referer：用来记录从那个页面链接访问过来的；\n        #$http_user_agent：记录客户浏览器的相关信息；\n        #通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。\n        log_format access '$remote_addr - $remote_user [$time_local] \"$request\" '\n        '$status $body_bytes_sent \"$http_referer\" '\n        '\"$http_user_agent\" $http_x_forwarded_for';\n          \n        #定义本虚拟主机的访问日志\n        access_log  /usr/local/nginx/logs/host.access.log  main;\n        access_log  /usr/local/nginx/logs/host.access.404.log  log404;\n          \n        #对 \"/\" 启用反向代理\n        location / {\n            proxy_pass http://127.0.0.1:88;\n            proxy_redirect off;\n            proxy_set_header X-Real-IP $remote_addr;\n              \n            #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n              \n            #以下是一些反向代理的配置，可选。\n            proxy_set_header Host $host;\n \n            #允许客户端请求的最大单文件字节数\n            client_max_body_size 10m;\n \n            #缓冲区代理缓冲用户端请求的最大字节数，\n            #如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。\n            #无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误\n            client_body_buffer_size 128k;\n \n            #表示使nginx阻止HTTP应答代码为400或者更高的应答。\n            proxy_intercept_errors on;\n \n            #后端服务器连接的超时时间_发起握手等候响应超时时间\n            #nginx跟后端服务器连接超时时间(代理连接超时)\n            proxy_connect_timeout 90;\n \n            #后端服务器数据回传时间(代理发送超时)\n            #后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据\n            proxy_send_timeout 90;\n \n            #连接成功后，后端服务器响应时间(代理接收超时)\n            #连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间）\n            proxy_read_timeout 90;\n \n            #设置代理服务器（nginx）保存用户头信息的缓冲区大小\n            #设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小\n            proxy_buffer_size 4k;\n \n            #proxy_buffers缓冲区，网页平均在32k以下的设置\n            #设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k\n            proxy_buffers 4 32k;\n \n            #高负荷下缓冲大小（proxy_buffers*2）\n            proxy_busy_buffers_size 64k;\n \n            #设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长\n            #设定缓存文件夹大小，大于这个值，将从upstream服务器传\n            proxy_temp_file_write_size 64k;\n        }\n          \n          \n        #设定查看Nginx状态的地址\n        location /NginxStatus {\n            stub_status on;\n            access_log on;\n            auth_basic \"NginxStatus\";\n            auth_basic_user_file confpasswd;\n            #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。\n        }\n          \n        #本地动静分离反向代理配置\n        #所有jsp的页面均交由tomcat或resin处理\n        location ~ .(jsp|jspx|do)?$ {\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_pass http://127.0.0.1:8080;\n        }\n          \n        #所有静态文件由nginx直接读取不经过tomcat或resin\n        location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt|\n        pdf|xls|mp3|wma)$\n        {\n            expires 15d;\n        }\n          \n        location ~ .*.(js|css)?$\n        {\n            expires 1h;\n        }\n    }\n}\n```\n\n　　\n\n```csharp\n######Nginx配置文件nginx.conf中文详解#####\n\n#定义Nginx运行的用户和用户组\nuser www www;\n\n#nginx进程数，建议设置为等于CPU总核心数。\nworker_processes 8;\n \n#全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]\nerror_log /usr/local/nginx/logs/error.log info;\n\n#进程pid文件\npid /usr/local/nginx/logs/nginx.pid;\n\n#指定进程可以打开的最大描述符：数目\n#工作模式与连接数上限\n#这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。\n#现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。\n#这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。\nworker_rlimit_nofile 65535;\n\nevents\n{\n    #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型\n    #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。\n    #补充说明：\n    #与apache相类，nginx针对不同的操作系统，有不同的事件模型\n    #A）标准事件模型\n    #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll\n    #B）高效事件模型\n    #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。\n    #Epoll：使用于Linux内核2.6版本及以后的系统。\n    #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。\n    #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。\n    use epoll;\n\n    #单个进程最大连接数（最大连接数=连接数*进程数）\n    #根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。\n    worker_connections 65535;\n\n    #keepalive超时时间。\n    keepalive_timeout 60;\n\n    #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。\n    #分页大小可以用命令getconf PAGESIZE 取得。\n    #[root@web001 ~]# getconf PAGESIZE\n    #4096\n    #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。\n    client_header_buffer_size 4k;\n\n    #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。\n    open_file_cache max=65535 inactive=60s;\n\n    #这个是指多长时间检查一次缓存的有效信息。\n    #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息.\n    open_file_cache_valid 80s;\n\n    #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。\n    #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location  这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态.\n    open_file_cache_min_uses 1;\n    \n    #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件是记录cache错误.\n    open_file_cache_errors on;\n}\n \n \n \n#设定http服务器，利用它的反向代理功能提供负载均衡支持\nhttp\n{\n    #文件扩展名与文件类型映射表\n    include /etc/nginx/mime.types;\n\n    #默认文件类型\n    default_type application/octet-stream;\n\n    #默认编码\n    #charset utf-8;\n\n    #服务器名字的hash表大小\n    #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小.\n    server_names_hash_bucket_size 128;\n\n    #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。\n    client_header_buffer_size 32k;\n\n    #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。\n    large_client_header_buffers 4 64k;\n\n    #设定通过nginx上传文件的大小\n    client_max_body_size 8m;\n\n    #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。\n    #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。\n    sendfile on;\n\n    #开启目录列表访问，合适下载服务器，默认关闭。\n    autoindex on;\n\n    #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用\n    tcp_nopush on;\n     \n    tcp_nodelay on;\n\n    #长连接超时时间，单位是秒\n    keepalive_timeout 120;\n\n    #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。\n    fastcgi_connect_timeout 300;\n    fastcgi_send_timeout 300;\n    fastcgi_read_timeout 300;\n    fastcgi_buffer_size 64k;\n    fastcgi_buffers 4 64k;\n    fastcgi_busy_buffers_size 128k;\n    fastcgi_temp_file_write_size 128k;\n\n    #gzip模块设置\n    gzip on; #开启gzip压缩输出\n    gzip_min_length 1k;    #最小压缩文件大小\n    gzip_buffers 4 16k;    #压缩缓冲区\n    gzip_http_version 1.0;    #压缩版本（默认1.1，前端如果是squid2.5请使用1.0）\n    gzip_comp_level 2;    #压缩等级\n    gzip_types text/plain application/x-javascript text/css application/xml;    #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。\n    gzip_vary on;\n\n    #开启限制IP连接数的时候需要使用\n    #limit_zone crawler $binary_remote_addr 10m;\n\n\n\n    #负载均衡配置\n    upstream piao.jd.com {\n     \n        #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。\n        server 192.168.80.121:80 weight=3;\n        server 192.168.80.122:80 weight=2;\n        server 192.168.80.123:80 weight=3;\n\n        #nginx的upstream目前支持4种方式的分配\n        #1、轮询（默认）\n        #每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。\n        #2、weight\n        #指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。\n        #例如：\n        #upstream bakend {\n        #    server 192.168.0.14 weight=10;\n        #    server 192.168.0.15 weight=10;\n        #}\n        #2、ip_hash\n        #每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。\n        #例如：\n        #upstream bakend {\n        #    ip_hash;\n        #    server 192.168.0.14:88;\n        #    server 192.168.0.15:80;\n        #}\n        #3、fair（第三方）\n        #按后端服务器的响应时间来分配请求，响应时间短的优先分配。\n        #upstream backend {\n        #    server server1;\n        #    server server2;\n        #    fair;\n        #}\n        #4、url_hash（第三方）\n        #按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。\n        #例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法\n        #upstream backend {\n        #    server squid1:3128;\n        #    server squid2:3128;\n        #    hash $request_uri;\n        #    hash_method crc32;\n        #}\n\n        #tips:\n        #upstream bakend{#定义负载均衡设备的Ip及设备状态}{\n        #    ip_hash;\n        #    server 127.0.0.1:9090 down;\n        #    server 127.0.0.1:8080 weight=2;\n        #    server 127.0.0.1:6060;\n        #    server 127.0.0.1:7070 backup;\n        #}\n        #在需要使用负载均衡的server中增加 proxy_pass http://bakend/;\n\n        #每个设备的状态设置为:\n        #1.down表示单前的server暂时不参与负载\n        #2.weight为weight越大，负载的权重就越大。\n        #3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误\n        #4.fail_timeout:max_fails次失败后，暂停的时间。\n        #5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。\n\n        #nginx支持同时设置多组的负载均衡，用来给不用的server来使用。\n        #client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug\n        #client_body_temp_path设置记录文件的目录 可以设置最多3层目录\n        #location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡\n    }\n```\n","tags":["Nginx"],"categories":["Nginx"]},{"title":"二分查找总结","url":"/2023/11/28/binary-search-summary/","content":"\n二分查找-红蓝染色法总结\n\n<!--more-->\n\n## 二分查找-红蓝染色法\n\nL, R分别指向数组的左右边界及闭区间[L,R],红色背景表示不满足条件，蓝色表示满足条件，白色表示不确定\n\n### 开闭区间问题\n\n闭区间（常用）：[0, nums.length - 1]  退出二分查找需要left <= right, 移动区间时 left = mid + 1, right = mid - 1,这样才不会出现死循环。\n\n左闭右开：[0, nums.length) 退出二分查找需要left < right, 移动区间时 left = mid + 1, right = mid\n\n开区间：(-1, nums.length) 退出二分查找需要left+1 < right, 移动区间时 left = mid, right = mid\n\n### [34.在排序数组中查找元素的第一个和最后一个位置](https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/)\n\n思想：使用二分查找去查找target的第一个位置，如果第一个满足位置是数组长度说明整个数组都是红色，或者返回的值不等于target，不符合条件。再使用二分查找去找target+1的前一个位置。\n\n二分查找（红蓝染色）的思想：利用数组有序的性质，从左右两端开始快速查找符合的target范围，如果nums[mid]小于target，left=mid+1，从右侧开始查找，反之亦然，找到符合条件的第一个target位置\n\n```java\nclass Solution {\n    public int[] searchRange(int[] nums, int target) {\n        int start = lowerBound(nums, target);\n        if(start == nums.length || nums[start] != target) {\n            return new int[]{-1, -1};\n        }\n        int end = lowerBound(nums, target+1);\n        return new int[]{start, end -1};\n\n    }\n\n    private int lowerBound(int[] nums, int target) {\n        int left = 0, right = nums.length-1;\n        int mid = 0;\n        while(left <= right) {\n            mid = left + (right - left) / 2;\n            if(nums[mid] < target) {\n                left = mid+1;\n            }else {\n                right = mid - 1;\n            }\n        }\n        return left;\n    }\n}\n```\n\n#### 非>= k 问题如何转化(在有序数组问题中)\n\n\">k\"可以看作是>= k+1\n\n\"<k\"可以看作 (>=k) -1,即找到大于等于k的位置的前一个位置\n\n\"<=k\"可以看作(>k) -1,即找到大于k的位置的前一个位置\n\n### [162.寻找峰值](https://leetcode.cn/problems/find-peak-element/)\n\n蓝色是峰值及其右侧，right = nums.length - 1, nums[right]一定是蓝色，预留这个位置\n\n每次二分确定一半的颜色\n\n简单来说就是当有nums[mid] > nums[mid + 1]就能确定是一个下坡，至少有一个峰值，如果nums[mid]之前是上坡，那么nums[mid]就是峰值，nums[mid]之前是下坡，也能确定一个峰值\n\n```java\nclass Solution {\n    public int findPeakElement(int[] nums) {\n        int left = -1, right = nums.length - 1; // 开区间 (-1, n-1)\n        while (left + 1 < right) { // 开区间不为空\n            int mid = left + (right - left) / 2;\n            if (nums[mid] > nums[mid + 1]) right = mid; // 蓝色\n            else left = mid; // 红色\n        }\n        return right;\n    }\n}\n```\n\n### [153. 寻找旋转排序数组中的最小值](https://leetcode.cn/problems/find-minimum-in-rotated-sorted-array/)\n\nred:最小值左侧\n\nblue:最小值及其右侧\n\n5 6 7 8 2 3 4\n\nnum[right]一定是蓝色，大于等于num[right]肯定要舍弃染成红色，最小值肯定小于num[right]，选择左侧区域寻找\n\n```java\nclass Solution {\n    public int findMin(int[] nums) {\n        int left = -1, right = nums.length - 1;\n        while(left+1 < right) {\n            int mid = left + (right - left) / 2;\n            if(nums[mid] < nums[right]){\n                right = mid;\n            }else {\n                left = mid;\n            }\n        }\n        return nums[right];\n    }\n}\n```\n\n### [154. 寻找旋转排序数组中的最小值 II](https://leetcode.cn/problems/find-minimum-in-rotated-sorted-array-ii/)\n\n本题与 153 的区别在于有相同元素，这会导致在二分查找时，可能会遇到「恰好」二分元素与数组末尾元素相同的情况，此时无法确定答案在左半区间还是右半区间。\n\n我们接着 153 题解 的代码来实现。本题需要稍加修改，改为与区间右端点处的元素比较（如果你写的是闭区间的话，那就是与右端点 +1 的元素比较）。\n\n在遇到相同元素时，既然无法确定最小值所在区间，那么干脆去掉末尾元素，继续二分。对应到代码上，就是 right 减一\n\n你可能会有疑问：这样做不会碰巧把最小值给去掉吗？这是不会的：\n\n- 如果末尾元素就是最小值，那么nums[mid] 也是最小值，说明最小值仍然在二分区间中；\n- 如果末尾元素不是最小值，这样做相当于排除了一个错误答案。\n\n```java\nclass Solution {\n    public int findMin(int[] nums) {\n        int left = -1, right = nums.length - 1; // 开区间 (-1, n-1)\n        while (left + 1 < right) { // 开区间不为空\n            int mid = (left + right) >>> 1;\n            if (nums[mid] < nums[right]) right = mid; // 蓝色\n            else if (nums[mid] > nums[right]) left = mid; // 红色\n            else --right;\n        }\n        return nums[right];\n    }\n}\n```\n\n### [33. 搜索旋转排序数组](https://leetcode.cn/problems/search-in-rotated-sorted-array/)\n\nred:目标target左侧\n\nblue:目标target及其右侧\n\n![image-20231128164541981](binary-search-summary/image-20231128164541981.png)\n\n当nums[mid]>end时，在左侧：\n\n- nums[mid ] > target 可以缩小蓝色区间， right=mid\n\n当nums[mid]<end时，在右侧：\n\n- 当nums[mid]已经在右侧，target>end说明target还在左侧，即可缩小蓝色区间\n- 当nums[mid] > target,与第一种情况同理\n\n```java\nclass Solution {\n    public int search(int[] nums, int target) {\n        int left = -1, right = nums.length - 1; // 开区间 (-1, n-1)\n        while (left + 1 < right) { // 开区间不为空\n            int mid = left + (right - left) / 2;\n            if (isBlue(nums, target, mid)) right = mid; // 蓝色\n            else left = mid; // 红色\n        }\n        return nums[right] == target ? right : -1;\n    }\n\n    private boolean isBlue(int[] nums, int target, int mid) {\n        int end = nums[nums.length - 1];\n        if (nums[mid] > end) {\n            return target > end && nums[i] >= target;\n        }\n        return target > end || nums[i] >= target;\n    }\n}\n```\n\n","tags":["Leetcode"],"categories":["Leetcode"]},{"title":"RocketMQ消息丢失，消息一致性，重复消费解决方案","url":"/2023/11/26/RocketMQ-message-loss-message-consistency-and-repeated-consumption-solutions/","content":"\n产生重复消费的场景和解决方案\n\n<!--more-->\n\n## 本章概括\n\n![img](RocketMQ-message-loss-message-consistency-and-repeated-consumption-solutions/6248a89027f86abb2aecf881.jpg)\n\n## 分布式事务\n\n### 由何而来\n\n我们在使用MQ在解决实际业务场景中的问题时，往往伴随诸多问题！比如如下图\n\n![img](RocketMQ-message-loss-message-consistency-and-repeated-consumption-solutions/624890bb27f86abb2ad47e14.jpg)\n\n上述两种可能都会导致数据不一致，在业务系统中是 **致命的问题**！\n\n这个时候我们就要保证事务消息。要不全部成功，要不全部失败。来达到订单服务，购物车服务的数据一致性！\n\n对于购物车服务收到订单创建成功消息清理购物车这个操作来说，失败的处理比较简单，只要成功执行购物车清理后再提交消费确认即可，如果失败，由于没有提交消费确认，消息队列会自动重试。\n\n解决了购物车服务问题，剩下的就是订单服务这边的创建订单，生产消息这两步了。要么全部成功，要么全部失败，不允许一个成功，一个失败的情况。\n\n> 一旦订单控制不住，购物车那边也是控制不住的！ **这就是事务需要解决的问题了！**\n\n### 什么是分布式事务\n\n事务就是为了保证这些数据的完整性和一致性，我们希望这些更新操作要么全部成功，要么全部失败。这就是我们通过对事务的理解。如果严格来说，MQ的事务和MySQL一样，都具有四种属性 **ACID**\n\n1. **原子性**：一个事务操作不可分割，要么成功，要么失败，不能有一半成功一半失败的情 况\n2. **一致性**：这些数据在事务执行完成这个时间点之前，读到的一定是更新前的数据，之后 读到的一定是更新后的数据，不应该存在一个时刻，让用户读到更新过程中的数据\n3. **隔离性**：指一个事务的执行不能被其他事务干扰。即一个事务内部的操作及使用的数据对 正在进行的其他事务是隔离的，并发执行的各个事务之间不能互相干扰\n4. **持久性**：指一个事务一旦完成提交，后续的其他操作和故障都不会对事务的结果产生任何 影响\n\n对于单体服务来说，都实现了ACID，但是对于分布式系统来说，实现ACID这几乎是不可能的，或者说代价太大。所有目前大家所说的分布式事务，更多的情况下，是一种分布式事务的不完整实现。不同的应用场景中，有不同的实现，目的都是通过一些妥协来解决实际问题。\n\n> 比较常见的分布式事务有\n>\n> - 2PC（Two-phase Commit，也叫二阶段提 交）\n> - TCC(Try-Confirm-Cancel)\n> - 和事务消息\n\n事务消息适用的场景主要是那些需要异步更新数据，并且对数据实时性要求不太高的场景。 比如我们在开始时提到的那个例子，在创建订单后，如果出现短暂的几秒，购物车里的商品没有被及时清空，也不是完全不可接受的，只要最终购物车的数据和订单数据保持一致就可以了。\n\n剩下的就不做过多解释了。\n\n### MQ是如何实现的\n\nMQ主要借助的是 **半消息** 实现的，如下图\n\n![img](RocketMQ-message-loss-message-consistency-and-repeated-consumption-solutions/624895fa27f86abb2adac36d.jpg)\n\n1. 订单服务首先会开启一个事务，就类似于MySQL那样。\n2. 对MQ生产一个半消息\n3. 以上都没有问题之后，就会执行事务，写入数据库\n4. 提交事务或回滚事务\n\n> 这里的半消息，并不是只有一半的数据。而是有全部的数据，这里的半只是 **在事务提交之前，对于消费者来说，这个消息是不可见的**\n\n到了这里，订单服务肯定是没有问题的，所以把数据写入到MQ的Broker之后\n\n> 这里回顾一下生产端的交互流程，**可以参考下列图片，理解**\n>\n> 1. 订单服务会向MQ的Broker发送一个ACK包\n> 2. 如果Broker确认收到了，会给订单服务回一个ACK+SYN包 （如果Broker没有收到，会开始重传）\n> 3. 如果Broker收到了，一定可以确保订单服务的数据执行完成，以及确保数据已经到Broker了。\n\n到了这里，订单服务，Broker端是没有问题的，把数据写入Broker之后，购物车服务就会开始进行消费这条消息\n\n> 这里回顾一下消费端的交互流程，**可以参考下列图片，理解**\n>\n> 1. 购物车服务在监听收到消息后进行消费\n> 2. 当购物车服务执行了当前的逻辑之后，会给Broker发送一个 ACK+SYN包确认消费\n> 3. 如果购物车服务没有给Broker回复，那么Broker就会开始重发\n\n![img](RocketMQ-message-loss-message-consistency-and-repeated-consumption-solutions/62471cb127f86abb2a5188df.jpg)\n\n到了这里，订单服务，Broker端，购物车服务基本实现了 **要么成功，要么失败** 的一致性要求。\n\n天网恢恢疏而不漏，在第四步的时候提交事务，如果失败了怎么办？\n\n> Kafka 的解决方案比较简单粗暴，直接抛出异常，让用户自行处理。我们可以在业务代码中 反复重试提交，直到提交成功，或者删除之前创建的订单进行补偿\n\n### RocketMQ是如何实现的\n\n这里RocketMQ也给出了相应的应对策略！在事务实现中，他加了 **事务反查的机制** 来解决事务的提交失败问题。\n\n如果订单服务，在提交或者回滚事务消息时发生网络异常，RocketMQ 的 Broker 没有收到提交或者回滚的请求，Broker 会定期去订单服务上反查这个事务对应的本地事务的状态，然后根据反查结果决定提交或者回滚这个事务。\n\n> 为了支撑这个机制，我们需要做一个反查本地事务状态的接口，告知RocketMQ本地事务是否成功。\n>\n> 例如 只需要根据消息中的订单ID，检查这个订单是否创建成功即可\n\n这个反查本地事务的实现，并不依赖订单服务的某个实例节点上的任何数据。这种情况下，即使是发送事务消息的那个订单服务节点宕机了，RocketMQ 依然 可以通过其他订单服务的节点来执行反查，确保事务的完整性\n\n![img](RocketMQ-message-loss-message-consistency-and-repeated-consumption-solutions/62489dd327f86abb2ae364cb.jpg)\n\n## 确保消息不会丢失\n\n聊到消息一致性，可靠性传输，我们可以从问题的根源入手。我先列举一些容易出问题的故障点\n\n- **生产阶段：**在这个阶段，从消息在 Producer 创建出来，经过网络传输发送到 Broker 端。\n- **存储阶段：**在这个阶段，消息在 Broker 端存储，如果是集群，消息会在这个阶段被复制到其他的副本上。\n- **消费阶段：**在这个阶段，Consumer 从 Broker 上拉取消息，经过网络传输发送到 Consumer 上。\n\n![img](RocketMQ-message-loss-message-consistency-and-repeated-consumption-solutions/6240250e27f86abb2aa537ce.jpg)\n\n#### 生产阶段\n\n在生产阶段，消息队列通过最常用的请求确认机制，来保证消息的可靠传递：当你的代码调用发消息方法时，消息队列的客户端会把消息发送到 Broker，Broker 收到消息后，会给客户端返回一个确认响应，表明消息已经收到了。客户端收到响应后，完成了一次正常消息的发送。\n\n**只要 Producer 收到了 Broker 的确认响应，就可以保证消息在生产阶段不会丢失**。有些消息队列在长时间没收到发送确认响应后，会自动重试，如果重试再失败，就会以返回值或者异常的方式告知用户。\n\n**你在编写发送消息代码时，需要注意，正确处理返回值或者捕获异常，就可以保证这个阶段的消息不会丢失**\n\n#### 存储阶段\n\n在存储阶段正常情况下，只要 Broker 在正常运行，就不会出现丢失消息的问题，但是如果 Broker 出现了故障，比如进程死掉了或者服务器宕机了，还是可能会丢失消息的。\n\n**如果对消息的可靠性要求非常高，可以通过配置 Broker 参数来避免因为宕机丢消息。**\n\n对于单个节点的 Broker，需要配置 Broker 参数，在收到消息后，将消息写入磁盘后再给 Producer 返回确认响应，这样即使发生宕机，由于消息已经被写入磁盘，就不会丢失消息，恢复后还可以继续消费。例如，在 RocketMQ 中，需要将刷盘方式 flushDiskType 配置为 SYNC_FLUSH 同步刷盘。\n\n集群我不会，后续再更新。\n\n#### 消费阶段\n\n消费阶段采用和生产阶段类似的确认机制来保证消息的可靠传递，客户端从 Broker 拉取消息后，执行用户的消费业务逻辑，**成功后，才会给 Broker 发送消费确认响应**。如果 Broker 没有收到消费确认响应，下次拉消息的时候还会返回同一条消息，确保消息不会在网络传输过程中丢失，也不会因为客户端在执行消费逻辑中出错导致丢失。\n\n你在编写消费代码时需要注意的是，**不要在收到消息后就立即发送消费确认，而是应该在执行完所有消费业务逻辑之后，再发送消费确认。**\n\n#### 消息丢失检测\n\n前期代码健壮性不友好的情况，可以在拦截器里编写日志输出，把消费的id号记录下来。\n\n- 生产者，生产一条就记录一条\n- 消费者，消费一条就记录一条\n\n这样这样两边对照就可以把丢失的id号  定位出来。也可以通过分布式链路追踪系统 **扯远了，以后再说吧**\n\n## 确保消息不被重复消费\n\n### 为什么会有重复消息\n\n在消息传递过程中，如果出现传递失败的情况，发送方会执行重试，重试的过程中就有可能 会产生重复的消息。对使用消息队列的业务系统来说，如果没有对重复消息进行处理，就有可能会导致系统的数据出现错误。\n\n**所以重复消费的情况必然存在**\n\n在MQTT协议中，大概提供了三种标准\n\n1. **At most once: 至多一次**。消息在传递时，最多会被送达一次。换一个说法就是，没什 么消息可靠性保证，允许丢消息。一般都是一些对消息可靠性要求不太高的监控场景使 用，比如每分钟上报一次机房温度数据，可以接受数据少量丢失。\n2. **At least once: 至少一次**。消息在传递时，至少会被送达一次。也就是说，不允许丢消 息，但是允许有少量重复消息出现。\n3. **Exactly once：恰好一次**。消息在传递时，只会被送达一次，不允许丢失也不允许重 复，这个是最高的等级。\n\n> 大多数的消息队列，都是采用的 **At least once: 至少一次**\n>\n> 根据上面介绍，我们可以得知 **消息队列很难保证消息不重复**\n\n既然消息队列，无法保证重复消费的问题，那我们就要在程序里解决这个问题了。\n\n### 如何解决重复消费（幂等性）\n\n> 幂等性是一个数学上的概念，它是这样定义的：如果一个函数 f(x) 满足：f(f(x)) = f(x)，则函数 f(x) 满足幂等性。\n>\n> 这里被扩展到计算机领域，**被广泛的应用于多次执行产生的影响均与一次执行的影响相同**\n>\n> 使用同样的参数，对它进行多次调用和一次调用，对系统产生的影响是一样的。所以，对于幂等的方法，不用担心重复执行会对系统造成任何改变。\n>\n> （这里可以联想到 用户充值，多次消费充值的话，肯定是有问题的！）\n\n如果说MQ解决不了数据重复消费的问题，那么现在可以转化成 **At least once + 幂等性 = Exactly once**  这样就可以保证重复消费了。主要有下列三种方法\n\n- 数据库的唯一约束实现幂等\n- 为更新的数据设置前置条件\n- 记录并检查操作\n\n#### 数据库的唯一约束实现幂等\n\n我先举一个我自己系统的例子：用户在充值账号余额时，会产生一个账单ID。\n\n我们在实现唯一约束的时候就可以重新创建一个表。伪代码如下\n\n```mysql\ncreate table aaa(\n    id  bigint(15) not null comment '约束id',\n    user_id bigint(15) not null comment '用户id',\n    bill_id bigint(15) not null comment '账单id',\n    money decimal(10,2) not null comment '充值金额',\n    PRIMARY KEY (`id`) USING BTREE,\n    KEY `adasdasdas` (`user_id`,`bill_id`),  -- 唯一约束  用户di和账单id\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='账单约束表';\n```\n\n![img](RocketMQ-message-loss-message-consistency-and-repeated-consumption-solutions/624030cf27f86abb2aea6496.jpg)\n\n这样，我们消费消息的逻辑可以变为：“在转账流水表中增加一条转账记录，然后再根据转账记录，异步操作更新用户余额即可。”在转账流水表增加一条转账记录这个操作中，由于我们在这个表中预先定义了“账户 ID 转账单 ID”的唯一约束，对于同一个转账单同一个账户只能插入一条记录，后续重复的插入操作都会失败，这样就实现了一个幂等的操作。我们只要写一个 SQL，正确地实现它就可以了。\n\n基于这个思路，不光是可以使用关系型数据库，只要是支持类似“INSERT IF NOT EXIST”语义的存储类系统都可以用于实现幂等，比如，你可以用 Redis 的 SETNX 命令来替代数据库中的唯一约束，来实现幂等消费。\n\n> 参考李玥老师的 消息队列高手课 思想\n\n#### 为更新的数据设置前置条件\n\n在更新数据时，我们可以设置一个更新前的值，如下图。\n\n这里可以加一个充值前金额，这里因为我的体量，并发不大，暂时没加，后面我会根据老板的要求再加的。\n\n![img](RocketMQ-message-loss-message-consistency-and-repeated-consumption-solutions/624034ed27f86abb2a034e3b.jpg)\n\n如果有重复订单打过来，那我就可以计算充值前的金额，以及当前的付款金额。来付款来实现幂等性。\n\n也可以通过版本号控制，每次更数据前，比较当前数据的版本号是否和消息中的版本号一致，如果不一致就拒绝更新数据，更新数据的同时将版本号 +1，一样可以实现幂等更新。\n\n#### 在修改数据记录并检查操作\n\n可以采用Token，UUID的方式实现幂等性。这种方式是通用性比较强的。实现的思路特别简单：在执行数据更新操作之前，先检查一下是否执行过这个更新操作。\n\n具体的实现方法是，在发送消息时，给每条消息指定一个全局唯一的 ID，消费时，先根据这个 ID 检查这条消息是否有被消费过，如果没有消费过，才更新数据，然后将消费状态置为已消费。\n","tags":["消息队列","RocketMQ"],"categories":["消息队列","RocketMQ"]},{"title":"rocketmq安装文档","url":"/2023/11/24/rocketmq-installation-document/","content":"\nRocketMQ安装文档，单机版和集群版\n\n<!--more-->\n\n## 单机版\n\n```\n# 拉取最新镜像\ndocker pull xuchengen/rocketmq:latest\n\n# Linux\n# --net=host 是使用宿主机端口，可以不用挂载\ndocker run -itd \\\n--name=rocketmq-standalone \\\n--hostname rocketmq-standalone \\\n-v /data/rocketmq-standalone:/usr/local/rocketmq/data \\\n-v /etc/localtime:/etc/localtime \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n--net=host \\\nxuchengen/rocketmq:latest\n\n# 监听端口解析\n-p 8080:8080 \\ # 控制台监听的端口(已经配置好，无法修改)\n-p 9876:9876 \\ # NameServer监听的端口\n-p 10909:10909 \\ # Broker监听的端口，fastRemotingServer使用的端口，当vipChannelEnable开启时，才会使用该端口\n-p 10911:10911 \\ # Broker监听的端口，remotingServer使用的端口，主要处理生产和消费数据等操作\n-p 10912:10912 \\ # Broker监听的端口，主要用于Broker集群间数据通讯\n\n```\n\n会出现以下错误，暂时无法解决\n\n```\nRocketMQLog:WARN No appenders could be found for logger (io.netty.util.internal.PlatformDependent0)\n```\n\n## 集群版\n\n### 拉取镜像\n\n```\ndocker pull rocketmqinc/rocketmq:4.4.0\n```\n\n### 创建NameServer\n\n创建相关目录\n\n```\ndocker run -d \\\n--name RocketMQNamesrv \\\n-p 9876:9876 \\\n-v /data/rocketmq/namesrv/conf:/root/config \\\n-v /data/rocketmq/namesrv/logs:/root/logs \\\n-v /data/rocketmq/namesrv/store:/root/store \\\n-e \"JAVA_OPTS=-Duser.home=/opt\" \\\nrocketmqinc/rocketmq:4.4.0 \\\nsh mqnamesrv \n\n```\n\n### 启动broker\n\n```\ndocker run -d  --name RocketMQBroker --link RocketMQNamesrv:namesrv -p 10911:10911 -p 10909:10909 -v  /data/rocketmq/broker/logs:/root/logs -v /data/rocketmq/broker/store:/root/store -v /data/rocketmq/broker/conf/broker.conf:/opt/rocketmq-4.4.0/conf/broker.conf --privileged=true -e \"NAMESRV_ADDR=namesrv:9876\" -e \"MAX_POSSIBLE_HEAP=200000000\" rocketmqinc/rocketmq:4.4.0 sh mqbroker -c /opt/rocketmq-4.4.0/conf/broker.conf\n```\n\n### 启动控制台\n\n```\ndocker run -d --name rmqadmin -e \"JAVA_OPTS=-Drocketmq.namesrv.addr=117.72.42.114:9876 \\\n-Dcom.rocketmq.sendMessageWithVIPChannel=false \\\n-Duser.timezone='Asia/Shanghai'\" -v /etc/localtime:/etc/localtime -p 9999:8080 styletang/rocketmq-console-ng\n```\n\n","tags":["消息队列","RocketMQ"],"categories":["消息队列","RocketMQ"]},{"title":"MySQL的InnoDB的三层B+树可以存放多少数据","url":"/2023/11/23/How-much-data-can-Innodb-three-layer-B-tree-store/","content":"\nMySQL的InnoDB的三层B+树可以存储两千万左右条数据的计算逻辑\n\n<!--more-->\n\nB+树是一种在非叶子节点存放排序好的索引而在叶子节点存放数据的[数据结构](https://cloud.tencent.com/developer/techpedia/1914?from_column=20065&from=20065)，值得注意的是，在叶子节点中，存储的并非只是一行表数据，而是以页为单位存储，一个页可以包含多行表记录。非叶子节点存放的是索引键值和页指针。\n\n那么，在MySql[数据库](https://cloud.tencent.com/solution/database?from_column=20065&from=20065)里，一个页的大小是多少呢？\n\n可以通过查询语句进行查看：show variables like 'innodb_page_size'\n\n\n\n![img](How-much-data-can-Innodb-three-layer-B-tree-store/c71b1631591ce034f0e237cdee6a8392.png)\n\n查询结果16384字节，可以通过1kb等于1024字节方式，计算出16384/1024 = 16kb，说明MySql数据库默认页大小是16kb。\n\n假设一行数据占用1kb的空间大小，然而实际上，除去字段很多的宽表外，其实很多简单的表行记录都远达不到1kb空间占比。这里我们用最坏的情况来假设一行记录大小为1kb，那么，一个16kb的页就可以存储16行数据。\n\n接下来，我们先画一个只要两层高的B+树结构图。\n\n假设第一层根节点存在以下情况：索引1对应页指针地址10，索引5对应页指针地址30，索引8对应页指针地址50。\n\n第二层节点作为叶子节点，存放的是大小为16kb的页数据，页数据里每一行记录大小为1kb，那么，一个叶子节点的页里就可以存放16条数据。\n\n![img](How-much-data-can-Innodb-three-layer-B-tree-store/7e003edf803677a6b0db1ef9c74faa3f.png)\n\n既然已经知道一个叶子节点的页中可以存放16条数据，那么，只需要知道根节点存在多少页地址指针即可，就能通过 “根节点页地址指针数量 * 单个叶子节点记录行数”。\n\n那么，根节点能存放多少个 索引：页地址指针的数据呢？\n\n在一个节点大小为16kb的情况下，我们只需要知道索引键值和页地址指针两者大小总和即可。\n\n根据一些资料得知，在MySql数据库当中，指针地址大小为6字节，若索引是bigint类型，那么就为8字节，两者加起来总共是14字节。\n\n接下来，通过以下计算步骤，就可以统计出两层的B+数大概可以存储多少条记录数据——\n\n一、先计算一个节点的字节大小：16kb * 1024 = 16384 字节。\n\n二、16384 字节 / 14 字节 = 1170 ，意味着，根节点有1170个页地址指针，然后，每个页地址指针指向的叶子节点可以存放16条数据。\n\n三、那么，根据“根节点页地址指针数量 * 单个叶子节点记录行数”，计算1170 * 16 = 18720 条记录，可见，两层B+数可以存放18720条记录，当然，这个数字是存在出入的，只是作为参考。\n\n既然已经知道两层B+数可以存放18720条数据，那么，三层不就可以进一步算出了吗？\n\n简单画一个三层B+数的存放数据计算逻辑——\n\n![img](How-much-data-can-Innodb-three-layer-B-tree-store/991e1afc21d4c4cd233a5c913d79cfc2.png)\n\n一、根节点最多有1170个指针数；\n\n二、说明第二层最多会有1170个子节点，同时，每个子节点里最多有1170个指针数；\n\n三、那么，第三层叶节点数量，可以通过 “第二层最多有1170个节点数量 * 每个节点里最多有1170个指针数量”，也就是1170 * 1170\n\n四、最后，计算第三层所有叶子数量 * 各个叶子节点存放的16条数据；\n\n最后，1170 * 1170 * 16 = 21902400，得出两千万左右条数据。\n","tags":["数据库","MySQL"],"categories":["MySQL"]},{"title":"索引type说明","url":"/2023/11/22/index-type-description/","content":"\n在使用sql的过程中经常需要建立索引，而每种索引是怎么触发的又是怎么起到作用的，首先必须知道索引和索引的类型。\n\n<!--more-->\n\n ![img](index-type-description/782972-20190425143541932-674488725.png)\n\n \n\n我们可以清楚的看到type那一栏有index ALL eq_ref，他们都代表什么意思呢？\n\n首先类型有许多，这里我只给大家介绍用的最多的几种类型：\n\n**system>const>eq_ref>ref>range>index>ALL**\n\n越往左边，性能越高，**比如system就比ALL类型性能要高出许多**，其中system、const只是理想类型，基本达不到；\n\n我们自己实际能优化到ref>range这两个类型，就是你自己写SQL，如果你没优化基本上就是ALL，如果你优化了，那就尽量达到**ref>range**这两个级别；\n\n**左边基本达不到！**\n\n所以，要对type优化的前提是，**你需要有索引，如果你连索引都没有创建，那你就不用优化了，肯定是ALL.....**；\n\n## **Type级别详解**\n\n### **一.system级别**\n\n索引类型能是system的只有两种情况：\n\n**1.只有一条数据的系统表**\n\n只有一条数据的系统表，就是系统里自带一张表，并且这个表就一条数据，这个基本上就达不到，这个是系统自带的表，而且就一条数据，所以基本达不到；\n\n**2.或衍生表只能有一条数据的主查询**\n\n这个是可以实现的，但是在实际开发当中，你不可能去写一个这么个玩意儿，不可能公司的业务去让你把SQL索引类型写实system...\n\nSQL语句:select * From (select * From test01) t where tid = 1;//前面需要加explain\n\n执行结果：\n\n ![img](index-type-description/782972-20190425143549230-45717682.png)\n\n \n\n就是把它凑出来即可；\n\n我之所以能达到system，是因为我满足了它的第二个条件；\n\n### **二.const级别**\n\nconst条件稍微低一点，**但是基本上也达不到；**\n\n**1.仅仅能查出一条的SQL语句并且用于Primary key 或 unique索引；**\n\n这个我就不说了把，都知道，所以在企业里根本不可能实现，能查出来一条SQL语句，你的索引还必须是Primary key或unique；\n\nSQL语句：select * tid From test01 where tid = 1;//前面需要加explain\n\n执行结果：\n\n ![img](index-type-description/782972-20190425143554134-686833777.png)\n\n \n\n**根据tid找，因为tid是我设置的主键，主键就是Primary key，并且只能有一条数据，我表里面本来就一条，所以我满足了；**\n\n### **三.eq_ref级别**\n\n**唯一性索引：对于每个索引键的查询，返回** **匹配唯一行数据（有且只有1个，不能多，不能0）;**\n\n解说：比如你select ...from 一张表 where 比方说有一个字段 name = 一个东西，也就是我们以name作为索引，假设我之前给name加了一个索引值，我现在根据name去查，查完后有20条数据，我就必须保证这二十条数据每行都是唯一的，不能重复不能为空！\n\n**只要满足以上条件，你就能达到eq_ref，当然前提是你要给name建索引，如果name连索引都没，那你肯定达不到eq_ref;**\n\n此种情况常见于唯一索引和主键索引；\n\n比如我根据name去查，但是一个公司里面或一个学校里面叫name的可能不止一个，一般你想用这个的时候，就要确保你这个字段是唯一的，id就可以，你可以重复两个张三，但是你身份证肯定不会重复；\n\n**添加唯一键语法：alter table 表名 add constraint 索引名 unique index(列名)**\n\n检查字段是否唯一键：show index form 表名；被展示出来的皆是有唯一约束的；\n\n#### **以上级别，均是可遇不可求！！！！**\n\n### **四 .ref级别**\n\n到ref还是问题不大的，**只要你上点心，就可以达到**；\n\n**非唯一性索引：对于每个索引键的查询，返回匹配的所有行（可以是0，或多个）**\n\n假设我现在要根据name查询，首先name可能有多个，因为一个公司或学校叫小明的不止一个人，但是你要用name去查，你必须name是索引，我们先给它加个索引，因为要达到ref级别，所以这里我给它加一个**单值索引**，\n\n**单值索引语法：alter table 表名 索引类型 索引名（字段）**\n\n现在我们根据索引来查数据，这里我假设我写的单值索引；\n\nalter table student add index index_name (name);\n\n这个时候我们再去编写sql语句：\n\nalter table student add index index_name (name);\n\n**因为name是索引列，这里假设有两个叫张三的，ref级别规则就是能查出多个或0个，很显然能查出来多个，那这条SQL语句，必然是ref级别！**\n\n执行结果：\n\n ![img](index-type-description/782972-20190425143613568-1424280825.png)\n\n \n\n数据：\n\n ![img](index-type-description/782972-20190425143622009-1263643692.png)\n\n \n\n### **五.range级别**\n\n检索指定范围的行，查找一个范围内的数据，where后面是一个范围查询 （between,in,> < >=);\n\n注：in 有时会失效，导致为ALL；\n\n现在我们写一个查询语句，前提是，tid一定是一个索引列，如果是id的话，就用主键索引，也就是唯一索引，值不可以重复，这个时候我们范围查询的时候要用它来做条件：\n\n EXPLAIN SELECT t.* FROM student t WHERE t.tid BETWEEN 1 AND 2; ;//查询tid是1到2；\n\n查看执行结果：\n\n ![img](index-type-description/782972-20190425143635585-1298799422.png)\n\n \n\n### **六.index级别**\n\n查询全部索引中的数据\n\n讲解：假设我有一张表，里面有id name age，这个时候name是一个单值索引，一旦name被设定成索引，**它就会成为B树一样，经过各种算法将name里面的值像树一样进行分类，这个时候我where name = \\**，就相当于把这颗B树查了一个遍，**\n\n**也就是说，你把name这一列给查了一遍；**\n\nSQL语句：select id From student;//我只查被索引声明的列，必然就是index了；\n\n执行结果：\n\n ![img](index-type-description/782972-20190425143641792-1375317565.png)\n\n \n\n### **七.ALL级别**\n\n查询全部表数据，就是select name From student;\n\n其中 name 不是索引；\n\n如果你查的这一列不是索引，就会导致全表扫描，**所以要避免全表扫描**；\n\n执行结果：\n\n \n\n ![img](index-type-description/782972-20190425143647380-1518567754.png)\n\n \n","tags":["数据库","MySQL"],"categories":["MySQL"]},{"title":"B树和B+树性质对比","url":"/2023/11/21/Comparison-of-properties-between-B-tree-and-B-tree/","content":"\nB树和B+树性质对比\n<!--more-->\n\n\n![image-20231121183856474](Comparison-of-properties-between-B-tree-and-B-tree/image-20231121183856474.png)\n\n### **通用概念：**\n\n **阶：**所有结点的孩子个数的最大值称为阶。通常用m表示\n\n **终端结点：**最后一排具有关键字的结点。\n\n **叶子结点：**也叫失败结点，没有任何信息的一排结点。\n\n------\n\n### B树（B-树）\n\n概念：\n\n 也叫作多路平衡查找树、B-树。\n\n 注：2-3树、2-3-4树是B树的一种特定情况。而B+树则是B树的变形。\n\n性质（条件）//注意分清子树和结点\n\nB树是一种平衡的多分树，通常我们说m阶的B树，它必须满足如下条件：\n\n- 每个结点最多有m棵子树。\n- 具有*k*个子树的非叶结点包含*k* -1个键。\n- 每个非叶子结点（除了根）具有至少**⌈ m/2⌉子树**，即最少有**⌈ m/2⌉-1个关键字**。\n- 如果**根不是终端结点，则根结点至少有一个关键字**，即至少有2棵子树。【根的关键字取值范围是[1，m-1]，子树的取值范围是[2,m]】\n- 所有叶子结点都出现在同一水平，没有任何信息（高度一致）。【带有关键字那个叫做终端结点】\n\n> **关键字：**最少⌈ m/2⌉-1，最多m-1\n> **子树**：最少⌈ m/2⌉，最多m\n\n\n\n![image-20231121183925738](Comparison-of-properties-between-B-tree-and-B-tree/image-20231121183925738.png)\n\n其他性质：\n\n![image-20231121183810713](Comparison-of-properties-between-B-tree-and-B-tree/image-20231121183810713.png)\n\n![image-20231121183942382](Comparison-of-properties-between-B-tree-and-B-tree/image-20231121183942382.png)\n\n------\n\n### B+树\n\n概念：\n\n B+树是应数据库所需要而出现的一种**B树的变形树。**\n\n性质（条件）//注意分清子树和结点\n\n一棵m阶B+树，它必须满足如下条件：\n\n- 每个结点最多有m棵子树。\n- 如果**根不是终端结点，则根结点至少有一个关键字**，即至少有2棵子树。【根的关键字取值范围是[1，m-1]】\n- **每个关键字对应一棵子树**（与B树的不同），具有*k*个子树的非叶结点包含*k* 个键。\n- 每个非叶子结点（除了根）具有至少**⌈ m/2⌉子树**，即最少有**⌈m/2⌉个关键字**。\n- 终端结点包含全部关键字及相应记录的指针，叶结点中将关键字按大小顺序排序，并且相邻叶结点按大小顺序相互链接起来。\n- 所有分支结点（可以视为索引的索引）中金包含他的各个子节点（即下一级的索引块）中关键字最大值，及指向其子结点的指针。\n\n\n\n![img](Comparison-of-properties-between-B-tree-and-B-tree/v2-b4dcfc3464962feda83fb1087147eff9_1440w.webp)\n\n\n\n------\n\n### m阶B树和B+树的主要对比：\n\n|             | B树                                             | B+树                                            |\n| ----------- | ----------------------------------------------- | ----------------------------------------------- |\n| 关键字      | 最少：⌈m/2⌉-1 最多：m-1                         | 最少：⌈m/2⌉ 最多：m                             |\n| 子树        | 非叶根：至少2棵，最多m棵 其他：至少⌈m/2⌉，最多m | 非叶根：至少2棵，最多m棵 其他：至少⌈m/2⌉，最多m |\n| key-subtree | k个key有k+1棵tree                               | k个key有k棵tree                                 |\n|             |                                                 |                                                 |\n| 非终端节点  | 包含有用信息                                    | 只是索引                                        |\n| 终端节点    | 不会出现非终端节点的key                         | 会出现非终端节点的key                           |\n\n- 在B+树中，叶结点包含信息，所有非叶结点仅起索引作用，非叶子结点中的每个索引项只是包含了对应子树最大关键字和指向该孩子树的指针，不含有该关键字对应记录的存储地址。\n- 在B+树中，终端结点包含全部关键字及相应记录的指针，即非终端结点出现过的关键字也会在这重复出现一次。而B树是不重复的。\n","tags":["数据结构"],"categories":["数据结构"]},{"title":"kafka入门","url":"/2023/10/25/kafka-introduction/","content":"\nKafka入门简介\n\n<!--more-->\n\n## 什么是 Kafka\n\nKafka 是由 `Linkedin` 公司开发的，它是一个分布式的，支持多分区、多副本，基于 Zookeeper 的分布式消息流平台，它同时也是一款开源的**基于发布订阅模式的消息引擎系统**。\n\n### Kafka 的基本术语\n\n消息：Kafka 中的数据单元被称为`消息`，也被称为记录，可以把它看作数据库表中某一行的记录。\n\n批次：为了提高效率， 消息会`分批次`写入 Kafka，批次就代指的是一组消息。\n\n主题：消息的种类称为 `主题`（Topic）,可以说一个主题代表了一类消息。相当于是对消息进行分类。主题就像是数据库中的表。\n\n分区：主题可以被分为若干个分区（partition），同一个主题中的分区可以不在一个机器上，有可能会部署在多个机器上，由此来实现 kafka 的`伸缩性`，单一主题中的分区有序，但是无法保证主题中所有的分区有序\n\n![img](kafka-introduction/1515111-20191128124302477-591839175.png)\n\n生产者： 向主题发布消息的客户端应用程序称为`生产者`（Producer），生产者用于持续不断的向某个主题发送消息。\n\n消费者：订阅主题消息的客户端程序称为`消费者`（Consumer），消费者用于处理生产者产生的消息。\n\n消费者群组：生产者与消费者的关系就如同餐厅中的厨师和顾客之间的关系一样，一个厨师对应多个顾客，也就是一个生产者对应多个消费者，`消费者群组`（Consumer Group）指的就是由一个或多个消费者组成的群体。\n\n![img](kafka-introduction/1515111-20191128124315609-278424659.png)\n\n偏移量：`偏移量`（Consumer Offset）是一种元数据，它是一个不断递增的整数值，用来记录消费者发生重平衡时的位置，以便用来恢复数据。\n\nbroker: 一个独立的 Kafka 服务器就被称为 `broker`，broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。\n\nbroker 集群：broker 是`集群` 的组成部分，broker 集群由一个或多个 broker 组成，每个集群都有一个 broker 同时充当了`集群控制器`的角色（自动从集群的活跃成员中选举出来）。\n\n副本：Kafka 中消息的备份又叫做 `副本`（Replica），副本的数量是可以配置的，Kafka 定义了两类副本：领导者副本（Leader Replica） 和 追随者副本（Follower Replica），前者对外提供服务，后者只是被动跟随。\n\n重平衡：Rebalance。消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。Rebalance 是 Kafka 消费者端实现高可用的重要手段。\n\n### Kafka 的特性（设计原则）\n\n- `高吞吐、低延迟`：kakfa 最大的特点就是收发消息非常快，kafka 每秒可以处理几十万条消息，它的最低延迟只有几毫秒。\n- `高伸缩性`： 每个主题(topic) 包含多个分区(partition)，主题中的分区可以分布在不同的主机(broker)中。\n- `持久性、可靠性`： Kafka 能够允许数据的持久化存储，消息被持久化到磁盘，并支持数据备份防止数据丢失，Kafka 底层的数据存储是基于 Zookeeper 存储的，Zookeeper 我们知道它的数据能够持久存储。\n- `容错性`： 允许集群中的节点失败，某个节点宕机，Kafka 集群能够正常工作\n- `高并发`： 支持数千个客户端同时读写\n\n### Kafka 的使用场景\n\n- 活动跟踪：Kafka 可以用来跟踪用户行为，比如我们经常回去淘宝购物，你打开淘宝的那一刻，你的登陆信息，登陆次数都会作为消息传输到 Kafka ，当你浏览购物的时候，你的浏览信息，你的搜索指数，你的购物爱好都会作为一个个消息传递给 Kafka ，这样就可以生成报告，可以做智能推荐，购买喜好等。\n- 传递消息：Kafka 另外一个基本用途是传递消息，应用程序向用户发送通知就是通过传递消息来实现的，这些应用组件可以生成消息，而不需要关心消息的格式，也不需要关心消息是如何发送的。\n- 度量指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。\n- 日志记录：Kafka 的基本概念来源于提交日志，比如我们可以把数据库的更新发送到 Kafka 上，用来记录数据库的更新时间，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等。\n- 流式处理：流式处理是有一个能够提供多种应用程序的领域。\n- 限流削峰：Kafka 多用于互联网领域某一时刻请求特别多的情况下，可以把请求写入Kafka 中，避免直接请求后端程序导致服务崩溃。\n\n### Kafka 的消息队列\n\nKafka 的消息队列一般分为两种模式：点对点模式和发布订阅模式\n\nKafka 是支持消费者群组的，也就是说 Kafka 中会有一个或者多个消费者，如果一个生产者生产的消息由一个消费者进行消费的话，那么这种模式就是点对点模式\n\n![img](kafka-introduction/1515111-20191128124324041-1692198328.png)\n\n如果一个生产者或者多个生产者产生的消息能够被多个消费者同时消费的情况，这样的消息队列成为发布订阅模式的消息队列\n\n![img](kafka-introduction/1515111-20191128124332321-1116151795.png)\n\n### Kafka 系统架构\n\n![img](kafka-introduction/1515111-20191128124342983-1550748707.png)\n\n如上图所示，一个典型的 Kafka 集群中包含若干Producer（可以是web前端产生的Page View，或者是服务器日志，系统CPU、Memory等），若干broker（Kafka支持水平扩展，一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。\n\n### 核心 API\n\nKafka 有四个核心API，它们分别是\n\n- Producer API，它允许应用程序向一个或多个 topics 上发送消息记录\n- Consumer API，允许应用程序订阅一个或多个 topics 并处理为其生成的记录流\n- Streams API，它允许应用程序作为流处理器，从一个或多个主题中消费输入流并为其生成输出流，有效的将输入流转换为输出流。\n- Connector API，它允许构建和运行将 Kafka 主题连接到现有应用程序或数据系统的可用生产者和消费者。例如，关系数据库的连接器可能会捕获对表的所有更改\n\n![img](kafka-introduction/1515111-20191128124404917-1128362623.png)\n\n### Kafka 为何如此之快\n\nKafka 实现了`零拷贝`原理来快速移动数据，避免了内核之间的切换。Kafka 可以将数据记录分批发送，从生产者到文件系统（Kafka 主题日志）到消费者，可以端到端的查看这些批次的数据。\n\n批处理能够进行更有效的数据压缩并减少 I/O 延迟，Kafka 采取顺序写入磁盘的方式，避免了随机磁盘寻址的浪费，更多关于磁盘寻址的了解，请参阅 [程序员需要了解的硬核知识之磁盘](https://mp.weixin.qq.com/s?__biz=MzU2NDg0OTgyMA==&mid=2247484654&idx=1&sn=9b6f5aaad05a49416e8f30e6b86691ae&chksm=fc45f91dcb32700b683b9a13d0d94d261171d346333d73967a4d501de3ecc273d67e8251aeae&token=674527772&lang=zh_CN#rd) 。\n\n总结一下其实就是四个要点\n\n- 顺序读写\n- 零拷贝\n- 消息压缩\n- 分批发送\n\n## Kafka 安装和重要配置\n\nKafka 安装我在 Kafka 系列第一篇应该比较详细了，详情见[带你涨姿势的认识一下kafka](https://mp.weixin.qq.com/s?__biz=MzU2NDg0OTgyMA==&mid=2247484570&idx=1&sn=1ad1c96bc7d47b88e976cbd045baf7d7&chksm=fc45f969cb32707f882c52d7434b2c0bf2ccbbc2cd854e1dc5c203deb8ae9c1831cf216e8bad&token=674527772&lang=zh_CN#rd) 这篇文章。\n\n那我们还是主要来说一下 Kafka 中的重要参数配置吧，这些参数对 Kafka 来说是非常重要的。\n\n### broker 端配置\n\n- broker.id\n\n每个 kafka broker 都有一个唯一的标识来表示，这个唯一的标识符即是 broker.id，它的默认值是 0。这个值在 kafka 集群中必须是唯一的，这个值可以任意设定，\n\n- port\n\n如果使用配置样本来启动 kafka，它会监听 9092 端口。修改 port 配置参数可以把它设置成任意的端口。要注意，如果使用 1024 以下的端口，需要使用 root 权限启动 kakfa。\n\n- zookeeper.connect\n\n用于保存 broker 元数据的 Zookeeper 地址是通过 zookeeper.connect 来指定的。比如我可以这么指定 `localhost:2181` 表示这个 Zookeeper 是运行在本地 2181 端口上的。我们也可以通过 比如我们可以通过 `zk1:2181,zk2:2181,zk3:2181` 来指定 zookeeper.connect 的多个参数值。该配置参数是用冒号分割的一组 `hostname:port/path` 列表，其含义如下\n\nhostname 是 Zookeeper 服务器的机器名或者 ip 地址。\n\nport 是 Zookeeper 客户端的端口号\n\n/path 是可选择的 Zookeeper 路径，Kafka 路径是使用了 `chroot` 环境，如果不指定默认使用跟路径。\n\n> 如果你有两套 Kafka 集群，假设分别叫它们 kafka1 和 kafka2，那么两套集群的`zookeeper.connect`参数可以这样指定：`zk1:2181,zk2:2181,zk3:2181/kafka1`和`zk1:2181,zk2:2181,zk3:2181/kafka2`\n\n- log.dirs\n\nKafka 把所有的消息都保存到磁盘上，存放这些日志片段的目录是通过 `log.dirs` 来制定的，它是用一组逗号来分割的本地系统路径，log.dirs 是没有默认值的，**你必须手动指定他的默认值**。其实还有一个参数是 `log.dir`，如你所知，这个配置是没有 `s` 的，默认情况下只用配置 log.dirs 就好了，比如你可以通过 `/home/kafka1,/home/kafka2,/home/kafka3` 这样来配置这个参数的值。\n\n- num.recovery.threads.per.data.dir\n\n对于如下3种情况，Kafka 会使用`可配置的线程池`来处理日志片段。\n\n服务器正常启动，用于打开每个分区的日志片段；\n\n服务器崩溃后重启，用于检查和截断每个分区的日志片段；\n\n服务器正常关闭，用于关闭日志片段。\n\n默认情况下，每个日志目录只使用一个线程。因为这些线程只是在服务器启动和关闭时会用到，所以完全可以设置大量的线程来达到井行操作的目的。特别是对于包含大量分区的服务器来说，一旦发生崩愤，在进行恢复时使用井行操作可能会省下数小时的时间。设置此参数时需要注意，所配置的数字对应的是 log.dirs 指定的单个日志目录。也就是说，如果 num.recovery.threads.per.data.dir 被设为 8，并且 log.dir 指定了 3 个路径，那么总共需要 24 个线程。\n\n- auto.create.topics.enable\n\n默认情况下，kafka 会使用三种方式来自动创建主题，下面是三种情况：\n\n当一个生产者开始往主题写入消息时\n\n当一个消费者开始从主题读取消息时\n\n当任意一个客户端向主题发送元数据请求时\n\n`auto.create.topics.enable`参数我建议最好设置成 false，即不允许自动创建 Topic。在我们的线上环境里面有很多名字稀奇古怪的 Topic，我想大概都是因为该参数被设置成了 true 的缘故。\n\n### 主题默认配置\n\nKafka 为新创建的主题提供了很多默认配置参数，下面就来一起认识一下这些参数\n\n- num.partitions\n\nnum.partitions 参数指定了新创建的主题需要包含多少个分区。如果启用了主题自动创建功能（该功能是默认启用的），主题分区的个数就是该参数指定的值。该参数的默认值是 1。要注意，我们可以增加主题分区的个数，但不能减少分区的个数。\n\n- default.replication.factor\n\n这个参数比较简单，它表示 kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务default.replication.factor 的默认值为1，这个参数在你启用了主题自动创建功能后有效。\n\n- log.retention.ms\n\nKafka 通常根据时间来决定数据可以保留多久。默认使用 log.retention.hours 参数来配置时间，默认是 168 个小时，也就是一周。除此之外，还有两个参数 log.retention.minutes 和 log.retentiion.ms 。这三个参数作用是一样的，都是决定消息多久以后被删除，推荐使用 log.retention.ms。\n\n- log.retention.bytes\n\n另一种保留消息的方式是判断消息是否过期。它的值通过参数 `log.retention.bytes` 来指定，作用在每一个分区上。也就是说，如果有一个包含 8 个分区的主题，并且 log.retention.bytes 被设置为 1GB，那么这个主题最多可以保留 8GB 数据。所以，当主题的分区个数增加时，整个主题可以保留的数据也随之增加。\n\n- log.segment.bytes\n\n上述的日志都是作用在日志片段上，而不是作用在单个消息上。当消息到达 broker 时，它们被追加到分区的当前日志片段上，当日志片段大小到达 log.segment.bytes 指定上限（默认为 1GB）时，当前日志片段就会被关闭，一个新的日志片段被打开。如果一个日志片段被关闭，就开始等待过期。这个参数的值越小，就越会频繁的关闭和分配新文件，从而降低磁盘写入的整体效率。\n\n- log.segment.ms\n\n上面提到日志片段经关闭后需等待过期，那么 `log.segment.ms` 这个参数就是指定日志多长时间被关闭的参数和，log.segment.ms 和 log.retention.bytes 也不存在互斥问题。日志片段会在大小或时间到达上限时被关闭，就看哪个条件先得到满足。\n\n- message.max.bytes\n\nbroker 通过设置 `message.max.bytes` 参数来限制单个消息的大小，默认是 1000 000， 也就是 1MB，如果生产者尝试发送的消息超过这个大小，不仅消息不会被接收，还会收到 broker 返回的错误消息。跟其他与字节相关的配置参数一样，该参数指的是压缩后的消息大小，也就是说，只要压缩后的消息小于 mesage.max.bytes，那么消息的实际大小可以大于这个值\n\n这个值对性能有显著的影响。值越大，那么负责处理网络连接和请求的线程就需要花越多的时间来处理这些请求。它还会增加磁盘写入块的大小，从而影响 IO 吞吐量。\n\n- retention.ms\n\n规定了该主题消息被保存的时常，默认是7天，即该主题只能保存7天的消息，一旦设置了这个值，它会覆盖掉 Broker 端的全局参数值。\n\n- retention.bytes\n\n`retention.bytes`：规定了要为该 Topic 预留多大的磁盘空间。和全局参数作用相似，这个值通常在多租户的 Kafka 集群中会有用武之地。当前默认值是 -1，表示可以无限使用磁盘空间。\n\n### JVM 参数配置\n\nJDK 版本一般推荐直接使用 JDK1.8，这个版本也是现在中国大部分程序员的首选版本。\n\n说到 JVM 端设置，就绕不开`堆`这个话题，业界最推崇的一种设置方式就是直接将 JVM 堆大小设置为 6GB，这样会避免很多 Bug 出现。\n\nJVM 端配置的另一个重要参数就是垃圾回收器的设置，也就是平时常说的 `GC` 设置。如果你依然在使用 Java 7，那么可以根据以下法则选择合适的垃圾回收器：\n\n- 如果 Broker 所在机器的 CPU 资源非常充裕，建议使用 CMS 收集器。启用方法是指定`-XX:+UseCurrentMarkSweepGC`。\n- 否则，使用吞吐量收集器。开启方法是指定`-XX:+UseParallelGC`。\n\n当然了，如果你已经在使用 Java 8 了，那么就用默认的 G1 收集器就好了。在没有任何调优的情况下，G1 表现得要比 CMS 出色，主要体现在更少的 Full GC，需要调整的参数更少等，所以使用 G1 就好了。\n\n一般 G1 的调整只需要这两个参数即可\n\n- MaxGCPauseMillis\n\n该参数指定每次垃圾回收默认的停顿时间。该值不是固定的，G1可以根据需要使用更长的时间。它的默认值是 200ms，也就是说，每一轮垃圾回收大概需要200 ms 的时间。\n\n- InitiatingHeapOccupancyPercent\n\n该参数指定了 G1 启动新一轮垃圾回收之前可以使用的堆内存百分比，默认值是45，这就表明G1在堆使用率到达45之前不会启用垃圾回收。这个百分比包括新生代和老年代。\n\n## Kafka Producer\n\n在 Kafka 中，我们把产生消息的那一方称为`生产者`，比如我们经常回去淘宝购物，你打开淘宝的那一刻，你的登陆信息，登陆次数都会作为消息传输到 Kafka 后台，当你浏览购物的时候，你的浏览信息，你的搜索指数，你的购物爱好都会作为一个个消息传递给 Kafka 后台，然后淘宝会根据你的爱好做智能推荐，致使你的钱包从来都禁不住诱惑，那么这些生产者产生的`消息`是怎么传到 Kafka 应用程序的呢？发送过程是怎么样的呢？\n\n尽管消息的产生非常简单，但是消息的发送过程还是比较复杂的，如图\n\n![img](kafka-introduction/01.png)\n\n我们从创建一个`ProducerRecord` 对象开始，ProducerRecord 是 Kafka 中的一个核心类，它代表了一组 Kafka 需要发送的 `key/value` 键值对，它由记录要发送到的主题名称（Topic Name），可选的分区号（Partition Number）以及可选的键值对构成。\n\n在发送 ProducerRecord 时，我们需要将键值对对象由序列化器转换为字节数组，这样它们才能够在网络上传输。然后消息到达了分区器。\n\n如果发送过程中指定了有效的分区号，那么在发送记录时将使用该分区。如果发送过程中未指定分区，则将使用key 的 hash 函数映射指定一个分区。如果发送的过程中既没有分区号也没有，则将以循环的方式分配一个分区。选好分区后，生产者就知道向哪个主题和分区发送数据了。\n\nProducerRecord 还有关联的时间戳，如果用户没有提供时间戳，那么生产者将会在记录中使用当前的时间作为时间戳。Kafka 最终使用的时间戳取决于 topic 主题配置的时间戳类型。\n\n- 如果将主题配置为使用 `CreateTime`，则生产者记录中的时间戳将由 broker 使用。\n- 如果将主题配置为使用`LogAppendTime`，则生产者记录中的时间戳在将消息添加到其日志中时，将由 broker 重写。\n\n然后，这条消息被存放在一个记录批次里，这个批次里的所有消息会被发送到相同的主题和分区上。由一个独立的线程负责把它们发到 Kafka Broker 上。\n\nKafka Broker 在收到消息时会返回一个响应，如果写入成功，会返回一个 RecordMetaData 对象，**它包含了主题和分区信息，以及记录在分区里的偏移量，上面两种的时间戳类型也会返回给用户**。如果写入失败，会返回一个错误。生产者在收到错误之后会尝试重新发送消息，几次之后如果还是失败的话，就返回错误消息。\n\n### 创建 Kafka 生产者\n\n要向 Kafka 写入消息，首先需要创建一个生产者对象，并设置一些属性。Kafka 生产者有3个必选的属性\n\n- bootstrap.servers\n\n该属性指定 broker 的地址清单，地址的格式为 `host:port`。清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找到其他的 broker 信息。不过建议至少要提供`两个` broker 信息，一旦其中一个宕机，生产者仍然能够连接到集群上。\n\n- key.serializer\n\nbroker 需要接收到序列化之后的 `key/value`值，所以生产者发送的消息需要经过序列化之后才传递给 Kafka Broker。生产者需要知道采用何种方式把 Java 对象转换为字节数组。key.serializer 必须被设置为一个实现了`org.apache.kafka.common.serialization.Serializer` 接口的类，生产者会使用这个类把键对象序列化为字节数组。这里拓展一下 Serializer 类\n\nSerializer 是一个接口，它表示类将会采用何种方式序列化，它的作用是把对象转换为字节，实现了 Serializer 接口的类主要有 `ByteArraySerializer`、`StringSerializer`、`IntegerSerializer` ，其中 ByteArraySerialize 是 Kafka 默认使用的序列化器，其他的序列化器还有很多，你可以通过 [这里](https://kafka.apache.org/23/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html) 查看其他序列化器。要注意的一点：**key.serializer 是必须要设置的，即使你打算只发送值的内容**。\n\n- value.serializer\n\n与 key.serializer 一样，value.serializer 指定的类会将值序列化。\n\n下面代码演示了如何创建一个 Kafka 生产者，这里只指定了必要的属性，其他使用默认的配置\n\n```java\nprivate Properties properties = new Properties();\nproperties.put(\"bootstrap.servers\",\"broker1:9092,broker2:9092\");\nproperties.put(\"key.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\");\nproperties.put(\"value.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\");\nproperties = new KafkaProducer<String,String>(properties);\n```\n\n来解释一下这段代码\n\n- 首先创建了一个 Properties 对象\n- 使用 `StringSerializer` 序列化器序列化 key / value 键值对\n- 在这里我们创建了一个新的生产者对象，并为键值设置了恰当的类型，然后把 Properties 对象传递给他。\n\n### Kafka 消息发送\n\n实例化生产者对象后，接下来就可以开始发送消息了，发送消息主要由下面几种方式\n\n### 简单消息发送\n\nKafka 最简单的消息发送如下：\n\n```java\nProducerRecord<String,String> record =\n                new ProducerRecord<String, String>(\"CustomerCountry\",\"West\",\"France\");\n\nproducer.send(record);\n```\n\n代码中生产者(producer)的 `send()` 方法需要把 `ProducerRecord` 的对象作为参数进行发送，ProducerRecord 有很多构造函数，这个我们下面讨论，这里调用的是\n\n```java\npublic ProducerRecord(String topic, K key, V value) {}\n```\n\n这个构造函数，需要传递的是 topic主题，key 和 value。\n\n把对应的参数传递完成后，生产者调用 send() 方法发送消息（ProducerRecord对象）。我们可以从生产者的架构图中看出，消息是先被写入分区中的缓冲区中，然后分批次发送给 Kafka Broker。\n\n![img](kafka-introduction/02.png)\n\n发送成功后，send() 方法会返回一个 `Future(java.util.concurrent)` 对象，Future 对象的类型是 `RecordMetadata` 类型，我们上面这段代码没有考虑返回值，所以没有生成对应的 Future 对象，所以没有办法知道消息是否发送成功。如果不是很重要的信息或者对结果不会产生影响的信息，可以使用这种方式进行发送。\n\n我们可以忽略发送消息时可能发生的错误或者在服务器端可能发生的错误，但在消息发送之前，生产者还可能发生其他的异常。这些异常有可能是 `SerializationException(序列化失败)`，`BufferedExhaustedException 或 TimeoutException(说明缓冲区已满)`，又或是 `InterruptedException(说明发送线程被中断)`\n\n### 同步发送消息\n\n第二种消息发送机制如下所示\n\n```java\nProducerRecord<String,String> record =\n                new ProducerRecord<String, String>(\"CustomerCountry\",\"West\",\"France\");\n\ntry{\n  RecordMetadata recordMetadata = producer.send(record).get();\n}catch(Exception e){\n  e.printStackTrace()；\n}\n```\n\n这种发送消息的方式较上面的发送方式有了改进，首先调用 send() 方法，然后再调用 get() 方法等待 Kafka 响应。如果服务器返回错误，get() 方法会抛出异常，如果没有发生错误，我们会得到 `RecordMetadata` 对象，可以用它来查看消息记录。\n\n生产者（KafkaProducer）在发送的过程中会出现两类错误：其中一类是重试错误，这类错误可以通过重发消息来解决。比如连接的错误，可以通过再次建立连接来解决；无`主`错误则可以通过重新为分区选举首领来解决。KafkaProducer 被配置为自动重试，如果多次重试后仍无法解决问题，则会抛出重试异常。另一类错误是无法通过重试来解决的，比如`消息过大`对于这类错误，KafkaProducer 不会进行重试，直接抛出异常。\n\n### 异步发送消息\n\n同步发送消息都有个问题，那就是同一时间只能有一个消息在发送，这会造成许多消息无法直接发送，造成消息滞后，无法发挥效益最大化。\n\n比如消息在应用程序和 Kafka 集群之间一个来回需要 10ms。如果发送完每个消息后都等待响应的话，那么发送100个消息需要 1 秒，但是如果是`异步`方式的话，发送 100 条消息所需要的时间就会少很多很多。大多数时候，虽然Kafka 会返回 `RecordMetadata` 消息，但是我们并不需要等待响应。\n\n为了在异步发送消息的同时能够对异常情况进行处理，生产者提供了回掉支持。下面是回调的一个例子\n\n```java\nProducerRecord<String, String> producerRecord = new ProducerRecord<String, String>(\"CustomerCountry\", \"Huston\", \"America\");\n        producer.send(producerRecord,new DemoProducerCallBack());\n\n\nclass DemoProducerCallBack implements Callback {\n\n  public void onCompletion(RecordMetadata metadata, Exception exception) {\n    if(exception != null){\n      exception.printStackTrace();;\n    }\n  }\n}\n```\n\n首先实现回调需要定义一个实现了`org.apache.kafka.clients.producer.Callback`的类，这个接口只有一个 `onCompletion`方法。如果 kafka 返回一个错误，onCompletion 方法会抛出一个非空(non null)异常，这里我们只是简单的把它打印出来，如果是生产环境需要更详细的处理，然后在 send() 方法发送的时候传递一个 Callback 回调的对象。\n\n### 生产者分区机制\n\nKafka 对于数据的读写是以`分区`为粒度的，分区可以分布在多个主机（Broker）中，这样每个节点能够实现独立的数据写入和读取，并且能够通过增加新的节点来增加 Kafka 集群的吞吐量，通过分区部署在多个 Broker 来实现`负载均衡`的效果。\n\n上面我们介绍了生产者的发送方式有三种：`不管结果如何直接发送`、`发送并返回结果`、`发送并回调`。由于消息是存在主题（topic）的分区（partition）中的，所以当 Producer 生产者发送产生一条消息发给 topic 的时候，你如何判断这条消息会存在哪个分区中呢？\n\n这其实就设计到 Kafka 的分区机制了。\n\n#### 分区策略\n\nKafka 的分区策略指的就是将生产者发送到哪个分区的算法。Kafka 为我们提供了默认的分区策略，同时它也支持你自定义分区策略。\n\n如果要自定义分区策略的话，你需要显示配置生产者端的参数 `Partitioner.class`，我们可以看一下这个类它位于 `org.apache.kafka.clients.producer` 包下\n\n```java\npublic interface Partitioner extends Configurable, Closeable {\n  \n  public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);\n\n  public void close();\n  \n  default public void onNewBatch(String topic, Cluster cluster, int prevPartition) {}\n}\n```\n\nPartitioner 类有三个方法，分别来解释一下\n\n- partition(): 这个类有几个参数: `topic`，表示需要传递的主题；`key` 表示消息中的键值；`keyBytes`表示分区中序列化过后的key，byte数组的形式传递；`value` 表示消息的 value 值；`valueBytes` 表示分区中序列化后的值数组；`cluster`表示当前集群的原数据。Kafka 给你这么多信息，就是希望让你能够充分地利用这些信息对消息进行分区，计算出它要被发送到哪个分区中。\n- close() : 继承了 `Closeable` 接口能够实现 close() 方法，在分区关闭时调用。\n- onNewBatch(): 表示通知分区程序用来创建新的批次\n\n其中与分区策略息息相关的就是 partition() 方法了，分区策略有下面这几种\n\n**顺序轮询**\n\n顺序分配，消息是均匀的分配给每个 partition，即每个分区存储一次消息。就像下面这样\n\n![img](kafka-introduction/1515111-20191128124446431-1399551981.png)\n\n上图表示的就是轮询策略，轮训策略是 Kafka Producer 提供的默认策略，如果你不使用指定的轮训策略的话，Kafka 默认会使用顺序轮训策略的方式。\n\n**随机轮询**\n\n随机轮询简而言之就是随机的向 partition 中保存消息，如下图所示\n\n![img](kafka-introduction/04.png)\n\n实现随机分配的代码只需要两行，如下\n\n```java\nList<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\nreturn ThreadLocalRandom.current().nextInt(partitions.size());\n```\n\n先计算出该主题总的分区数，然后随机地返回一个小于它的正整数。\n\n本质上看随机策略也是力求将数据均匀地打散到各个分区，但从实际表现来看，它要逊于轮询策略，所以**如果追求数据的均匀分布，还是使用轮询策略比较好**。事实上，随机策略是老版本生产者使用的分区策略，在新版本中已经改为轮询了。\n\n**按照 key 进行消息保存**\n\n这个策略也叫做 **key-ordering** 策略，Kafka 中每条消息都会有自己的key，一旦消息被定义了 Key，那么你就可以保证同一个 Key 的所有消息都进入到相同的分区里面，由于每个分区下的消息处理都是有顺序的，故这个策略被称为按消息键保序策略，如下图所示\n\n![img](kafka-introduction/05.png)\n\n实现这个策略的 partition 方法同样简单，只需要下面两行代码即可：\n\n```java\nList<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\nreturn Math.abs(key.hashCode()) % partitions.size();\n```\n\n上面这几种分区策略都是比较基础的策略，除此之外，你还可以自定义分区策略。\n\n### 生产者压缩机制\n\n压缩一词简单来讲就是一种互换思想，它是一种经典的用 CPU 时间去换磁盘空间或者 I/O 传输量的思想，希望以较小的 CPU 开销带来更少的磁盘占用或更少的网络 I/O 传输。如果你还不了解的话我希望你先读完这篇文章 [程序员需要了解的硬核知识之压缩算法](https://mp.weixin.qq.com/s?__biz=MzU2NDg0OTgyMA==&mid=2247484672&idx=1&sn=7de8762995227b21f35c6bbb47b22233&chksm=fc45f8f3cb3271e5deac7e08d5dcdbfb24e3d66a1ac431fd78252bb0e0cb3cd1c727f053211a&token=343157109&lang=zh_CN#rd)，然后你就明白压缩是怎么回事了。\n\n#### Kafka 压缩是什么\n\nKafka 的消息分为两层：消息集合 和 消息。一个消息集合中包含若干条日志项，而日志项才是真正封装消息的地方。Kafka 底层的消息日志由一系列消息集合日志项组成。Kafka 通常不会直接操作具体的一条条消息，它总是在消息集合这个层面上进行`写入`操作。\n\n在 Kafka 中，压缩会发生在两个地方：Kafka Producer 和 Kafka Consumer，为什么启用压缩？说白了就是消息太大，需要`变小一点` 来使消息发的更快一些。\n\nKafka Producer 中使用 `compression.type` 来开启压缩\n\n```java\nprivate Properties properties = new Properties();\nproperties.put(\"bootstrap.servers\",\"192.168.1.9:9092\");\nproperties.put(\"key.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\");\nproperties.put(\"value.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\");\nproperties.put(\"compression.type\", \"gzip\");\n\nProducer<String,String> producer = new KafkaProducer<String, String>(properties);\n\nProducerRecord<String,String> record =\n  new ProducerRecord<String, String>(\"CustomerCountry\",\"Precision Products\",\"France\");\n```\n\n上面代码表明该 Producer 的压缩算法使用的是 GZIP\n\n**有压缩必有解压缩**，Producer 使用压缩算法压缩消息后并发送给服务器后，由 Consumer 消费者进行解压缩，因为采用的何种压缩算法是随着 key、value 一起发送过去的，所以消费者知道采用何种压缩算法。\n\n### Kafka 重要参数配置\n\n在上一篇文章 [带你涨姿势的认识一下kafka](https://mp.weixin.qq.com/s?__biz=MzU2NDg0OTgyMA==&mid=2247484570&idx=1&sn=1ad1c96bc7d47b88e976cbd045baf7d7&chksm=fc45f969cb32707f882c52d7434b2c0bf2ccbbc2cd854e1dc5c203deb8ae9c1831cf216e8bad&token=343157109&lang=zh_CN#rd)中，我们主要介绍了一下 kafka 集群搭建的参数，本篇文章我们来介绍一下 Kafka 生产者重要的配置，生产者有很多可配置的参数，在文档里（http://kafka.apache.org/documentation/#producerconfigs）都有说明，我们介绍几个在内存使用、性能和可靠性方面对生产者影响比较大的参数进行说明\n\n**key.serializer**\n\n用于 key 键的序列化，它实现了 `org.apache.kafka.common.serialization.Serializer` 接口\n\n**value.serializer**\n\n用于 value 值的序列化，实现了 `org.apache.kafka.common.serialization.Serializer` 接口\n\n**acks**\n\nacks 参数指定了要有多少个分区副本接收消息，生产者才认为消息是写入成功的。此参数对消息丢失的影响较大\n\n- 如果 acks = 0，就表示生产者也不知道自己产生的消息是否被服务器接收了，它才知道它写成功了。如果发送的途中产生了错误，生产者也不知道，它也比较懵逼，因为没有返回任何消息。这就类似于 UDP 的运输层协议，只管发，服务器接受不接受它也不关心。\n- 如果 acks = 1，只要集群的 Leader 接收到消息，就会给生产者返回一条消息，告诉它写入成功。如果发送途中造成了网络异常或者 Leader 还没选举出来等其他情况导致消息写入失败，生产者会受到错误消息，这时候生产者往往会再次重发数据。因为消息的发送也分为 `同步` 和 `异步`，Kafka 为了保证消息的高效传输会决定是同步发送还是异步发送。如果让客户端等待服务器的响应（通过调用 `Future` 中的 `get()` 方法），显然会增加延迟，如果客户端使用回调，就会解决这个问题。\n- 如果 acks = all，这种情况下是只有当所有参与复制的节点都收到消息时，生产者才会接收到一个来自服务器的消息。不过，它的延迟比 acks =1 时更高，因为我们要等待不只一个服务器节点接收消息。\n\n**buffer.memory**\n\n此参数用来设置生产者内存缓冲区的大小，生产者用它缓冲要发送到服务器的消息。如果应用程序发送消息的速度超过发送到服务器的速度，会导致生产者空间不足。这个时候，send() 方法调用要么被阻塞，要么抛出异常，具体取决于 `block.on.buffer.null` 参数的设置。\n\n**compression.type**\n\n此参数来表示生产者启用何种压缩算法，默认情况下，消息发送时不会被压缩。该参数可以设置为 snappy、gzip 和 lz4，它指定了消息发送给 broker 之前使用哪一种压缩算法进行压缩。下面是各压缩算法的对比\n\n![img](kafka-introduction/06.png)\n\n![img](kafka-introduction/07.png)\n\n**retries**\n\n生产者从服务器收到的错误有可能是临时性的错误（比如分区找不到首领），在这种情况下，`reteis` 参数的值决定了生产者可以重发的消息次数，如果达到这个次数，生产者会放弃重试并返回错误。默认情况下，生产者在每次重试之间等待 100ms，这个等待参数可以通过 `retry.backoff.ms` 进行修改。\n\n**batch.size**\n\n当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。当批次被填满，批次里的所有消息会被发送出去。不过生产者井不一定都会等到批次被填满才发送，任意条数的消息都可能被发送。\n\n**client.id**\n\n此参数可以是任意的字符串，服务器会用它来识别消息的来源，一般配置在日志里\n\n**max.in.flight.requests.per.connection**\n\n此参数指定了生产者在收到服务器响应之前可以发送多少消息，它的值越高，就会占用越多的内存，不过也会提高吞吐量。把它设为1 可以保证消息是按照发送的顺序写入服务器。\n\n**timeout.ms、request.timeout.ms 和 metadata.fetch.timeout.ms**\n\nrequest.timeout.ms 指定了生产者在发送数据时等待服务器返回的响应时间，metadata.fetch.timeout.ms 指定了生产者在获取元数据（比如目标分区的首领是谁）时等待服务器返回响应的时间。如果等待时间超时，生产者要么重试发送数据，要么返回一个错误。timeout.ms 指定了 broker 等待同步副本返回消息确认的时间，与 asks 的配置相匹配----如果在指定时间内没有收到同步副本的确认，那么 broker 就会返回一个错误。\n\n**max.block.ms**\n\n此参数指定了在调用 send() 方法或使用 partitionFor() 方法获取元数据时生产者的阻塞时间当生产者的发送缓冲区已捕，或者没有可用的元数据时，这些方法就会阻塞。在阻塞时间达到 max.block.ms 时，生产者会抛出超时异常。\n\n**max.request.size**\n\n该参数用于控制生产者发送的请求大小。它可以指能发送的单个消息的最大值，也可以指单个请求里所有消息的总大小。\n\n**receive.buffer.bytes 和 send.buffer.bytes**\n\nKafka 是基于 TCP 实现的，为了保证可靠的消息传输，这两个参数分别指定了 TCP Socket 接收和发送数据包的缓冲区的大小。如果它们被设置为 -1，就使用操作系统的默认值。如果生产者或消费者与 broker 处于不同的数据中心，那么可以适当增大这些值。\n\n## Kafka Consumer\n\n应用程序使用 `KafkaConsumer` 从 Kafka 中订阅主题并接收来自这些主题的消息，然后再把他们保存起来。应用程序首先需要创建一个 KafkaConsumer 对象，订阅主题并开始接受消息，验证消息并保存结果。一段时间后，生产者往主题写入的速度超过了应用程序验证数据的速度，这时候该如何处理？如果只使用单个消费者的话，应用程序会跟不上消息生成的速度，就像多个生产者像相同的主题写入消息一样，这时候就需要多个消费者共同参与消费主题中的消息，对消息进行分流处理。\n\nKafka 消费者从属于`消费者群组`。一个群组中的消费者订阅的都是`相同`的主题，每个消费者接收主题一部分分区的消息。下面是一个 Kafka 分区消费示意图\n\n![img](kafka-introduction/1515111-20191128124510141-287877636.png)\n\n上图中的主题 T1 有四个分区，分别是分区0、分区1、分区2、分区3，我们创建一个消费者群组1，消费者群组中只有一个消费者，它订阅主题T1，接收到 T1 中的全部消息。由于一个消费者处理四个生产者发送到分区的消息，压力有些大，需要帮手来帮忙分担任务，于是就演变为下图\n\n![img](kafka-introduction/1515111-20191128124523391-1428148798.png)\n\n这样一来，消费者的消费能力就大大提高了，但是在某些环境下比如用户产生消息特别多的时候，生产者产生的消息仍旧让消费者吃不消，那就继续增加消费者。\n\n![img](kafka-introduction/1515111-20191128124532591-676030370.png)\n\n如上图所示，每个分区所产生的消息能够被每个消费者群组中的消费者消费，如果向消费者群组中增加更多的消费者，那么多余的消费者将会闲置，如下图所示\n\n![img](kafka-introduction/1515111-20191128124543543-1532704944.png)\n\n向群组中增加消费者是横向伸缩消费能力的主要方式。总而言之，我们可以通过增加消费组的消费者来进行`水平扩展提升消费能力`。这也是为什么建议创建主题时使用比较多的分区数，这样可以在消费负载高的情况下增加消费者来提升性能。另外，消费者的数量不应该比分区数多，因为多出来的消费者是空闲的，没有任何帮助。\n\nKafka 一个很重要的特性就是，只需写入一次消息，可以支持任意多的应用读取这个消息。换句话说，每个应用都可以读到全量的消息。为了使得每个应用都能读到全量消息，应用需要有不同的消费组。对于上面的例子，假如我们新增了一个新的消费组 G2，而这个消费组有两个消费者，那么就演变为下图这样\n\n![img](kafka-introduction/1515111-20191128124553311-1167412207.png)\n\n在这个场景中，消费组 G1 和消费组 G2 都能收到 T1 主题的全量消息，在逻辑意义上来说它们属于不同的应用。\n\n**总结起来就是如果应用需要读取全量消息，那么请为该应用设置一个消费组；如果该应用消费能力不足，那么可以考虑在这个消费组里增加消费者**。\n\n### 消费者组和分区重平衡\n\n#### 消费者组是什么\n\n`消费者组（Consumer Group）`是由一个或多个消费者实例（Consumer Instance）组成的群组，具有可扩展性和可容错性的一种机制。消费者组内的消费者`共享`一个消费者组ID，这个ID 也叫做 `Group ID`，组内的消费者共同对一个主题进行订阅和消费，同一个组中的消费者只能消费一个分区的消息，多余的消费者会闲置，派不上用场。\n\n我们在上面提到了两种消费方式\n\n- 一个消费者群组消费一个主题中的消息，这种消费模式又称为`点对点`的消费方式，点对点的消费方式又被称为消息队列\n- 一个主题中的消息被多个消费者群组共同消费，这种消费模式又称为`发布-订阅`模式\n\n#### 消费者重平衡\n\n我们从上面的`消费者演变图`中可以知道这么一个过程：最初是一个消费者订阅一个主题并消费其全部分区的消息，后来有一个消费者加入群组，随后又有更多的消费者加入群组，而新加入的消费者实例`分摊`了最初消费者的部分消息，这种把分区的所有权通过一个消费者转到其他消费者的行为称为`重平衡`，英文名也叫做 `Rebalance` 。如下图所示\n\n![img](kafka-introduction/1515111-20191128124603277-431025951.png)\n\n重平衡非常重要，它为消费者群组带来了`高可用性` 和 `伸缩性`，我们可以放心的添加消费者或移除消费者，不过在正常情况下我们并不希望发生这样的行为。在重平衡期间，消费者无法读取消息，造成整个消费者组在重平衡的期间都不可用。另外，当分区被重新分配给另一个消费者时，消息当前的读取状态会丢失，它有可能还需要去刷新缓存，在它重新恢复状态之前会拖慢应用程序。\n\n消费者通过向`组织协调者`（Kafka Broker）发送心跳来维护自己是消费者组的一员并确认其拥有的分区。对于不同不的消费群体来说，其组织协调者可以是不同的。只要消费者定期发送心跳，就会认为消费者是存活的并处理其分区中的消息。当消费者检索记录或者提交它所消费的记录时就会发送心跳。\n\n如果过了一段时间 Kafka 停止发送心跳了，会话（Session）就会过期，组织协调者就会认为这个 Consumer 已经死亡，就会触发一次重平衡。如果消费者宕机并且停止发送消息，组织协调者会等待几秒钟，确认它死亡了才会触发重平衡。在这段时间里，**死亡的消费者将不处理任何消息**。在清理消费者时，消费者将通知协调者它要离开群组，组织协调者会触发一次重平衡，尽量降低处理停顿。\n\n重平衡是一把双刃剑，它为消费者群组带来高可用性和伸缩性的同时，还有有一些明显的缺点(bug)，而这些 bug 到现在社区还无法修改。\n\n重平衡的过程对消费者组有极大的影响。因为每次重平衡过程中都会导致万物静止，参考 JVM 中的垃圾回收机制，也就是 Stop The World ，STW，(引用自《深入理解 Java 虚拟机》中 p76 关于 Serial 收集器的描述)：\n\n> 更重要的是它在进行垃圾收集时，必须暂停其他所有的工作线程。直到它收集结束。`Stop The World` 这个名字听起来很帅，但这项工作实际上是由虚拟机在后台自动发起并完成的，在用户不可见的情况下把用户正常工作的线程全部停掉，这对很多应用来说都是难以接受的。\n\n也就是说，在重平衡期间，消费者组中的消费者实例都会停止消费，等待重平衡的完成。而且重平衡这个过程很慢......\n\n### 创建消费者\n\n上面的理论说的有点多，下面就通过代码来讲解一下消费者是如何消费的\n\n在读取消息之前，需要先创建一个 `KafkaConsumer` 对象。创建 KafkaConsumer 对象与创建 KafkaProducer 对象十分相似 --- 把需要传递给消费者的属性放在 `properties` 对象中，后面我们会着重讨论 Kafka 的一些配置，这里我们先简单的创建一下，使用3个属性就足矣，分别是 `bootstrap.server`，`key.deserializer`，`value.deserializer` 。\n\n这三个属性我们已经用过很多次了，如果你还不是很清楚的话，可以参考 [带你涨姿势是认识一下Kafka Producer](https://mp.weixin.qq.com/s/Br0_DQ854n-Is0W88DbPOg)\n\n还有一个属性是 `group.id` 这个属性不是必须的，它指定了 KafkaConsumer 是属于哪个消费者群组。创建不属于任何一个群组的消费者也是可以的\n\n```java\nProperties properties = new Properties();\n        properties.put(\"bootstrap.server\",\"192.168.1.9:9092\");     properties.put(\"key.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\");   properties.put(\"value.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\");\nKafkaConsumer<String,String> consumer = new KafkaConsumer<>(properties);\n```\n\n#### 主题订阅\n\n创建好消费者之后，下一步就开始订阅主题了。`subscribe()` 方法接受一个主题列表作为参数，使用起来比较简单\n\n```java\nconsumer.subscribe(Collections.singletonList(\"customerTopic\"));\n```\n\n为了简单我们只订阅了一个主题 `customerTopic`，参数传入的是一个正则表达式，正则表达式可以匹配多个主题，如果有人创建了新的主题，并且主题的名字与正则表达式相匹配，那么会立即触发一次重平衡，消费者就可以读取新的主题。\n\n要订阅所有与 test 相关的主题，可以这样做\n\n```java\nconsumer.subscribe(\"test.*\");\n```\n\n#### 轮询\n\n我们知道，Kafka 是支持订阅/发布模式的，生产者发送数据给 Kafka Broker，那么消费者是如何知道生产者发送了数据呢？其实生产者产生的数据消费者是不知道的，KafkaConsumer 采用轮询的方式定期去 Kafka Broker 中进行数据的检索，如果有数据就用来消费，如果没有就再继续轮询等待，下面是轮询等待的具体实现\n\n```java\ntry {\n  while (true) {\n    ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(100));\n    for (ConsumerRecord<String, String> record : records) {\n      int updateCount = 1;\n      if (map.containsKey(record.value())) {\n        updateCount = (int) map.get(record.value() + 1);\n      }\n      map.put(record.value(), updateCount);\n    }\n  }\n}finally {\n  consumer.close();\n}\n```\n\n- 这是一个无限循环。消费者实际上是一个长期运行的应用程序，它通过轮询的方式向 Kafka 请求数据。\n- 第三行代码非常重要，Kafka 必须定期循环请求数据，否则就会认为该 Consumer 已经挂了，会触发重平衡，它的分区会移交给群组中的其它消费者。传给 `poll()` 方法的是一个超市时间，用 `java.time.Duration` 类来表示，如果该参数被设置为 0 ，poll() 方法会立刻返回，否则就会在指定的毫秒数内一直等待 broker 返回数据。\n- poll() 方法会返回一个记录列表。每条记录都包含了记录所属主题的信息，记录所在分区的信息、记录在分区中的偏移量，以及记录的键值对。我们一般会遍历这个列表，逐条处理每条记录。\n- 在退出应用程序之前使用 `close()` 方法关闭消费者。网络连接和 socket 也会随之关闭，并立即触发一次重平衡，而不是等待群组协调器发现它不再发送心跳并认定它已经死亡。\n\n> **线程安全性**\n>\n> 在同一个群组中，我们无法让一个线程运行多个消费者，也无法让多个线程安全的共享一个消费者。按照规则，一个消费者使用一个线程，如果一个消费者群组中多个消费者都想要运行的话，那么必须让每个消费者在自己的线程中运行，可以使用 Java 中的 `ExecutorService` 启动多个消费者进行进行处理。\n\n### 消费者配置\n\n到目前为止，我们学习了如何使用消费者 API，不过只介绍了几个最基本的属性，Kafka 文档列出了所有与消费者相关的配置说明。大部分参数都有合理的默认值，一般不需要修改它们，下面我们就来介绍一下这些参数。\n\n- fetch.min.bytes\n\n该属性指定了消费者从服务器获取记录的最小字节数。broker 在收到消费者的数据请求时，如果可用的数据量小于 `fetch.min.bytes` 指定的大小，那么它会等到有足够的可用数据时才把它返回给消费者。这样可以降低消费者和 broker 的工作负载，因为它们在主题使用频率不是很高的时候就不用来回处理消息。如果没有很多可用数据，但消费者的 CPU 使用率很高，那么就需要把该属性的值设得比默认值大。如果消费者的数量比较多，把该属性的值调大可以降低 broker 的工作负载。\n\n- fetch.max.wait.ms\n\n我们通过上面的 **fetch.min.bytes** 告诉 Kafka，等到有足够的数据时才会把它返回给消费者。而 **fetch.max.wait.ms** 则用于指定 broker 的等待时间，默认是 500 毫秒。如果没有足够的数据流入 kafka 的话，消费者获取的最小数据量要求就得不到满足，最终导致 500 毫秒的延迟。如果要降低潜在的延迟，就可以把参数值设置的小一些。如果 fetch.max.wait.ms 被设置为 100 毫秒的延迟，而 fetch.min.bytes 的值设置为 1MB，那么 Kafka 在收到消费者请求后，要么返回 1MB 的数据，要么在 100 ms 后返回所有可用的数据。就看哪个条件首先被满足。\n\n- max.partition.fetch.bytes\n\n该属性指定了服务器从每个分区里返回给消费者的`最大字节数`。它的默认值时 1MB，也就是说，`KafkaConsumer.poll()` 方法从每个分区里返回的记录最多不超过 max.partition.fetch.bytes 指定的字节。如果一个主题有20个分区和5个消费者，那么每个消费者需要`至少`4 MB的可用内存来接收记录。在为消费者分配内存时，可以给它们多分配一些，因为如果群组里有消费者发生崩溃，剩下的消费者需要处理更多的分区。max.partition.fetch.bytes 的值必须比 broker 能够接收的最大消息的字节数(通过 max.message.size 属性配置大)，**否则消费者可能无法读取这些消息，导致消费者一直挂起重试**。 在设置该属性时，另外一个考量的因素是消费者处理数据的时间。消费者需要频繁的调用 poll() 方法来避免会话过期和发生分区再平衡，如果单次调用poll() 返回的数据太多，消费者需要更多的时间进行处理，可能无法及时进行下一个轮询来避免会话过期。如果出现这种情况，可以把 max.partition.fetch.bytes 值改小，或者延长会话过期时间。\n\n- session.timeout.ms\n\n这个属性指定了消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。如果消费者没有在 **session.timeout.ms** 指定的时间内发送心跳给群组协调器，就会被认定为死亡，协调器就会触发重平衡。把它的分区分配给消费者群组中的其它消费者，此属性与 `heartbeat.interval.ms` 紧密相关。heartbeat.interval.ms 指定了 poll() 方法向群组协调器发送心跳的频率，session.timeout.ms 则指定了消费者可以多久不发送心跳。所以，这两个属性一般需要同时修改，heartbeat.interval.ms 必须比 session.timeout.ms 小，一般是 session.timeout.ms 的三分之一。如果 session.timeout.ms 是 3s，那么 heartbeat.interval.ms 应该是 1s。把 session.timeout.ms 值设置的比默认值小，可以更快地检测和恢复崩愤的节点，不过长时间的轮询或垃圾收集可能导致非预期的重平衡。把该属性的值设置得大一些，可以减少意外的重平衡，不过检测节点崩溃需要更长的时间。\n\n- auto.offset.reset\n\n该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下的该如何处理。它的默认值是 `latest`，意思指的是，在偏移量无效的情况下，消费者将从最新的记录开始读取数据。另一个值是 `earliest`，意思指的是在偏移量无效的情况下，消费者将从起始位置处开始读取分区的记录。\n\n- enable.auto.commit\n\n我们稍后将介绍几种不同的提交偏移量的方式。该属性指定了消费者是否自动提交偏移量，默认值是 true，为了尽量避免出现重复数据和数据丢失，可以把它设置为 false，由自己控制何时提交偏移量。如果把它设置为 true，还可以通过 **auto.commit.interval.ms** 属性来控制提交的频率\n\n- partition.assignment.strategy\n\n我们知道，分区会分配给群组中的消费者。`PartitionAssignor` 会根据给定的消费者和主题，决定哪些分区应该被分配给哪个消费者，Kafka 有两个默认的分配策略`Range` 和 `RoundRobin`\n\n- client.id\n\n该属性可以是任意字符串，broker 用他来标识从客户端发送过来的消息，通常被用在日志、度量指标和配额中\n\n- max.poll.records\n\n该属性用于控制单次调用 call() 方法能够返回的记录数量，可以帮你控制在轮询中需要处理的数据量。\n\n- receive.buffer.bytes 和 send.buffer.bytes\n\nsocket 在读写数据时用到的 TCP 缓冲区也可以设置大小。如果它们被设置为 -1，就使用操作系统默认值。如果生产者或消费者与 broker 处于不同的数据中心内，可以适当增大这些值，因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。\n\n### 提交和偏移量的概念\n\n#### 特殊偏移\n\n我们上面提到，消费者在每次调用`poll()` 方法进行定时轮询的时候，会返回由生产者写入 Kafka 但是还没有被消费者消费的记录，因此我们可以追踪到哪些记录是被群组里的哪个消费者读取的。消费者可以使用 Kafka 来追踪消息在分区中的位置（偏移量）\n\n消费者会向一个叫做 `_consumer_offset` 的特殊主题中发送消息，这个主题会保存每次所发送消息中的分区偏移量，这个主题的主要作用就是消费者触发重平衡后记录偏移使用的，消费者每次向这个主题发送消息，正常情况下不触发重平衡，这个主题是不起作用的，当触发重平衡后，消费者停止工作，每个消费者可能会分到对应的分区，这个主题就是让消费者能够继续处理消息所设置的。\n\n如果提交的偏移量小于客户端最后一次处理的偏移量，那么位于两个偏移量之间的消息就会被重复处理\n\n![img](kafka-introduction/1515111-20191128124619917-951347964.png)\n\n如果提交的偏移量大于最后一次消费时的偏移量，那么处于两个偏移量中间的消息将会丢失\n\n![img](kafka-introduction/1515111-20191128124627673-1587568519.png)\n\n既然`_consumer_offset` 如此重要，那么它的提交方式是怎样的呢？下面我们就来说一下####提交方式\n\nKafkaConsumer API 提供了多种方式来提交偏移量\n\n#### 自动提交\n\n最简单的方式就是让消费者自动提交偏移量。如果 `enable.auto.commit` 被设置为true，那么每过 5s，消费者会自动把从 poll() 方法轮询到的最大偏移量提交上去。提交时间间隔由 `auto.commit.interval.ms` 控制，默认是 5s。与消费者里的其他东西一样，自动提交也是在轮询中进行的。消费者在每次轮询中会检查是否提交该偏移量了，如果是，那么就会提交从上一次轮询中返回的偏移量。\n\n#### 提交当前偏移量\n\n把 `auto.commit.offset` 设置为 false，可以让应用程序决定何时提交偏移量。使用 `commitSync()` 提交偏移量。这个 API 会提交由 poll() 方法返回的最新偏移量，提交成功后马上返回，如果提交失败就抛出异常。\n\ncommitSync() 将会提交由 poll() 返回的最新偏移量，如果处理完所有记录后要确保调用了 commitSync()，否则还是会有丢失消息的风险，如果发生了在均衡，从最近一批消息到发生在均衡之间的所有消息都将被重复处理。\n\n#### 异步提交\n\n异步提交 `commitAsync()` 与同步提交 `commitSync()` 最大的区别在于异步提交不会进行重试，同步提交会一致进行重试。\n\n#### 同步和异步组合提交\n\n一般情况下，针对偶尔出现的提交失败，不进行重试不会有太大的问题，因为如果提交失败是因为临时问题导致的，那么后续的提交总会有成功的。但是如果在关闭消费者或再均衡前的最后一次提交，就要确保提交成功。\n\n因此，**在消费者关闭之前一般会组合使用commitAsync和commitSync提交偏移量**。\n\n#### 提交特定的偏移量\n\n消费者API允许调用 commitSync() 和 commitAsync() 方法时传入希望提交的 partition 和 offset 的 map，即提交特定的偏移量。\n","tags":["消息队列","Kafka"],"categories":["消息队列","Kafka"]},{"title":"如何解决Linux中buff-cache占用过高","url":"/2023/09/08/how-to-solve-the-problem-of-excessive-buffer-cache-usage-in-Linux/","content":"\nLinux中buff-cache占用过高解决手段\n\n<!--more-->\n\n### 简介\n\n    使用free -h命令可以查看当前系统的内存使用情况\n\n```\n      total        used        free      shared  buff/cache   available \nMem:  1.8G        1.4G         66M        952K        313M        211M\nSwap:  0B          0B          0B\n```\n\n> available表示应用程序还可以申请到的内存\n\n首先了解下两个概念buff和cache\n\n> - buff（Buffer Cache）是一种I/O缓存，用于内存和硬盘的缓冲，是io设备的读写缓冲区。根据磁盘的读写设计的，把分散的写操作集中进行，减少磁盘碎片和硬盘的反复寻道，从而提高系统性能。\n\n> - cache（Page Cache）是一种高速缓存，用于CPU和内存之间的缓冲 ,是文件系统的cache。\n>     把读取过的数据保存起来，重新读取时若命中（找到需要的数据）就不要去读硬盘了，若没有命中就读硬盘。其中的数据会根据读取频率进行组织，把最频繁读取的内容放在最容易找到的位置，把不再读的内容不断往后排，直至从中删除。\n\n    它们都是占用内存。两者都是RAM中的数据。简单来说，buff是即将要被写入磁盘的，而cache是被从磁盘中读出来的。\n\n    目前进程正在实际被使用的内存的计算方式为used-buff/cache，通过释放buff/cache内存后，我们还可以使用的内存量free+buff/cache。通常我们在频繁存取文件后，会导致buff/cache的占用量增高。\n\n### 处理方式\n\n#### 手动清除\n\n    执行以下命令即可\n\n```\n[root@izbp17wg1wphb6f95b76obz ~]# sync\n[root@izbp17wg1wphb6f95b76obz ~]# echo 1 > /proc/sys/vm/drop_caches\n[root@izbp17wg1wphb6f95b76obz ~]# echo 2 > /proc/sys/vm/drop_caches\n[root@izbp17wg1wphb6f95b76obz ~]# echo 3 > /proc/sys/vm/drop_caches\n```\n\n- sync：将所有未写的系统缓冲区写到磁盘中，包含已修改的i-node、已延迟的块I/O和读写映射文件\n- echo 1 > /proc/sys/vm/drop_caches：清除page cache\n- echo 2 > /proc/sys/vm/drop_caches：清除回收slab分配器中的对象（包括目录项缓存和inode缓存）。slab分配器是内核中管理内存的一种机制，其中很多缓存数据实现都是用的pagecache。\n- echo 3 > /proc/sys/vm/drop_caches：清除pagecache和slab分配器中的缓存对象。\n    /proc/sys/vm/drop_caches的值,默认为0\n\n#### 定时清除\n\n1、创建脚本cleanCache.sh\n\n```\n#!/bin/bash#每两小时清除一次缓存\necho \"开始清除缓存\"\nsync;sync;sync #写入硬盘，防止数据丢失\nsleep 10#延迟10秒\necho 1 > /proc/sys/vm/drop_caches\necho 2 > /proc/sys/vm/drop_caches\necho 3 > /proc/sys/vm/drop_caches\n```\n\n2、创建定时任务\n\n```\ncrontab -e #弹出配置文件\n```\n\n3、添加定时任务执行频率\n\n```\n#分　 时　 日　 月　 周　 命令\n0 */2 * * * ./cleanCache.sh\n```\n\n4、设置crond启动以及开机自启\n\n```\nsystemctl start crond.service\nsystemctl enable crond.service\n```\n\n5、查看定时任务是否被执行\n\n```\ncat /var/log/cron | grep cleanCache\n```\n\n### 参考资料\n\n- [linux top命令中的cache & buffers](https://blog.csdn.net/Cooling88/article/details/50969013)\n- [buff/cache的问题](https://blog.csdn.net/dyh4201/article/details/85266235)\n- [Linux 内存缓存占用过大，Centos7设置定时清除buff/cache的脚本](http://www.sapv.cn/article/83)\n","tags":["Linux"],"categories":["Linux"]},{"title":"使用TCP和KCP传输一张图片","url":"/2023/08/12/Transfer-a-picture-with-TCP-and-KCP/","content":"\n基于Go分别在两台虚拟机上使用TCP和KCP传输一张图片的demo\n\n<!--more-->\n\n## TCP传输\n\n### TCP服务端\n\n使用服务端接收图片，此虚拟机的ip是192.168.10.107\n\n```go\npackage main\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"os\"\n\t\"time\"\n)\n\nfunc main() {\n\tlis, err := net.Listen(\"tcp\", \"192.168.10.107:10000\")\n\tif err != nil {\n\t\tfmt.Printf(\"Error connecting to server: %s\\n\", err)\n\t\treturn\n\t}\n\tdefer lis.Close()\n\n\t// 创建本地文件用于保存接收的图片数据\n\tfile, err := os.Create(\"received_image.png\")\n\tif err != nil {\n\t\tfmt.Printf(\"Error creating file: %s\\n\", err)\n\t\treturn\n\t}\n\tdefer file.Close()\n\n\t// 创建一个缓冲区，用于接收数据\n\tbuffer := make([]byte, 1024*256)\n\tendSignal := []byte(\"TRANSFER_COMPLETED\")\n\n\tstartTime := time.Now()\n\n\tconn, err := lis.Accept()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer conn.Close()\n\n\tfmt.Println(time.Now())\n\n\tfor {\n\t\tn, err := conn.Read(buffer)\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak // 读取结束\n\t\t\t}\n\t\t\tfmt.Printf(\"Error reading data: %s\\n\", err)\n\t\t\treturn\n\t\t}\n\n\t\t // 检查是否收到结束标志\n\t\t if bytes.Equal(buffer[:n], endSignal) {\n\t\t\tbreak\n\t\t}\n\n\t\t// 将数据写入本地文件\n\t\t_, err = file.Write(buffer[:n])\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Error writing to file: %s\\n\", err)\n\t\t\treturn\n\t\t}\n\n\t}\n\n\tendTime := time.Now()\n\telapsedTime := endTime.Sub(startTime)\n\tprint(elapsedTime.Milliseconds())\n\n\t// 获取文件大小\n\tfileInfo, err := file.Stat()\n\tif err != nil {\n\t\tfmt.Printf(\"Error getting file info: %s\\n\", err)\n\t\treturn\n\t}\n\tfileSize := fileInfo.Size()\n\tfmt.Println(fileSize)\n\n\t// 计算传输速度\n\ttransferSpeed := float64(fileSize) / float64((elapsedTime.Milliseconds() * (1024 * 1024))/1000) // MB/s\n\n\tfmt.Printf(\"File transfer complete. Transfer speed: %.2f MB/s\\n\", transferSpeed)\n\n}\n```\n\n首先监听本地ip和相应端口，获取一个listener，通过listener的Accept方法后就可以获取一个连接，通过连接便可传送数据。先创建一个空文件和缓冲区，进入循环，循环中将连接的数据写入到缓冲区中，获取得到的字节数n，并将数据写入到本地文件，直到遇到传输结束信号\"TRANSFER_COMPLETED\"\n\n### TCP客户端\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"net\"\n\t\"os\"\n\t\"time\"\n)\n\n\nfunc main() {\n\t// kcp\n\t// conn, err := kcp.DialWithOptions(serverAddr, nil, 10, 3)\n\t// tcp\n\tconn, err := net.Dial(\"tcp\", \"192.168.10.107:10000\")\n\tif err!= nil {\n\t\tfmt.Println(err)\n\t}\n\tdefer conn.Close()\n\tif err != nil {\n\t\tlog.Fatal(\"Listen failed:\", err)\n\t}\n\n\tfmt.Println(\"Server started, waiting for connections...\")\n\n\thandleClientWithTCP(conn)\n}\n\nfunc handleClientWithTCP(conn net.Conn) {\n\n\tfile, err := os.Open(\"mountain.png\")\n\tif err != nil {\n\t\tlog.Fatal(\"Create file failed:\", err)\n\t}\n\tdefer file.Close()\n\n\tbuf := make([]byte, 1024*256)\n\n\tfor {\n\t\tn, err := file.Read(buf)\n\t\tif err != nil {\n\t\t\tif err != io.EOF {\n\t\t\t\tlog.Println(\"Read from file failed:\", err)\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\n\t\t_, err = conn.Write(buf[:n])\n\t\tif err != nil {\n\t\t\tlog.Println(\"Write to client failed:\", err)\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// 图片传输完成后发送结束标志\n\t_, err = conn.Write([]byte(\"TRANSFER_COMPLETED\"))\n\tif err != nil {\n\t    fmt.Printf(\"Error sending completion signal: %s\\n\", err)\n\t    return\n\t}\n\t\n\tfmt.Println(time.Now())\n\tfmt.Println(\"Image transfer completed.\")\n}\n```\n\n获取到连接conn后，循环将文件数据file写入到缓冲区中，然后将缓冲区的数据发送到连接中，直到遇到io.EOF跳出循环，发送了结束标志\n\n## KCP传输\n\n### KCP服务端\n\n```go\npackage main\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"github.com/xtaci/kcp-go\"\n\t\"io\"\n\t\"net\"\n\t\"os\"\n\t\"time\"\n)\n\nconst (\n\tserverAddr = \"192.168.10.107:10000\"\n\t\n)\n\nfunc kcpRecv() {\n    // 这里的10， 3是设置了前向纠错\n    // 也可以使用kcp.Listen，与tcp接口兼容 但是默认没有使用前向纠错\n\tlis, err := kcp.ListenWithOptions(serverAddr, nil, 10, 3)\n\tif err != nil {\n\t\tfmt.Printf(\"Error connecting to server: %s\\n\", err)\n\t\treturn\n\t}\n\tdefer lis.Close()\n\n\t// 创建本地文件用于保存接收的图片数据\n\tfile, err := os.Create(\"received_image.png\")\n\tif err != nil {\n\t\tfmt.Printf(\"Error creating file: %s\\n\", err)\n\t\treturn\n\t}\n\tdefer file.Close()\n\n\t// 创建一个缓冲区，用于接收数据\n\tbuffer := make([]byte, 1024*256)\n\tendSignal := []byte(\"TRANSFER_COMPLETED\")\n\n\tstartTime := time.Now()\n\n\tconn, err := lis.AcceptKCP()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer conn.Close()\n\n\tfmt.Println(time.Now())\n\n\tfor {\n\t\tn, err := conn.Read(buffer)\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak // 读取结束\n\t\t\t}\n\t\t\tfmt.Printf(\"Error reading data: %s\\n\", err)\n\t\t\treturn\n\t\t}\n\n\t\t // 检查是否收到结束标志\n\t\t if bytes.Equal(buffer[:n], endSignal) {\n\t\t\tbreak\n\t\t}\n\n\t\t// 将数据写入本地文件\n\t\t_, err = file.Write(buffer[:n])\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"Error writing to file: %s\\n\", err)\n\t\t\treturn\n\t\t}\n\n\t}\n\n\tendTime := time.Now()\n\telapsedTime := endTime.Sub(startTime)\n\n\t// 获取文件大小\n\tfileInfo, err := file.Stat()\n\tif err != nil {\n\t\tfmt.Printf(\"Error getting file info: %s\\n\", err)\n\t\treturn\n\t}\n\tfileSize := fileInfo.Size()\n\n\t// 计算传输速度\n\ttransferSpeed := float64(fileSize) / elapsedTime.Seconds() / (1024 * 1024) // MB/s\n\n\tfmt.Printf(\"File transfer complete. Transfer speed: %.2f MB/s\\n\", transferSpeed)\n\n}\n```\n\nkcp-go与tcp的接口做了兼容处理，可以无障碍替换使用tcp，也增加了一些可选项，如前向纠错，如kcp.ListenWithOptions(serverAddr, nil, 10, 3)的10和3就代表着每发送10个包就发送3个冗余包\n\n### KCP客户端\n\n```go\npackage main\n\nimport (\n\t\"time\"\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\t\"io\"\n\t\"github.com/xtaci/kcp-go\"\n)\n\nconst (\n\tserverAddr = \"192.168.10.107:10000\"\n)\n\nfunc kcpSend() {\n\tconn, err := kcp.DialWithOptions(serverAddr, nil, 10, 3)\n\tif err!= nil {\n\t\tfmt.Println(err)\n\t}\n\tdefer conn.Close()\n\tif err != nil {\n\t\tlog.Fatal(\"Listen failed:\", err)\n\t}\n\n\tfmt.Println(\"Server started, waiting for connections...\")\n\n\thandleClient(conn)\n}\n\nfunc handleClient(conn *kcp.UDPSession) {\n\n\tfile, err := os.Open(\"mountain.png\")\n\tif err != nil {\n\t\tlog.Fatal(\"Create file failed:\", err)\n\t}\n\tdefer file.Close()\n\n\tbuf := make([]byte, 1024*256)\n\n\tfor {\n\t\tn, err := file.Read(buf)\n\t\tif err != nil {\n\t\t\tif err != io.EOF {\n\t\t\t\tlog.Println(\"Read from file failed:\", err)\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\n\t\t_, err = conn.Write(buf[:n])\n\t\tif err != nil {\n\t\t\tlog.Println(\"Write to client failed:\", err)\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// 图片传输完成后发送结束标志\n\t_, err = conn.Write([]byte(\"TRANSFER_COMPLETED\"))\n\tif err != nil {\n\t    fmt.Printf(\"Error sending completion signal: %s\\n\", err)\n\t    return\n\t}\n\n\tfmt.Println(time.Now())\n\tfmt.Println(\"Image transfer completed.\")\n}\n```\n\n","tags":["计算机网络","KCP"],"categories":["计算机网络","KCP"]},{"title":"TCP系列01-概述及协议头格式","url":"/2023/08/11/TCP-series-01-Overview-and-protocol-header-format/","content":"\n对TCP的概述以及对协议头格式的解释\n\n<!--more-->\n\n## 一、TCP简单介绍\n\n​    我们经常听人说TCP是一个面向连接的(connection-oriented)、可靠的(reliable)、字节流式(byte stream)传输协议，  TCP的这三个特性该怎么理解呢？\n\n- 面向连接：在应用TCP协议进行通信之前双方通常需要通过三次握手来建立TCP连接，连接建立后才能进行正常的数据传输，因此广播和多播不会承载在TCP协议上。(谷歌提交了一个RFC文档，建议在TCP三次握手的过程允许SYN数据包中带数据，即 TFO(TCP Fast Open)，ubuntu14.04已经支持该TFO功能)。但是同时面向连接的特性给TCP带来了复杂的连接管理以及用于检测连接状态的存活检测机制。\n- 可靠性：由于TCP处于多跳通信的IP层之上，而IP层并不提供可靠的传输，因此在TCP层看来就有四种常见传输错误问题，分别是比特错误(packet bit errors)、包乱序(packet reordering)、包重复(packet duplication)、丢包(packet erasure或称为packet drops)，TCP要提供可靠的传输，就需要有额外的机制处理这几种错误。因此个人理解可靠性体现在三个方面，首先TCP通过超时重传和快速重传两个常见手段来保证数据包的正确传输，也就是说接收端在没有收到数据包或者收到错误的数据包的时候会触发发送端的数据包重传(处理比特错误和丢包)。其次TCP接收端会缓存接收到的乱序到达数据，重排序后在向应用层提供有序的数据(处理包乱序)。最后TCP发送端会维持一个发送\"窗口\"动态的调整发送速率以适用接收端缓存限制和网络拥塞情况，避免了网络拥塞或者接收端缓存满而大量丢包的问题(降低丢包率)。因此可靠性需要TCP协议具有超时与重传管理、窗口管理、流量控制、拥塞控制等功能。另外TFO下TCP有可能向应用层提供重复的数据，也就是不可靠传输，但是只会发生在连接建立阶段，我们后续会进行介绍。\n- 字节流式：应用层发送的数据会在TCP的发送端缓存起来，统一分片(例如一个应用层的数据包分成两个TCP包)或者打包(例如两个或者多个应用层的数据包打包成一个TCP数据包)发送，到接收端的时候接收端也是直接按照字节流将数据传递给应用层。作为对比，同样是传输层的协议，UDP并不会对应用层的数据包进行打包和分片的操作，一般一个应用层的数据包就对应一个UDP包。这个也是伴随TCP窗口管理、拥塞控制等。\n\n​    在接下来的TCP系列中，我们将会依次介绍TCP协议的连接管理、超时与重传、流量控制、窗口管理、拥塞控制、存活检测等机制。在深入介绍这些内容之前我们先来看一下TCP的封装和协议头的格式(TCP/IP的网络分层等基础网络概念本处不在介绍)\n\n## 二、TCP的封装和协议头的格式\n\n  TCP封装在IP报文中的时候，如下图所示，TCP头紧接着IP头(IPV6有扩展头的时候，则TCP头在扩展头后面)，不携带选项(option)的TCP头长为20bytes，携带选项的TCP头最长可到60bytes。\n\n \n\n![img](TCP-series-01-Overview-and-protocol-header-format/740952-20161107132806686-1082325930.png)\n\n  其中不携带选项的TCP头如下图所示(其中阴影部分的四个字段表示了相反方向的数据流信息)，其中header length字段由4比特构成，最大为15，单位是32比特(32-bit word)，即头长的最大值为15*32 bits = 60bytes，因此上面说携带选项的TCP头长最长为60bytes。\n\n[![img](TCP-series-01-Overview-and-protocol-header-format/740952-20161107132809311-2059896218.png)](http://images2015.cnblogs.com/blog/740952/201611/740952-20161107132809311-2059896218.png)\n\nTCP头中的相关字段顺序解释如下：\n\n  **TCP源端口(Source Port)**：16位的源端口其中包含发送方应用程序对应的端口。源端口和源IP地址标示报文发送端的地址。\n\n  **TCP目的端口(Destination port)**：16位的目的端口域定义传输的目的。这个端口指明报文接收计算机上的应用程序地址接口。\n\nTCP的源端口、目的端口、以及IP层的源IP地址、目的IP地址四元组唯一的标识了一个TCP连接，一个IP地址和一个端口号的组合叫做一个endpoint或者socket。也即一对endpoint或者一对socket唯一的标识了一个TCP连接。接收端的TCP层就是根据不同的端口号来将数据包传送给应用层的不同程序，这个过程叫做解复用(demultiplex)。相应的发送端会把应用层不同程序的数据映射到不同的端口号，这个过程叫做复用(multiplex)。\n\n  **TCP序列号（序列码SN,Sequence　Number）**：32位的序列号标识了TCP报文中第一个byte在对应方向的传输中对应的字节序号。当SYN出现，序列码实际上是初始序列码（ISN），而第一个数据字节是ISN+1，单位是byte。比如发送端发送的一个TCP包净荷(不包含TCP头)为12byte，SN为5，则发送端接着发送的下一个数据包的时候，SN应该设置为5+12=17。通过系列号，TCP接收端可以识别出重复接收到的TCP包，从而丢弃重复包，同时对于乱序数据包也可以依靠系列号进行重排序，进而对高层提供有序的数据流。另外SYN标志和FIN标志在逻辑上也占用一个byte，当SYN标志位有效的时候，该字段也称为ISN(initial sequence number)，详细请参考后续的TCP连接管理。\n\n  **TCP应答号(Acknowledgment  Number简称ACK Number或简称为ACK Field)**：32位的ACK Number标识了报文发送端期望接收的字节序列。如果设置了ACK控制位，这个值表示一个准备接收的包的序列码，注意是准备接收的包，比如当前接收端接收到一个净荷为12byte的数据包，SN为5，则发送端可能会回复一个确认收到的数据包，如果这个数据包之前的数据也都已经收到了，这个数据包中的ACK Number则设置为12+5=17，表示17byte之前的数据都已经收到了。在举一个例子，如果在这个数据包之前有个SN为3，净荷为2byte的数据包丢失，则在接受端接收到这个SN为5的乱序数据包的时候，协议要求接收端必须要回复一个ACK确认包，这个确认包中的Ack Number只能设置为3。\n\n  **头长(Header Length)**：4位包括TCP头大小，指示TCP头的长度，即数据从何处开始。最大为15，单位是32比特(32-bit word)。\n\n  **保留(Reserved)**：4位值域，这些位必须是0。为了将来定义新的用途所保留，其中RFC3540将Reserved字段中的最后一位定义为Nonce标志。后续拥塞控制部分的讲解我们会简单介绍Nonce标志位。\n\n  **标志(Code Bits)**：8位标志位，下面介绍。\n\n  **窗口大小(Window Size)**：16位，该值指示了从Ack Number开始还愿意接收多少byte的数据量，也即用来表示当前接收端的接收窗还有多少剩余空间。用于TCP的流量控制。\n\n  **校验位(Checksum)**：16位TCP头。发送端基于数据内容计算一个数值，接收端要与发送端数值结果完全一样，才能证明数据的有效性。接收端checksum校验失败的时候会直接丢掉这个数据包。CheckSum是根据伪头+TCP头+TCP数据三部分进行计算的。另外对于大的数据包，checksum并不能可靠的反应比特错误，应用层应该再添加自己的校验方式。\n\n  **优先指针（紧急,Urgent Pointer）**：16位，指向后面是优先数据的字节，在URG标志设置了时才有效。如果URG标志没有被设置，紧急域作为填充。加快处理标示为紧急的数据段。\n\n  **选项(Option)**：长度不定，但长度必须以是32bits的整数倍。常见的选项包括MSS、SACK、Timestamp等等，后续的内容会分别介绍相关选项。\n\n \n\n在**标志(Code Bits)**中的八位标志位分别介绍如下\n\n**CWR(Congestion Window Reduce****)**：拥塞窗口减少标志被发送主机设置，用来表明它接收到了设置ECE标志的TCP包，发送端通过降低发送窗口的大小来降低发送速率\n\n**ECE(ECN Echo)**：ECN响应标志被用来在TCP3次握手时表明一个TCP端是具备ECN功能的，并且表明接收到的TCP包的IP头部的ECN被设置为11。更多信息请参考RFC793。\n\n**URG(Urgent)**：该标志位置位表示紧急(The urgent pointer) 标志有效。该标志位目前已经很少使用参考后面流量控制和窗口管理部分的介绍。\n\n**ACK(Acknowledgment)**：取值1代表Acknowledgment Number字段有效，这是一个确认的TCP包，取值0则不是确认包。后续文章介绍中当ACK标志位有效的时候我们称呼这个包为ACK包，使用大写的ACK称呼。\n\n**PSH(Push)**：该标志置位时，一般是表示发送端缓存中已经没有待发送的数据，接收端不将该数据进行队列处理，而是尽可能快将数据转由应用处理。在处理 telnet 或 rlogin 等交互模式的连接时，该标志总是置位的。\n\n**RST(Reset)**：用于复位相应的TCP连接。通常在发生异常或者错误的时候会触发复位TCP连接。\n\n**SYN(Synchronize****)**：同步序列编号(Synchronize Sequence Numbers)有效。该标志仅在三次握手建立TCP连接时有效。它提示TCP连接的服务端检查序列编号，该序列编号为TCP连接初始端(一般是客户端)的初始序列编号。在这里，可以把TCP序列编号看作是一个范围从0到4，294，967，295的32位计数器。通过TCP连接交换的数据中每一个字节都经过序列编号。在TCP报头中的序列编号栏包括了TCP分段中第一个字节的序列编号。类似的后续文章介绍中当这个SYN标志位有效的时候我们称呼这个包为SYN包。\n\n**FIN(Finish)**：带有该标志置位的数据包用来结束一个TCP会话，但对应端口仍处于开放状态，准备接收后续数据。当FIN标志有效的时候我们称呼这个包为FIN包。\n\n  另外我们一般称呼链路层的发出去的数据包为**帧(frame)**，称呼网络层发给链路层的数据包为**包(packet)**，称呼传输层发给网络层的数据包为**段(segment)**。但是正如我们描述所用，段、包、帧也经常统称为数据包或者数据报文。\n\n  对应用层来说TCP是一个双向对称的全双工(full-duplex)协议，也就是说应用层可以同时发送数据和接收数据。这就意味着数据流在一个方向上的传输是独立于另一个方向的传输的，每个方向上都有独立的SN。\n\n## 三、TCP中的数据包窗口和滑窗\n\n  在TCP的发送端和接收端都会维持一个窗口，因为一个TCP连接是双向的，因此实际上一个TCP连接一共有四个窗口。此处我们先简单介绍一个发送端的窗口如下。图中的数字表示byte也就是和上面介绍的TCP协议头中的SN是对应的，3号byte以及3号之前的数据表示已经发送并且收到了接收端的ACK确认包的数据；4、5、6三个byte表示当前可以发送的数据包，也有可能已经已经发送了但是还没有收到ACK确认包；7号byte及之后的数据表示为了控制发送速率暂时不能发送的数据。其中4-6这三个byte就称呼为**窗口大小(window size)**。当TCP连接建立的时候，双方会通过TCP头中的窗口大小字段向对方通告自己接收端的窗口大小，发送端依据接收端通告的窗口大小来设置发送端的发送窗口大小，另外在拥塞控制的时候也是通过调整发送端的发送窗口来调整发送速率的。窗口这个词的来源就是当我们从这一个数据序列中单独看4、5、6这几个byte的时候，我们仿佛是从一个\"窗口\"中观察的一样。此处先简单有个滑窗的概念后续我们讲到TCP的窗口管理的时候会继续进一步介绍TCP的滑窗。\n\n \n\n![img](TCP-series-01-Overview-and-protocol-header-format/740952-20161107132809967-988574516.png)\n\n \n\n相关补充：\n1、TCPIP最初传输层和IP层是同一个层的，关于网络分层的小故事可以参考http://news.cnblogs.com/n/187131/ ， IETF上还专门有一个愚人节系列的RFC，参考[https://en.wikipedia.org/wiki/April_Fools%27_Day_Request_for_Comments](https://en.wikipedia.org/wiki/April_Fools'_Day_Request_for_Comments)\n\n2、TCP头中CheckSum的计算可以参考资料\n\nhttp://www.roman10.net/2011/11/27/how-to-calculate-iptcpudp-checksumpart-1-theory/这个连接里面一共三个系列分别是理论、实现、用例和验证。\n\nhttp://www.tcpipguide.com/free/t_TCPChecksumCalculationandtheTCPPseudoHeader-2.htm \n\n或者参考TCPIP详解卷一第二版(暂时只有英文版，名字TCP/IP Illustrated Volume 1 Second Edition The Protocols) P475页\n\n3、后面涉及的相关RFC文档可以去IETF官网 http://www.ietf.org/查询 直接搜索RFC号码就行了。整个TCP相关的协议体系，IETF梳理后在2015年以RFC7414发布，想了解或者查询TCP相关RFC协议的可以先看看RFC7414协议，以对整个TCP协议有个概括了解\n\n4、目前linux4.4实现上还不支持Nonce标志，可以参考内核代码中*struct tcphdr*结构对TCP头的定义。关于Llinux不同版本内核TCP实现相关的更新可以查看linux的changelog，网址https://kernelnewbies.org/LinuxVersions 其中有每个版本的changelog 还有对应的修改代码\n\n5、TCP传输中的包重复，不见得是TCP重传导致的，也可能是因为IP层提供的不可靠传输导致的 http://stackoverflow.com/questions/12871760/packet-loss-and-packet-duplication\n\n6、在网络传输过程中NAT可能会修改checksum甚至系列号seq，后面我们讨论窗口管理等内容的时候为了简化不考虑nat修改seq的场景，即认为发送端发包的seq与接收端接收这个包的时候seq相同。\n","tags":["TCP","计算机网络"],"categories":["计算机网络","TCP"]},{"title":"使用go-callvis查看函数调用流程","url":"/2023/08/06/how-to-use-go-callvis-to-see-how-funtion-are-made/","content":"\n使用go-callvis查看逻辑复杂的项目的函数调用流程，减少理解成本\n\n<!--more-->\n\n## go-callvis是什么\n\ngo-callvis是代码调用链路可视化工具，是代码方法级别的调用关系，主要用于代码设计。可视化工具可以将代码间的调用关系通过图表的方式展示出来，如下图。\n\n![image-20230807113823511](how-to-use-go-callvis-to-see-how-funtion-are-made/image-20230807113823511.png)\n\n## 如何生成调用关系图\n\ngo-callvis除了可以生成图片文件，还可以生成svg图，它默认会启动一个Web Server，我们可以在浏览器访问它的地址，在页面上实现交互式的浏览调用关系。\n\n### **SVG：**\n\n```text\nSVG是一种用XML定义的语言，用来描述二维矢量及矢量/栅格图形。\nSVG图形是可交互的和动态的，可以在SVG文件中嵌入动画元素或通过脚本来定义动画。\n```\n\n下面是一段SVG代码：\n\n```text\n<g id=\"a_clust3\"><a xlink:href=\"/?f=github.com/goccy/go-graphviz/cgraph\" xlink:title=\"package: github.com/goccy/go&#45;graphviz/cgraph\">\n<polygon fill=\"#ffffe0\" stroke=\"#000000\" stroke-width=\".8\" points=\"861.8909,-442.8 861.8909,-521.8 972.5803,-521.8 972.5803,-442.8 861.8909,-442.8\"/>\n<text text-anchor=\"middle\" x=\"917.2356\" y=\"-503.4\" font-family=\"Tahoma bold\" font-size=\"16.00\" fill=\"#000000\">cgraph</text> \n```\n\n可以看到和HTML类似，同样是一种标记语言。\n\n### **go-callvis使用介绍**\n\n首先使用`go get -u github.com/ofabry/go-callvis` 命令进行安装，安装完成后go-callvis将出现在你得GOPATH/bin目录下。\n\n### **命令行参数解释：**\n\n```text\ngo-callvis: visualize call graph of a Go program.\n\nUsage:\n  go-callvis [flags] package //package即想要进行分析的包名，注意：package必须是main包或者包含单元测试的包，原因稍候介绍\n\n\nFlags:\n\n  -cacheDir string\n        如果指定了缓存目录，生成过的图片将被保存下来，后续使用时不需要再渲染\n  -debug\n        开启调试日志.\n  -file string\n        指定输出文件名，使用后将不在启动Web Server\n  -focus string\n        定位到指定的package，package可以是包名也可以是包的import路径，默认main包\n  -format string\n        指定输出文件格式[svg | png | jpg | ...] (default \"svg\")\n  -graphviz\n        使用本地安装的graphviz的dot命令，否则使用graphviz的go库\n  -group string\n        分组方式： packages and/or types [pkg, type] (separated by comma) (default \"pkg\")\n  -http string\n        Web Server地址. (default \":7878\")\n  -ignore string\n        忽略的packages，多个使用逗号分隔。（使用前缀匹配）\n  -include string\n        必须包含的packages，多个使用逗号分隔。优先级比ignore和limit高（使用前缀匹配）\n  -limit string\n        限制的packages，多个使用逗号分隔（使用前缀匹配）\n  -minlen uint\n        两个节点直接最小连线长度（用于更宽的输出）. (default 2)\n  -nodesep float\n        同一列中两个相邻节点之间的最小空间（用于更高的输出）. (default 0.35)\n  -nodeshape string\n        节点形状 (查看graphvis文档，获取更多可用值) (default \"box\")\n  -nodestyle string\n        节点style(查看graphvis文档，获取更多可用值) (default \"filled,rounded\")\n  -nointer\n        忽略未导出的方法\n  -nostd\n        忽略标准库的方法\n  -rankdir string\n        对齐方式 [LR 调用关系从左到右| RL 从右到左| TB 从上到下| BT 从下到上] (default \"LR\")\n  -skipbrowser\n        不打开浏览器\n  -tags build tags\n        支持传入build tags\n  -tests\n        包含测试代码\n  -version\n        Show version and exit. \n```\n\n### **使用示例**\n\n### **1. 最简单的命令如下：**\n\n```text\ngo-callvis .\n```\n\n此命令会在当前目录进行分析，如果没有错误，会自动打开浏览器，在浏览器中展示图\n\n### **2. 指定package**\n\n```text\ngo-callvis github.com/ofabry/go-callvis\n```\n\n指定的package是main，工具将以main方法作为起始点进行链路生成\n\n### **3. 指定包含单元测试方法的package**\n\n```text\ngo-callvis -tests yourpackage\n```\n\n如果不想从main方法开始，可以使用-tests参数，在想要进行链路生成的package下面创建一个单元测试方法，测试方法中调用你想要作为起始点的方法。\n\n### **4. 输出结果到文件**\n\n以上都是打开浏览器进行交互式浏览和操作，如果只要输出文件，可以使用-file参数\n\n```text\ngo-callvis -file yourfilename -format png  yourpackage\n```\n\n### **5. include、limit、ignore参数**\n\n这三个参数用来控制过滤哪些调用关系（pkg1.FuncA -> pkg2.FuncB，形成一条调用关系，pkg1.FuncA为caller，pkg2.FuncB为callee）。例如代码中频繁出现的log包方法调用，没必要输出到链路中。可以使用ignore参数进行过滤\n\n```text\n go-callvis -ignore yourlogpkg yourpackage\n```\n\n1. 当调用关系中caller的pkg或者callee的pkg有任意一个在include中，则这条关系被保留。\n2. 不满足1时，当调用关系中caller的pkg或者callee的pkg有任意一个不在limit中，则这条关系被过滤。\n3. 不满足1时，当调用关系中caller的pkg或者callee的pkg有任意一个在ignore中，则这条关系被过滤。\n\n### **6. 过滤标准库**\n\n过滤掉代码中频繁使用的标准库方法调用，例如：fmt、math、strings等\n\n```text\n go-callvis -nostd yourpackage\n```\n\n### **7. build tags**\n\ngo build命令可以允许我们传入-tags参数，来控制编译的版本\n\n```text\ngo build -tags release \n```\n\n例如有两个配置文件dev_config.go和release_config.go，内容分别为\n\ndev_config.go\n\n```text\n // +build dev\n\npackage main\n\nvar version = \"DEV\"\n```\n\nrelease_config.go\n\n```text\n// +build release\n\npackage main\n\nconst version = \"RELEASE\"\n```\n\n每个文件都有一个编译选项（+build），编译器会根据-tags传入的参数识别应该编译哪一个文件。从而达到区分环境的效果。\ngo-callvis的tags参数同理。\n\n## 本地代码使用实战\n\n以kcp-go为例\n\n```\ngo-callvis -nostd examples/echo.go\n```\n\n<img src=\"how-to-use-go-callvis-to-see-how-funtion-are-made/image-20230807115955151.png\" alt=\"image-20230807115955151\" style=\"zoom: 67%;\" />\n\nmain包调用情况，点击黄色区域可以查看kcp包的调用情况（会耗时几分钟）\n\n![image-20230807120123267](how-to-use-go-callvis-to-see-how-funtion-are-made/image-20230807120123267.png)\n\n查看函数调用最密集的地方，也是最核心的地方，如下图便是Input，这也是kcp-go中最核心的代码\n\n![image-20230807120312873](how-to-use-go-callvis-to-see-how-funtion-are-made/image-20230807120312873.png)\n\n### 效果图说明\n\n![UntitledImage](how-to-use-go-callvis-to-see-how-funtion-are-made/1926214-20201014100459031-1029465438.png)\n","tags":["go"],"categories":["go"]},{"title":"快速判断一个数是否是2的幂次方","url":"/2023/07/31/quickly-determine-whether-a-number-is-a-power-of-2/","content":"\n如何利用位运算判断一个数是否是2的幂次方，如果是，是2的多少次方\n\n<!--more-->\n\n 将2的幂次方写成二进制形式后，很容易就会发现有一个特点：二进制中只有一个1，并且1后面跟了n个0； 因此问题可以转化为判断1后面是否跟了n个0就可以了。\n\n如果将这个数减去1后会发现，仅有的那个1会变为0，而原来的那n个0会变为1；因此将原来的数与去减去1后的数字进行与运算后会发现为零。\n\n最快速的方法： (number & number - 1) == 0\n\n原因：因为2的N次方换算是二进制为10……0这样的形式(0除外)。与上自己-1的位数，这们得到结果为0。例如。8的二进制为1000；8-1=7，7的二进制为111。两者相与的结果为0。计算如下：\n\n```\n     1000\n   & 0111\n    -------\n     0000\n```\n\n递归实现的代码与非递归实现的代码\n\n```java\npublic class IsTwoPower {\n    public static void main(String[] args) {\n        int n = 1024;\n        if((n & (n - 1) )== 0){\n            System.out.println(\"是2的\" + log2(n) + \"次方\");\n        }else {\n            System.out.println(\"不是2的n次方\");\n        }\n    }\n\n    // 递归实现\n    // 注意跳出递归条件，n == 1返回0效果一样但是多加了一层栈\n    public static int log2_recursion(int n) {\n        if(n == 2) {\n            return 1;\n        }else{\n            return log2_recursion(n >> 1) + 1;\n        }\n    }\n\n    // 非递归实现\n    // 注意跳出循环条件，不要多加\n    public static int log2(int n) {\n        int count = 0;\n        while (n != 1) {\n            n = n >> 1;\n            count ++;\n        }\n        return count;\n    }\n}\n```\n\n扩展1：求一个数n的二进制中的1的个数\n\n了解了(number & number - 1) == 0这个特性以后，我们可以发现利用它可以移除一个数最右边的1，循环移除就可以得到1的个数\n\n```java\n    public static int fun3(int n) {\n        int count = 0;\n        while (n != 0) {\n            n = n & (n - 1);\n            count++;\n        }\n        return count;\n    }\n```\n\n扩展2：数A与数B的二进制位中有多少个数不相同\n\n由于异或的特性，可知只有不同的数异或才为1，那么1的个数就是不相同的个数\n\n```java\n    public static int fun3(int n) {\n        int count = 0;\n        while (n != 0) {\n            n = n & (n - 1);\n            count++;\n        }\n        return count;\n    }\n\n    public static int difference(int i, int j) {\n        int c = i ^ j;\n        return fun3(c);\n    }\n```\n\n","tags":["位运算","Algorithm"],"categories":["位运算"]},{"title":"Cannot Resolve Symbol XXX问题的解决方法汇总","url":"/2023/07/31/Summary-of-solutions-to-the-Cannot-Resolve-Symbol-XXX-Problem/","content":"\n IDEA在使用过程中，会出现各式各样的`Cannot Resolve Symbol xxx`问题，这篇文章总结了所有的情况\n\n<!--more-->\n\n## 第一类:依赖问题\n\n　1、检查项目的pom文件，是否必要的依赖都写清楚了；\n\n　2、是否使用自己的私有库：<repositories>\n\n```\n　<repository>\n　　<id>release</id>\n　　<name>Private  Repository</name>\n　　<url>http://xxxxxx</url>\n　　</repository>\n　　</repositories>\n　　<pluginRepositories>\n　　<pluginRepository>\n　　<id>release</id>\n　　<name>PrivateRepository</name>\n　　<url>http://xxxxxxx</url>\n　　</pluginRepository>\n　　</pluginRepositories>\n```\n\n`pom`文件中有这样的就是使用自身的私有库，库的用户名密码有没有写清楚，一般在maven程序目录的`settings.xml`文件中设置，在<`servers`>标签下设置，类似这样：\n\n```\n<servers>\n　　<server>\n　　<id>nexus</id>\n　　<username>admin</username>\n　　<password>aaaaaaaaaa</password>\n　　</server>\n<servers>　\n```\n\n3、依赖添加正确后，检查本地的类有没有下载下来，一般是找**C:\\Users\\用户名.m2\\repository**或者自己定义的maven仓库目录这个路径下有没有相应的jar包，如果没有的话，就在编译器中打开`Maven Projects` 标签，先进行`clean`一下，再执行`install`，这里与在命令行下执行是一样的效果。\n\n## 第二类:SDK问题\n\n1、 File - Project Structure - Project SDK，看看SDK有没有选，重选一个本地的自己安装的jdk。\n\n2、编译器中的maven有没有设置成功，File - Settings - 搜索maven，Maven home directory，设置为自己安装的maven路径\n\n3、如果还是报错找不到，试试右侧Maven Projects - Report ，刷新样式的按钮，清除下编译器的缓存就好了\n\n## 第三类:POM文件未被正确加载\n\n当pom未被正确加载，module是灰色的，这是新增加的模块没有被正式识别\n\n解决方法：\n\n在idea中点击File–>settings，搜索maven，点击ignored Files，可以看到灰色的模块处于选中状态，去掉选中状态点击保存即可正常使用\n\n![img](Summary-of-solutions-to-the-Cannot-Resolve-Symbol-XXX-Problem/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JlbmJlbmppbGU=,size_16,color_FFFFFF,t_70.png)\n\n","tags":["debug","Spring Boot"],"categories":["debug"]},{"title":"快排及其优化","url":"/2023/07/23/quicksort-optimize/","content":"\n  快排的split实现和partition实现及其优化\n\n<!--more-->\n\n## 快排的思想\n\n快排的核心思想在于数组的划分，，通常我们将数组的第一个元素定义为比较元素，然后将数组中小于比较元素的数放到左边，将大于比较元素的放到右边，\n\n这样我们就将数组拆分成了左右两部分：小于比较元素的数组；大于比较元素的数组。我们再对这两个数组进行同样的拆分，直到拆分到不能再拆分，数组就自然而然地以升序排列了。\n\n![img](quicksort-optimize/1514171-20181123212307575-952364244.png)\n\n## split算法\n\nsplit算法使用一个单向的指针来对数组进行遍历，首先将数组首元素设置为比较元素，然后将第二个开始的元素依次与比较元素比较，如果大于比较元素则跳过，如果小于比较元素，则将其与前面较大的元素进行交换，将数组中所有元素交换完毕后，再将比较元素放到中间位置。简单来说就是让j在前面扫描所有的数，让i记录小于比较元素的值，j扫描到大于比较元素，就往前移动，然后等扫描到小于比较元素的位置就将其交换。\n\n![img](quicksort-optimize/1514171-20181123212456916-1889158192.png)\n\n```java\n//划分数组\n    public static int split(int a[], int low, int high)\n    {\n        int i = low;    //i指向比较元素的期望位置\n        int x = a[low];    //将该组的第一个元素作为比较元素\n        //从第二个元素开始，若当前元素大于比较元素，将其跳过\n        for(int j = low+1; j <= high; j++)\n            //若找到了小于比较元素的元素，将其与前面较大的元素进行交换\n            if(a[j] <= x)\n            {\n                i++;\n                if(i != j)\n                    swap(a, i, j);\n\n            }\n        swap(a, i, low);     //将比较元素交换到正确的位置上\n        return i;    //返回比较元素的位置\n    }\n```\n\n## partition算法\n\npartition算法使用头尾两个方向相反的指针进行遍历，先将数组第一个元素设置为比较元素，头指针从左至右找到第一个大于比较元素的数，尾指针从右至左找到第一个小于比较元素的数，全部交换完毕后将比较元素放到中间位置。\n\n![img](quicksort-optimize/1514171-20181123213830176-1199589796.png)\n\n```\n//划分数组\n    public static int partition(int a[], int low, int high)\n    {\n        int x = a[low];    //将该数组第一个元素设置为比较元素\n        int i=low;\n        int j=high;\n        while(i < j)\n        {\n            while(i<j && a[i] <= x)\n                i++;\n            while(i<j && a[j] >= x)\n                j--;\n\n\n            if(i!=j)\n                swap(a, i, j);\n        }\n        swap(a, j, low);\n        return j;\n    }\n```\n\n## 快排优化方法\n\n当元素排列趋近有序或者有大量的相同元素，那么快排会退化成O(n2)的算法，可以从以下几个方面优化：\n\n- 三数取中法（Median-of-Three Partitioning）：选择子数组中的三个元素（通常是左端、中间和右端），取它们的中值作为枢纽元素。这样可以减少最坏情况发生的概率，提高排序的效率\n- 插入排序优化：在子数组较小（一般小于10个元素）时，采用插入排序而不是继续使用快速排序。插入排序在小数组上的效率较高，可以减少递归的层数，从而提高整体性能\n- 随机化：随机选择枢纽元素，以减少最坏情况发生的概率\n\n### 三点中值法\n\n```\n//三点中值法\n        public static int partition(int[] arr, int p, int r) {\n            //优化，在p, r, mid之间，选一个中间值作为主元\n            int midIndex = p + ((r - p) >> 1);//中间下标\n            int midValueIndex = -1;//中值的下标\n            if(arr[p] <= arr[midIndex] && arr[p] >= arr[r]) {\n                midValueIndex = p;\n            }else if(arr[r] <= arr[midIndex] && arr[r] >= arr[p]) {\n                midValueIndex = r;\n            }else {\n                midValueIndex = midIndex;\n            }\n            swap(arr, p, midValueIndex);\n            int pivot = arr[p];\n            int left = p + 1; //左侧指针\n            int right = r; //右侧指针\n            while(left <= right) {\n                while(left <= right && arr[left] <= pivot) {\n                    left++;\n                }\n                while(left <= right && arr[right] > pivot) {\n                    right--;\n                }\n                if(left < right) {\n                    swap(arr, left, right);\n                }\n            }\n            swap(arr, p, right);\n            return right;\n        }\n```\n\n三值中值和插入排序的实例\n\n```java\nimport java.util.Arrays;\nimport java.util.Random;\n\npublic class OptimizedQuickSort {\n\n    private static final int INSERTION_THRESHOLD = 10;\n\n    public static void quickSort(int[] arr) {\n        quickSort(arr, 0, arr.length - 1);\n    }\n\n    public static void quickSort(int[] arr, int left, int right) {\n        if (right - left <= INSERTION_THRESHOLD) {\n            insertionSort(arr, left, right);\n        } else {\n            int pivotIndex = medianOfThree(arr, left, right);\n            int pivot = arr[pivotIndex];\n            arr[pivotIndex] = arr[right];\n            arr[right] = pivot;\n\n            int i = left - 1;\n            for (int j = left; j < right; j++) {\n                if (arr[j] <= pivot) {\n                    i++;\n                    int temp = arr[i];\n                    arr[i] = arr[j];\n                    arr[j] = temp;\n                }\n            }\n            int temp = arr[i + 1];\n            arr[i + 1] = arr[right];\n            arr[right] = temp;\n\n            quickSort(arr, left, i);\n            quickSort(arr, i + 2, right);\n        }\n    }\n\n    private static int medianOfThree(int[] arr, int left, int right) {\n        int mid = left + (right - left) / 2;\n        if (arr[left] > arr[mid]) {\n            swap(arr, left, mid);\n        }\n        if (arr[left] > arr[right]) {\n            swap(arr, left, right);\n        }\n        if (arr[mid] > arr[right]) {\n            swap(arr, mid, right);\n        }\n        return mid;\n    }\n\n    private static void insertionSort(int[] arr, int left, int right) {\n        for (int i = left + 1; i <= right; i++) {\n            int key = arr[i];\n            int j = i - 1;\n            while (j >= left && arr[j] > key) {\n                arr[j + 1] = arr[j];\n                j--;\n            }\n            arr[j + 1] = key;\n        }\n    }\n\n    private static void swap(int[] arr, int i, int j) {\n        int temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n    }\n\n    public static void main(String[] args) {\n        int[] arr = {38, 27, 43, 3, 9, 82, 10};\n        quickSort(arr);\n        System.out.println(Arrays.toString(arr)); // Output: [3, 9, 10, 27, 38, 43, 82]\n    }\n}\n\n```\n\n","tags":["数据结构","Algorithm"],"categories":["Algorithm"]},{"title":"事务隔离级别实例","url":"/2023/07/15/transaction-isolation-level-demo/","content":"\n以转账实例演示四种隔离级别\n\n<!--more-->\n\n事务隔离级别有四种，读未提交，读已提交，可重复读，串行化\n\n![image-20230715172836733](transaction-isolation-level-demo/image-20230715172836733.png)\n\n![image-20230715173210891](transaction-isolation-level-demo/image-20230715173210891.png)\n\n演示时先打开两个mysql窗口模拟并发场景![image-20230715173021672](transaction-isolation-level-demo/image-20230715173021672.png)\n\n### 未提交读\n\n脏读就是读取到了另一个事务未提交的数据\n\n先将事务隔离级别切换到未提交读\n\n左右窗口都开启了事务，左窗口先查询，结果正常，右窗口事务只进行更新未提交，左窗口又查询，结果变化了，这就是未提交读\n\n![image-20230715173642328](transaction-isolation-level-demo/image-20230715173642328.png)\n\n### 读已提交\n\n将事务隔离级别改为读已提交\n\n![image-20230715193942820](transaction-isolation-level-demo/image-20230715193942820.png)\n\n在进行一次同样的操作\n\n![image-20230715194220234](transaction-isolation-level-demo/image-20230715194220234.png)\n\n左窗口事务再查询就没有发生变化，右窗口事务提交以后在进行查询才发生变化![image-20230715194336414](transaction-isolation-level-demo/image-20230715194336414.png)\n\n读已提交解决了脏读问题，但是又会出现不可重复读问题\n\n比方说还是在刚刚的场景下：左右窗口同时开事务，左窗口事务查询数据，右窗口添加数据，左窗口查询无法查到，右窗口提交以后左窗口再查询就查到了，但是左窗口的事务还未结束，但是查询到的数据却不一样，这就是不可重复读问题\n\n![image-20230715195144084](transaction-isolation-level-demo/image-20230715195144084.png)\n\n### 可重复读\n\n将事务隔离级别切换为可重复读\n\n![image-20230715195315141](transaction-isolation-level-demo/image-20230715195315141.png)\n\n场景还是一样，在同一事务下查询，观察数据是否变化，提交以后数据是否变化![image-20230715195603759](transaction-isolation-level-demo/image-20230715195603759.png)\n\n可以看到左窗口事务提交以后才能看到更新\n\n但是可重复读无法解决幻读\n\n![image-20230715200019711](transaction-isolation-level-demo/image-20230715200019711.png)\n\n### 串行化\n\n开启串行化\n\n![image-20230715200251299](transaction-isolation-level-demo/image-20230715200251299.png)\n\n![image-20230715200422405](transaction-isolation-level-demo/image-20230715200422405.png)\n\n串行化就是并发事务只允许一个事务操作，等一个事务操作完以后再执行另一个事务\n","tags":["MySQL"],"categories":["MySQL"]},{"title":"如何生成火焰图","url":"/2023/06/28/how-to-generate-a-flame-graph/","content":"\n如何使用perf工具和火焰图工具生成火焰图\n\n<!--more-->\n\n火焰图仅用一张小图，就可以定量展示所有的性能瓶颈的全景图，而不论目标软件有多么复杂。\n传统的性能分析工具通常会给用户展示大量的细节信息和数据， 而用户很难看到全貌，反而容易去优化那些并不重要的地方，经常浪费大量时间和精力却看不到明显效果。传统分析器的另一个缺点是，它们通常会孤立地显示每个函数调用的延时，但很难看出各个函数调用的上下文，而且用户还须刻意区分当前函数本身运行的时间（exclusive time）和包括了其调用其他函数的时间在内的总时间（inclusive time）。\n\n而相比之下，火焰图可以把大量信息压缩到一个大小相对固定的图片当中（通常一屏就可以显示全）。 不怎么重要的代码路径会在图上自然地淡化乃至消失，而真正重要的代码路径则会自然地凸显出来。越重要的，则会显示得越明显。火焰图总是为用户提供最适当的信息量，不多，也不少。\n\n## 1 火焰图简介\n\n官方博客：[https://www.brendangregg.com/flamegraphs.html](https://www.brendangregg.com/flamegraphs.html)，火焰图的源资料皆出自该博客。\n\n火焰图能做什么：\n\n- 可以分析函数执行的频繁程度\n- 可以分析哪些函数经常阻塞\n- 可以分析哪些函数频繁分配内存\n\n以分析程序的性能瓶颈。\n\n![flame](how-to-generate-a-flame-graph/flame.png)\n\n火焰图整个图形看起来就像一个跳动的火焰，这就是它名字的由来。\n火焰图有以下特征（这里以 on-cpu 火焰图为例）：\n\n- 每一列代表一个调用栈，每一个格子代表一个函数\n- 纵轴展示了栈的深度，按照调用关系从下到上排列。最顶上格子代表采样时，正在占用 cpu 的函数。\n- 横轴的意义是指：火焰图将采集的多个调用栈信息，通过按字母横向排序的方式将众多信息聚合在一起。需要注意的是它并不代表时间。\n- 横轴格子的宽度代表其在采样中出现频率，所以一个格子的宽度越大，说明它是瓶颈原因的可能性就越大。\n- 火焰图格子的颜色是随机的暖色调，方便区分各个调用信息。\n- 其他的采样方式也可以使用火焰图， on-cpu 火焰图横轴是指 cpu 占用时间，off-cpu 火焰图横轴则代表阻塞时间。\n- 采样可以是单线程、多线程、多进程甚至是多 host，进阶用法可以参考附录进阶阅读。\n\n### 1.1 火焰图类型\n\n常见的火焰图类型有 On-CPU，Off-CPU，还有 Memory，Hot/Cold，Differential 等等。它们有各自适合处理的场景。\n\n\n\n| 火焰图类型     | 横轴含义                  | 纵轴含义 | 解决问题                                                     | 采样方式                                                   |\n| -------------- | ------------------------- | -------- | ------------------------------------------------------------ | ---------------------------------------------------------- |\n| on-cpu火焰图   | cpu占用时间               | 调用栈   | 找出cpu占用搞的问题函数；分析代码热路径                      | 固定频率采样cpu调用栈                                      |\n| off-cpu火焰图  | 阻塞时间                  | 调用栈   | i/o、网络等阻塞场景导致的性能下降；锁竞争、死锁导致的性能下降问题 | 固定频率采样阻塞事件调用栈                                 |\n| 内存火焰图     | 内存申请/释放函数调用次数 | 调用栈   | 内存泄漏问题；内存占用高的对象/申请内存多的函数；虚拟内存或物理内存泄漏问题 | 有四种方式：跟踪malloc/free；跟踪brk；跟踪mmap；跟踪页错误 |\n| Hot/Cold火焰图 | on-cpu和off-cpu综合展示   | 调用栈   | 需要结合cpu占用以及阻塞分析的场景；off-cpu火焰图无法直观判断的场景 | on-cpu火焰图和off-cpu火焰图结合                            |\n|                |                           |          |                                                              |                                                            |\n\n### 1.2 什么时候使用 On-CPU 火焰图? 什么时候使用 Off-CPU 火焰图呢?\n\n取决于当前的瓶颈到底是什么：\n\n- 如果是 CPU 则使用 On-CPU 火焰图,\n- 如果是 IO 或锁则使用 Off-CPU 火焰图.\n- 如果无法确定, 那么可以通过压测工具来确认：\n- 通过压测工具看看能否让 **CPU 使用率趋于饱和**, 如果能那么使用 On-CPU 火焰图\n- 如果不管怎么压, **CPU 使用率始终上不来**, 那么多半说明程序被 IO 或锁卡住了, 此时适合使用 Off-CPU 火焰图.\n- 如果还是确认不了, 那么不妨 On-CPU 火焰图和 Off-CPU 火焰图都搞搞, 正常情况下它们的差异会比较大, 如果两张火焰图长得差不多, 那么通常认为 CPU 被其它进程抢占了\n\n### 1.3 火焰图分析技巧\n\n1. 纵轴代表调用栈的深度（栈桢数），用于表示函数间调用关系：下面的函数是上面函数的父函数。\n2. 横轴代表调用频次，一个格子的宽度越大，越说明其可能是瓶颈原因。\n3. 不同类型火焰图适合优化的场景不同，比如 on-cpu 火焰图适合分析 cpu 占用高的问题函数，off-cpu 火焰图适合解决阻塞和锁抢占问题。\n4. 无意义的事情：横向先后顺序是为了聚合，跟函数间依赖或调用关系无关；火焰图各种颜色是为方便区分，本身不具有特殊含义\n5. 多练习：进行性能优化有意识的使用火焰图的方式进行性能调优（如果时间充裕）\n\n## 2 如何绘制火焰图？\n\n### 2.1 生成火焰图的流程\n\nBrendan D. Gregg 的 Flame Graph 工程实现了一套生成火焰图的脚本。Flame Graph 项目位于 GitHub上\n[https://github.com/brendangregg/FlameGraph](https://github.com/brendangregg/FlameGraph)\n\n当GitHub网络不通畅的时候可以使用码云的链接：\ngit clone [https://gitee.com/mirrors/FlameGraph.git](https://gitee.com/mirrors/FlameGraph.git)\n\n用 git 将其 clone下来\n\n\n生成和创建火焰图需要如下几个步骤\n\n| 流程       | 描述                                                         | 脚本                               |\n| ---------- | ------------------------------------------------------------ | ---------------------------------- |\n| 捕获堆栈   | 使用 perf/systemtap/dtrace 等工具抓取程序的运行堆栈          | perf/systemtap/dtrace              |\n| 折叠堆栈   | trace 工具抓取的系统和程序运行每一时刻的堆栈信息, 需要对他们进行分析组合, 将重复的堆栈累计在一起, 从而体现出负载和关键路径 | FlameGraph 中的 stackcollapse 程序 |\n| 生成火焰图 | 分析 stackcollapse 输出的堆栈信息生成火焰图                  | flamegraph.pl                      |\n|            |                                                              |                                    |\n\n\n\n不同的 trace 工具抓取到的信息不同, 因此 Flame Graph 提供了一系列的 stackcollapse 工具.\n\n| stackcollapse                | 描述                                       |\n| ---------------------------- | ------------------------------------------ |\n| stackcollapse.pl             | for DTrace stacks                          |\n| stackcollapse-perf.pl        | for Linux perf_events “perf script” output |\n| stackcollapse-pmc.pl         | for FreeBSD pmcstat -G stacks              |\n| stackcollapse-stap.pl        | for SystemTap stacks                       |\n| stackcollapse-instruments.pl | for XCode Instruments                      |\n| stackcollapse-vtune.pl       | for Intel VTune profiles                   |\n| stackcollapse-ljp.awk        | for Lightweight Java Profiler              |\n| stackcollapse-jstack.pl      | for Java jstack(1) output                  |\n| stackcollapse-gdb.pl         | for gdb(1) stacks                          |\n| stackcollapse-go.pl          | for Golang pprof stacks                    |\n| stackcollapse-vsprof.pl      | for Microsoft Visual Studio profiles       |\n\n\n**查看帮助**\n`./FlameGraph/flamegraph.pl -h`\n\n### 2.2 安装 perf工具\n\nperf 命令(performance 的缩写)讲起, 它是 Linux 系统原生提供的性能分析工具, 会返回 CPU 正在执行的函数名以及调用栈(stack)\n\n具体用法：\n\n- **perf Examples** [https://www.brendangregg.com/perf.html](https:///www.brendangregg.com/perf.html)\n- **Linux kernel profiling with perf** [https://perf.wiki.kernel.org/index.php/Tutorial](https://perf.wiki.kernel.org/index.php/Tutorial)\n\n#### 2.2.1 安装perf\n\nubuntu下\n\n```console\n# apt install linux-tools-common\n```\n\ncentos下\n\n```\nyum install perf\n```\n\n#### 2.2.2 测试perf是否可用\n\n```bash\n# perf record -F 99 -a -g -- sleep 10    \n```\n\n如果报错\n\n> WARNING: perf not found for kernel **4.15.0-48**\n>\n> You may need to install the following packages for this specific kernel:\n> linux-tools-4.15.0-48-generic\n> linux-cloud-tools-4.15.0-48-generic\n>\n> You may also want to install one of the following packages to keep up to date:\n> linux-tools-generic\n> linux-cloud-tools-generic\n>\n> apt install linux-tools-generic\n> apt install linux-cloud-tools-generic\n\n\n则需要安装linux-tools-generic和linux-cloud-tools-generic，但需选择对应的版本，比如提示的是**4.15.0-48，则我们安装4.15.0-48版本**。\n\n> \\# apt-get install linux-tools-4.15.0-48-generic linux-cloud-tools-4.15.0-48-generic linux-tools-generic linux-cloud-tools-generic\n\n再次测试\n\n```bash\n# perf record -F 99 -a -g -- sleep 10    \n```\n\n如果没有报错则在执行的目录产生perf.data\n\n![image-20230806204937168](how-to-generate-a-flame-graph/image-20230806204937168.png)\n\n#### 2.2.3 perf常用命令\n\n查看帮助文档，perf功能非常强大，我们这里只关注record和report功能，record和report也可以继续通过二级命令查询帮助文档。\n\n**perf -h**\n\n常用的五个命令：\n\n- perf list：查看当前软硬件环境支持的性能事件\n- perf stat：分析指定程序的性能概况\n- perf top：实时显示系统/进程的性能统计信息\n- **perf record**：记录一段时间内系统/进程的性能事件perf report：读取perf record生成的perf.data文件，并显示分析数据（生成火焰图用的采集命令）\n- perf report：交互式命令查看资源使用情况\n\n### 2.3 perf 采集数据\n\n```\nperf record -F 99 -p 3887 -g -- sleep 30\n```\n\nperf record 表示采集系统事件, 没有使用 -e 指定采集事件, 则默认采集 cycles(即 CPU clock 周期), -F 99 表示每秒 99 次, -p 13204 是进程号, 即对哪个进程进行分析, -g 表示记录调用栈, sleep 30 则是持续 30 秒.\n\n一定要加上 - g，否则无法记录数据！\n\n> -F 指定采样频率为 99Hz(每秒99次), 如果 99次 都返回同一个函数名, 那就说明 CPU 这一秒钟都在执行同一个函数, 可能存在性能问题.\n\n运行后会产生一个庞大的文本文件. 如果一台服务器有 16 个 CPU, 每秒抽样 99 次, 持续 30 秒, 就得到 47,520 个调用栈, 长达几十万甚至上百万行.\n\n为了便于阅读, perf record 命令可以统计每个调用栈出现的百分比, 然后从高到低排列.\n\n```\nperf report -n --stdio\n```\n\n### 2.4 生成火焰图\n\n1. 首先用 **perf script** 工具对 perf.data 进行解析\n\n> \\# 生成折叠后的调用栈\n> perf script -i perf.data &> perf.unfold\n\n这里在任意文件夹都可使用\n\n2. 然后将解析出来的信息存下来, 供生成火焰图\n\n用 **stackcollapse-perf.pl** 将 perf 解析出的内容 **perf.unfold** 中的符号进行折叠 :\n\n> \\# 在FlameGraph同路径下的文件夹生成火焰图\n> \\# ./FlameGraph/stackcollapse-perf.pl perf.unfold &> perf.folded\n>\n> 或者使用全路径\n>\n> /opt/rh/FlameGraph/stackcollapse-perf.pl perf.unfold &> perf.folded\n\n3. 最后生成 svg 图\n\n> \\# ./FlameGraph/flamegraph.pl perf.folded > perf.svg\n>\n> /opt/rh/FlameGraph/flamegraph.pl perf.folded > perf.svg\n\n## 3.1 火焰图的含义\n\n火焰图是基于 stack 信息生成的 SVG 图片, 用来展示 CPU 的调用栈。\n\n- y 轴表示调用栈, 每一层都是一个函数. 调用栈越深, 火焰就越高, 顶部就是正在执行的函数, 下方都是它的父函数.\n- x 轴表示抽样数, 如果一个函数在 x 轴占据的宽度越宽, 就表示它被抽到的次数多, 即执行的时间长. 注意, x 轴不代表时间, 而是所有的调用栈合并后, 按字母顺序排列的.\n- 火焰图就是看顶层的哪个函数占据的宽度最大. 只要有 “平顶”(plateaus), 就表示该函数可能存在性能问题。\n- 颜色没有特殊含义, 因为火焰图表示的是 CPU 的繁忙程度, 所以一般选择暖色调.\n\n## 4 互动\n\n火焰图是 SVG 图片，可以与用户互动\n\n火焰的每一层都会标注函数名，鼠标悬浮时会显示完整的函数名、抽样抽中的次数、占据总抽样次数的百分比。下面是一个例子\n\n在某一层点击，火焰图会水平放大，该层会占据所有宽度，显示详细信息。\n\n![image-20230806210451419](how-to-generate-a-flame-graph/image-20230806210451419.png)\n","tags":["perf"],"categories":["perf"]},{"title":"go中slice扩容机制","url":"/2023/06/12/slice-expansion-mechanism-in-go/","content":"\nGo1.18前后扩容机制的区别\n\n<!--more-->\n\n## Go1.17及以前\n\n### 扩容机制\n\n过去的扩容机制主要分为两个过程：第一步是分配新的内存空间，第二步是将原有切片内容进行复制。分配新空间时候需要估计大致容量，然后再确定容量。\n\n根据该切片当前容量选择不同的策略：\n\n- 如果期望容量大于当前容量的两倍，就会使用期望容量\n- 如果当前切片的长度小于 1024，容量就会翻倍\n- 如果当前切片的长达大于 1024，每次扩容 25% 的容量，直到新容量大于期望容量\n- 在进行循环1.25倍计算时，最终容量计算值发生溢出，即超过了int的最大范围，则最终容量就是新申请的容量\n\n对于切片的扩容\n\n- 当切片比较小的，采用较大的扩容倍速进行扩容，避免频繁扩容，从而减少内存分配的次数和数据拷贝的代价\n- 当切片较大的时，采用较小的扩容倍速，主要避免空间浪费\n\n![image-20230812151240785](slice-expansion-mechanism-in-go/image-20230812151240785.png)\n\n### 旧规则存在的问题\n\n我们知道，slice扩容时会调用`runtime.growslice`函数(不熟悉slice底层原理的同学可以先看看这篇[《Go语言切片剖析》](https://mp.weixin.qq.com/s/vIX5NHtDTaqAu2WVUwn6WA))。这里我们只关注该函数slice计算容量部分的逻辑，计算方法如下:\n\n```go\n// runtime/slice.go\n// et：表示slice的一个元素；old：表示旧的slice； cap：表示新切片需要的容量；\nfunc growslice(et *_type, old slice, cap int) slice {\n\tif cap < old.cap {\n\t\tpanic(errorString(\"growslice: cap out of range\"))\n\t}\n\n\tif et.size == 0 {\n\t\t// append should not create a slice with nil pointer but non-zero len.\n\t\t// We assume that append doesn't need to preserve old.array in this case.\n\t\treturn slice{unsafe.Pointer(&zerobase), old.len, cap}\n\t}\n\n\tnewcap := old.cap\n        // 两倍扩容\n\tdoublecap := newcap + newcap\n        // 新切片需要的容量大于当前容量的两倍，则直接按照新切片需要的容量扩容\n\tif cap > doublecap {\n\t\tnewcap = cap\n\t} else {\n        // 原 slice 容量小于 1024 的时候，新 slice 容量按2倍扩容\n\t\tif old.cap < 1024 {\n\t\t\tnewcap = doublecap\n\t\t} else { // 原 slice 容量超过 1024，新 slice 容量变成原来的1.25倍。\n\t\t\t// Check 0 < newcap to detect overflow\n\t\t\t// and prevent an infinite loop.\n\t\t\tfor 0 < newcap && newcap < cap {\n\t\t\t\tnewcap += newcap / 4\n\t\t\t}\n\t\t\t// Set newcap to the requested cap when\n\t\t\t// the newcap calculation overflowed.\n\t\t\tif newcap <= 0 {\n\t\t\t\tnewcap = cap\n\t\t\t}\n\t\t}\n\t}\n\n        // 后半部分还对 newcap 作了一个内存对齐，这个和内存分配策略相关。进行内存对齐之后，新 slice 的容量是要 大于等于 老 slice 容量的 2倍或者1.25倍。\n\tvar overflow bool\n\tvar lenmem, newlenmem, capmem uintptr\n\t// Specialize for common values of et.size.\n\t// For 1 we don't need any division/multiplication.\n\t// For sys.PtrSize, compiler will optimize division/multiplication into a shift by a constant.\n\t// For powers of 2, use a variable shift.\n\tswitch {\n\tcase et.size == 1:\n\t\tlenmem = uintptr(old.len)\n\t\tnewlenmem = uintptr(cap)\n\t\tcapmem = roundupsize(uintptr(newcap))\n\t\toverflow = uintptr(newcap) > maxAlloc\n\t\tnewcap = int(capmem)\n\tcase et.size == sys.PtrSize:\n\t\tlenmem = uintptr(old.len) * sys.PtrSize\n\t\tnewlenmem = uintptr(cap) * sys.PtrSize\n\t\tcapmem = roundupsize(uintptr(newcap) * sys.PtrSize)\n\t\toverflow = uintptr(newcap) > maxAlloc/sys.PtrSize\n\t\tnewcap = int(capmem / sys.PtrSize)\n\tcase isPowerOfTwo(et.size):\n\t\tvar shift uintptr\n\t\tif sys.PtrSize == 8 {\n\t\t\t// Mask shift for better code generation.\n\t\t\tshift = uintptr(sys.Ctz64(uint64(et.size))) & 63\n\t\t} else {\n\t\t\tshift = uintptr(sys.Ctz32(uint32(et.size))) & 31\n\t\t}\n\t\tlenmem = uintptr(old.len) << shift\n\t\tnewlenmem = uintptr(cap) << shift\n\t\tcapmem = roundupsize(uintptr(newcap) << shift)\n\t\toverflow = uintptr(newcap) > (maxAlloc >> shift)\n\t\tnewcap = int(capmem >> shift)\n\tdefault:\n\t\tlenmem = uintptr(old.len) * et.size\n\t\tnewlenmem = uintptr(cap) * et.size\n\t\tcapmem, overflow = math.MulUintptr(et.size, uintptr(newcap))\n\t\tcapmem = roundupsize(capmem)\n\t\tnewcap = int(capmem / et.size)\n\t}\n}\n\n```\n\n打印扩容的容量\n\n```go\npackage main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    for i := 0; i < 2000; i += 100 {\n        fmt.Println(i, cap(append(make([]bool, i), true)))\n    }\n}\n```\n\n该程序的输出如下(旧版本的扩容规则):\n\n```\n// 第一列是切片的旧容量\n// 第二列是扩容后的容量\n0 8\n100 208\n200 416\n300 640\n400 896\n500 1024\n600 1280\n700 1408\n800 1792\n900 2048\n1000 2048\n1100 1408 <-- 在这个点，扩容后的新容量比上面的容量要小\n1200 1536\n1300 1792\n1400 1792\n1500 2048\n1600 2048\n1700 2304\n1800 2304\n1900 2688\n```\n\n![image-20230812152116742](slice-expansion-mechanism-in-go/image-20230812152116742.png)\n\n## Go1.18后：更加平滑的扩容算法\n\n从`go1.18`开始，slice容量的计算方法被改为了这样:\n\n```go\n// 只关心扩容规则的简化版growslice\nfunc growslice(old, cap int) int {\n    newcap := old\n    doublecap := newcap + newcap\n    if cap > doublecap {\n        newcap = cap\n    } else {\n        const threshold = 256 // 不同点1\n        if old.cap < threshold {\n            newcap = doublecap\n        } else {\n            for 0 < newcap && newcap < cap {\n                newcap += (newcap + 3*threshold) / 4 // 不同点2\n            }\n            if newcap <= 0 {\n                newcap = cap\n            }\n        }\n    }\n    return newcap\n}\n```\n\n首先是双倍容量扩容的最大阈值**从1024降为了256**，只要超过了256，就开始进行缓慢的增长。其次是增长比例的调整，之前超过了阈值之后，基本为恒定的1.25倍增长，而现在超过了阈值之后，增长比例是会动态调整的。\n\n![image-20230812152404106](slice-expansion-mechanism-in-go/image-20230812152404106.png)\n\n## 内存对齐\n\n分析完两个版本的扩容策略之后，再看前面的那段测试代码，就会发现扩容之后的容量并不是严格按照这个策略的。\n\n那是为什么呢？\n\n实际上，growslice 的后半部分还有更进一步的优化（内存对齐等），靠的是 roundupsize 函数，在计算完 newcap 值之后，还会有一个步骤计算最终的容量：\n\n```\ncapmem = roundupsize(uintptr(newcap) * ptrSize)\nnewcap = int(capmem / ptrSize)\n```\n\n","tags":["go"],"categories":["go"]},{"title":"DDD架构中数据传递的过程","url":"/2023/05/11/the-process-of-data-transfer-in-DDD-architecture/","content":"\nDDD架构中如何domain层和infrastructure如何传递数据，以及作用\n\n<!--more-->\n\n### 传统的三层架构\n\n传统的三层架构包括表示层、业务逻辑层和数据访问层，它的设计目标是将系统划分为独立的、高内聚、低耦合的模块，使得每个模块的职责清晰、易于维护和升级。在这种架构中，业务逻辑层是系统的核心，它处理所有的业务逻辑，而表示层和数据访问层则分别负责与用户交互和访问数据。\n\n![1895018-20230314115040094-875486601](the-process-of-data-transfer-in-DDD-architecture/1895018-20230314115040094-875486601-16843216712372.png)\n\nMVC模式中业务逻辑层直接与数据访问层交互，导致与数据层面有较强的依赖关系，以致于不便后续数据层面的更新，并且也会导致自身的庞大和臃肿，不便后续的维护。\n\n### 领域驱动设计的四层架构\n\n![2-02-16826858766012](the-process-of-data-transfer-in-DDD-architecture/2-02-16826858766012.png)\n\n领域驱动设计的四层架构则更加强调领域模型的设计和实现，它包括表示层、应用层、领域层和基础设施层。在这种架构中，领域层是系统的核心，它包含了系统的领域模型和业务逻辑，而应用层负责协调各个领域层的操作，提供应用程序的服务接口，表示层负责与用户交互，基础设施层负责提供与外部系统的交互和数据存储等服务。\n\n领域层只关注业务逻辑的实现，提供领域数据的接口，而不关注具体数据如何操作，基础层实现领域层提供的接口，配合DAO和po完成具体的方法。\n\n#### 领域层包结构\n\n![image-20230517192910187](the-process-of-data-transfer-in-DDD-architecture/image-20230517192910187.png)\n\nactivity, award,strategy是三个领域，领域下在repository包中定义具体的数据接口。\n\n#### 基础层包结构\n\ndao包含数据访问逻辑，po是持久化对象，repository是对领域层提供的接口的实现\n\n![image-20230517193502810](the-process-of-data-transfer-in-DDD-architecture/image-20230517193502810.png)\n\n在infrastructure层下实现这些接口，DAO做数据层面的交互，处理数据访问逻辑，可以实现解耦合。\n\n并且，仓储层可以在领域模型的操作之上实施领域规则。这意味着在执行数据访问操作之前或之后，可以在仓储中添加额外的逻辑来保持领域模型的一致性和完整性。\n\n![image-20230517200449992](the-process-of-data-transfer-in-DDD-architecture/image-20230517200449992.png)\n\n仓储层可以负责将基础设施层的数据映射为领域模型所需的数据结构，以及将领域模型的数据转换为基础设施层所需的格式。这种转换和映射逻辑可以在仓储中集中处理，减少领域模型与基础设施层之间的耦合。如转换成聚合根。\n\n![image-20230517200645985](the-process-of-data-transfer-in-DDD-architecture/image-20230517200645985.png)\n\n过在仓储层中定义接口，可以实现技术的灵活性，使得可以更换底层数据访问技术或使用不同的数据存储策略，而无需修改领域模型。同时，仓储接口也有助于编写针对领域模型的单元测试，因为可以使用模拟或存根实现来模拟数据访问。\n","tags":["设计模式"],"categories":["设计模式"]},{"title":"理清@Autowired和@Resource的区别和联系","url":"/2023/04/17/difference-between-resource-and-autowired/","content":"\n使用方法以及区别联系\n\n<!--more-->\n\n### 联系\n\n@Autowired和@Resource注解都是作为bean对象注入的时候使用的\n两者都可以声明在字段和setter方法上\n注意：如果声明在字段上，那么就不需要再写setter方法。但是本质上，该对象还是作为set方法的实参，通过执行set方法注入，只是省略了setter方法罢了\n\n### 区别\n\n@Autowired注解是Spring提供的，而@Resource注解是J2EE本身提供的，JDK1.6之后才能使用\n@Autowird注解默认通过byType方式注入，而@Resource注解默认通过byName方式注入\n@Autowired注解注入的对象需要在IOC容器中存在，否则需要加上属性required=false，表示忽略当前要注入的bean，如果有直接注入，没有跳过，不会报错\n\n#### 什么是byType，什么是byName\n\n```\n<bean id=\"userService\" class=\"com.test.UserServiceImpl\">\n</bean> \n```\n\n```\n@Autowired\nprivate UserService userService;\n```\n\n此处byName就是拿变量名userService去匹配IOC容器的iduserService，匹配成功；而byType就是拿变量类型UserService去匹配IOC容器的idcom.test.UserService.UserServiceImpl，因为UserServiceImpl是UserService实现，所以也匹配成功\n\n### @Autowird注解的使用\n\n@Autowird默认的注入方式为byType，也就是根据类型匹配，当有多个实现时，则通过byName注入，也可以通过配合@Qualifier注解来显式指定name值，指明要使用哪个具体的实现类\n\n举例：\n\n首先有一个接口UserService和两个实现类UserServiceImpl1和UserServiceImpl2，并且这两个实现类已经加入到Spring的IOC容器中了\n\n```\n@Service\npublic class UserServiceImpl1 implements UserService\n\n@Service\npublic class UserServiceImpl2 implements UserService\n```\n\n通过@Autowired注入使用\n\n```\n@Autowired\nprivate UserService userService;\n```\n\n根据上面的步骤，可以很容易判断出，直接这么使用是会报错的\n原因：首先通过byType注入，判断UserService类型有两个实现，无法确定具体是哪一个，于是通过byName方式，这里的变量名userService也无法匹配IOC容器中id（此处指的userServiceImpl1和userServiceImpl2），于是报错。\n\n注意：通过注解注入到IOC容器的id值默认是其类名首字母小写\n\n解决方案\n\n方式一：\n\n// 方式一：改变变量名\n\n```\n@Autowired\nprivate UserService userServiceImpl1;\n```\n\n方式二：\n\n// 方式二：配合@Qualifier注解来显式指定name值\n\n```\n@Autowired\n@Qualifier(value = \"userServiceImpl1\")\nprivate UserService userService;\n```\n\n### @Resource注解的使用\n\n步骤：@Resource默认通过byName注入，如果没有匹配则通过byType注入\n\n举例：\n\n```\n@Service\npublic class UserServiceImpl1 implements UserService\n\n@Service\npublic class UserServiceImpl2 implements UserService\n\n@Resource\nprivate UserService userService;\n```\n\n首先通过byName匹配，变量名userService无法匹配IOC容器中任何一个id（这里指的userServiceImpl1和userServiceImpl2），于是通过byType匹配，发现类型UserService的实现类有两个，仍然无法确定，于是BeanCreationException异常。\n\n同时@Resource还有两个重要的属性：name和type，用来显式指定byName和byType方式注入\n\n#### @Resource装配顺序\n\n```\n/ 1. 默认方式：byName\n@Resource  \nprivate UserService userDao; \n\n// 2. 指定byName\n@Resource(name=\"userService\")  \nprivate UserService userService; \n\n// 3. 指定byType\n@Resource(type=UserService.class)  \nprivate UserService userService; \n\n// 4. 指定byName和byType\n@Resource(name=\"userService\",type=UserService.class)  \nprivate UserService userService; \n```\n\n既没指定name属性，也没指定type属性：默认通过byName方式注入，如果byName匹配失败，则使用byType方式注入（也就是上面的那个例子）\n指定name属性：通过byName方式注入，把变量名和IOC容器中的id去匹配，匹配失败则报错\n指定type属性：通过byType方式注入，在IOC容器中匹配对应的类型，如果匹配不到或者匹配到多个则报错\n同时指定name属性和type属性：在IOC容器中匹配，名字和类型同时匹配则成功，否则失败\n","tags":["Spring Boot","Java"],"categories":["Java","Spring Boot"]},{"title":"git使用过程中遇到的问题以及解决方法","url":"/2023/04/09/the-usage-of-git/","content":"\n记录了git使用过程中遇到的问题以及解决方法\n\n<!--more-->\n\n### git stash暂存文件\n\n当前分支还未完成所有的工作，然而又需要切换分支或者不能继续工作，可以采用`git stash`命令，切换回来以后再采用`git stash pop`命令切回\n\n在idea 2020版中使用`git stash`命令有几率丢失文件\n\n所以也可以commit 之后再切换分支然后再reset\n\n### git pull拉取分支\n\n在团队开发时，可能出现增加了其他文件和修改统一文件的情况，此时需要`git pull`先合并再push，若不想合并则先fetch\n\n### git pull和git fetch的区别\n\n#### 目的不同\n\n**git fetch**：从远程获取最新版本到本地，但不会自动 merge，用于从远程跟踪分支下载和查看其他人完成的最新提交，但不将这些提交合并到本地存储库中。它从远程存储库中获取更改并将其存储在本地存储库中。\n\n**git pull**：从远程获取最新版本并 merge 到本地，它会自动将提交合并到您的本地存储库中，而无需查看提交。\n\n#### 用途不同\n\n**git fetch**：Fetch 只是通过将提交从远程存储库传输到本地存储库来使远程存储库的本地副本保持最新。将提交导入到本地分支将允许您跟上其他人所做的更改。\n\n**git pull**：Pull 将更改引入本地代码存储库，以使用远程存储库更新本地存储库。\n\n#### 用法不同\n\n**git fetch**：当您想要查看其他人正在处理的内容时，Fetch 命令非常有用，这使您可以在将更改与本地存储库集成之前轻松查看其他开发人员推送的提交。您可以通过使用命令“git fetch ”来做到这一点，该命令从远程存储库中获取所有分支。\n\n**git pull**：您可以使用命令“git pull ”来执行拉取，该命令检索分支的远程副本并将其与本地副本合并。这与使用命令“git fetch ”后跟“git merge ”完全相同。\n\n#### 远端跟踪分支不同\n\n**git fetch**：Git fetch能够直接更改远端跟踪分支。\n\n**git pull**：git pull无法直接对远程跟踪分支操作，我们必须先切回本地分支然后创建一个新的commit提交。\n\n### 使用 git revert 回滚某次的提交\n\n想象这么一个场景，你的项目最近有2个版本要上线，这两个版本还伴随着之前遗留的 bug 的修复，一开始的时候，你将 bug 修复在了第一个版本的 release 分支上，突然在发版前一天，测试那边反馈，需要把第一个版本修复 bug 的内容改在第二个版本上，这个时候，第一个版本的集成分支的提交应该包括了第一个版本的功能内容，遗留 bug 修复的提交和其他同事提交的内容，想要通过 reset 的方式粗暴摘除之前的关于 bug 修复的 commit 肯定是不行的，同时，这种做法比较危险，此时，我们既不想破坏之前的提交记录，又想撤回我们遗留 bug 的 commit 记录应该怎么做呢？git revert 就派上了用场。\n\n `git revert <commit-id>` 针对普通 commit\n\n`git revert <commit-id> -m` 针对 merge 的 commit\n\n下面就用一个案例来理解一下这个命令，如下图所示，假设被红框框起来的地方是会引起 bug 的一次提交，在他的提交之后，又进行了2次提交，其中包含了其它同事的提交。\n\n![image-20210519142702752.png](the-usage-of-git/f36331158e084072a033802bf4fa0478tplv-k3u1fbpfcp-zoom-in-crop-mark4536000.awebp)\n\n此时想把引起提交的 bug 的干掉，执行 `git revert 1121932`，执行操作后，再打开查看日志，如下图所示，可以看到是新增了一条 commit 记录，这个 commit 的产生的 msg 是自动生成的，Revert 开头，后面跟撤回的 commit-msg 信息 之前的 commit 记录并没有消失，此时也达到了代码回退的效果\n\n![image-20210519142824836.png](the-usage-of-git/9729e537218e4609b54df3e899fd332ftplv-k3u1fbpfcp-zoom-in-crop-mark4536000.awebp)\n\n此外 git revert 也可以回滚多次的提交\n\n语法：`git revert [commit-id1] [commit-id2] ...`  注意这是一个前开后闭区间，即不包括 commit1 ，但包括 commit2 。\n\n回滚我们的提交有二种方式，一种是上文提到的`git revert`命令外，还可以使用 `git reset` 命令，那么它们两者有什么区别呢？\n\n`git revert` 会新建一条 commit 信息，来撤回之前的修改。\n\n`git reset` 会直接将提交记录退回到指定的 commit 上。\n\n对于个人的 feature 分支而言，可以使用 `git reset` 来回退历史记录，之后使用 `git push --force` 进行推送到远程，但是如果是在多人协作的集成分支上，不推荐直接使用 `git reset` 命令，而是使用更加安全的 `git revert` 命令进行撤回提交。这样，提交的历史记录不会被抹去，可以安全的进行撤回。\n","tags":["git"],"categories":["git"]},{"title":"使用go实现一个简单的高性能RPC","url":"/2023/04/02/implment-a-high-performance-rpc-with-go/","content":"\nRPC是远程过程调用（**Remote Procedure Call**）的缩写形式。RPC调用的原理其实很简单，它类似于三层构架的C/S系统，第三方的客户程序通过接口调用RPC内部的标准或自定义函数，获得函数返回的数据进行处理后显示或打印。\n\n<!--more-->\n\n[TinyRPC](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc) 是基于Go语言标准库 **net/rpc** 扩展的远程过程调用框架，它具有以下特性：\n\n- 基于TCP传输层协议\n- 支持多种**压缩格式**：gzip、snappy、zlib；\n- 基于二进制的 **Protocol Buffer** 序列化协议：具有协议编码小及高扩展性和跨平台性；\n- 支持生成工具：TinyRPC提供的 **protoc-gen-tinyrpc** 插件可以帮助开发者快速定义自己的服务；\n- 支持自定义序列化器\n\n[TinyRPC](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc) 的源代码仅有一千行左右，通过学习 **TinyRPC** ，开发者可以得到以下收获：\n\n- 代码简洁规范\n- 涵盖大多数 Go 语言基础用法和高级特性\n- 单元测试编写技巧\n- TCP流中处理数据包的技巧\n- RPC框架的设计理念\n\n## 基于TCP的TinyRPC协议\n\n在TinyRPC中，请求消息由TinyRPC客户端的应用程序发出，在TCP的字节流中，请求消息分为三部分：\n\n- 由可变长量编码的 **uint 类型**用来标识请求头的长度；\n- 基于自定义协议编码的请求头部信息\n- 基于 **Protocol Buffer** 协议编码的请求体，见图所示：\n\n![img](implment-a-high-performance-rpc-with-go/v2-62b7995089963eb1477a66476c18f3f3_b.jpg)\n\n在TinyRPC中，响应消息由TinyRPC服务端的应用程序响应，在TCP的字节流中，响应消息分为三部分：\n\n- 由可变长量编码的 **uint 类型**用来标识响应头的长度；\n- 基于自定义协议编码的响应头部信息\n- 基于 **Protocol Buffer** 协议编码的响应体，见图所示：\n\n![img](implment-a-high-performance-rpc-with-go/v2-512d68763da8ca61ebbaa6735485a768_b.jpg)\n\n其中ID为RPC调用的序号，以便在并发调用时，客户端根据响应的ID序号来判断RPC的调用结果；\n\nError message为调用时发生错误的消息，若该内容为空则表示未出现RPC调用错误；\n\n在请求I/O流中，请求体（Request Body）表示RPC的参数内容；而在响应I/O流中，响应体（Response Body）则表示RPC调用的结果，这些Body在TinyRPC中均采用 **Protocol Buffer** 协议编码。\n\n## 请求头部消息编码\n\n由于[TinyRPC](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc)的请求头部是自定义协议编码的，我们可以查看文件[header/header.go](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc/blob/main/header/header.go)了解它的细节：\n\n```go\n// CompressType type of compressions supported by rpc\ntype CompressType uint16\n\n// RequestHeader request header structure looks like:\n// +--------------+----------------+----------+------------+----------+\n// | CompressType |      Method    |    ID    | RequestLen | Checksum |\n// +--------------+----------------+----------+------------+----------+\n// |    uint16    | uvarint+string |  uvarint |   uvarint  |  uint32  |\n// +--------------+----------------+----------+------------+----------+\ntype RequestHeader struct {\n        sync.RWMutex\n\tCompressType CompressType  // 它表示RPC的协议内容的压缩类型，TinyRPC支持四种压缩类型，Raw、Gzip、Snappy、Zlib\n\tMethod       string  // 方法名\n\tID           uint64  // 请求ID\n\tRequestLen   uint32  // 请求体长度\n\tChecksum     uint32  // 请求体校验 使用CRC32摘要算法\n}\n```\n\n其中 RequestHeader 的编解码过程如下所示：\n\n```go\n\n// Marshal will encode request header into a byte slice\nfunc (r *RequestHeader) Marshal() []byte {\n        r.RLock()\n\tdefer r.RUnlock()\n\tidx := 0\n\theader := make([]byte, MaxHeaderSize+len(r.Method))\n        // 写入uint16类型的压缩类型\n\tbinary.LittleEndian.PutUint16(header[idx:], uint16(r.CompressType))\n\tidx += Uint16Size\n        \n\tidx += writeString(header[idx:], r.Method)\n\tidx += binary.PutUvarint(header[idx:], r.ID)  // 写入uvarint类型的请求ID号\n\tidx += binary.PutUvarint(header[idx:], uint64(r.RequestLen)) // 写入uvarint类型的请求体长度\n\n\tbinary.LittleEndian.PutUint32(header[idx:], r.Checksum)  // 写入uvarint类型的校验码\n\tidx += Uint32Size\n\treturn header[:idx]\n}\n\n// Unmarshal will decode request header into a byte slice\nfunc (r *RequestHeader) Unmarshal(data []byte) (err error) {\n\tr.Lock()\n\tdefer r.Unlock()\n        if len(data) == 0 {\n\t\treturn UnmarshalError\n\t}\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = UnmarshalError\n\t\t}\n\t}()\n\tidx, size := 0, 0\n\tr.CompressType = CompressType(binary.LittleEndian.Uint16(data[idx:]))\n\tidx += Uint16Size // 读取uint16类型的压缩类型\n\n\tr.Method, size = readString(data[idx:])\n\tidx += size\n\n\tr.ID, size = binary.Uvarint(data[idx:]) // 读取uvarint类型的请求ID号\n\tidx += size\n\n\tlength, size := binary.Uvarint(data[idx:])   // 读取uvarint类型的请求体长度\n\tr.RequestLen = uint32(length)\n\tidx += size\n\n\tr.Checksum = binary.LittleEndian.Uint32(data[idx:]) // 读取uvarint类型的校验码\n\treturn\n}\n\nfunc readString(data []byte) (string, int) {\n\tidx := 0\n\tlength, size := binary.Uvarint(data)  // 读取一个uvarint类型表示字符的长度\n\tidx += size\n\tstr := string(data[idx : idx+int(length)])\n\tidx += len(str)\n\treturn str, idx\n}\n\nfunc writeString(data []byte, str string) int {\n\tidx := 0\n\tidx += binary.PutUvarint(data, uint64(len(str)))  // 写入一个uvarint类型表示字符长度\n\tcopy(data[idx:], str)\n\tidx += len(str)\n\treturn idx\n}\n```\n\n## 响应头部消息编码\n\n由于[TinyRPC](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc)的响应头部是自定义协议编码的，我们可以查看文件[header/header.go](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc/blob/main/header/header.go)了解它的细节：\n\n```go\n// ResponseHeader request header structure looks like:\n// +--------------+---------+----------------+-------------+----------+\n// | CompressType |    ID   |      Error     | ResponseLen | Checksum |\n// +--------------+---------+----------------+-------------+----------+\n// |    uint16    | uvarint | uvarint+string |    uvarint  |  uint32  |\n// +--------------+---------+----------------+-------------+----------+\ntype ResponseHeader struct {\n        sync.RWMutex\n\tCompressType CompressType  // 压缩类型\n\tID           uint64  // 响应ID号\n\tError        string  // 错误信息\n\tResponseLen  uint32  // 响应体长度\n\tChecksum     uint32  // 响应体校验码\n}\n```\n\n其中 ResponseHeader 的编解码过程如下所示，与RequestHeader 的编解码过程类似：\n\n```go\n\n// Marshal will encode response header into a byte slice\nfunc (r *ResponseHeader) Marshal() []byte {\n        r.RLock()\n\tdefer r.RUnlock()\n\tidx := 0\n\theader := make([]byte, MaxHeaderSize+len(r.Error))\n\n\tbinary.LittleEndian.PutUint16(header[idx:], uint16(r.CompressType))\n\tidx += Uint16Size\n\n\tidx += binary.PutUvarint(header[idx:], r.ID)\n\tidx += writeString(header[idx:], r.Error)\n\tidx += binary.PutUvarint(header[idx:], uint64(r.ResponseLen))\n\n\tbinary.LittleEndian.PutUint32(header[idx:], r.Checksum)\n\tidx += Uint32Size\n\treturn header[:idx]\n}\n\n// Unmarshal will decode response header into a byte slice\nfunc (r *ResponseHeader) Unmarshal(data []byte) (err error) {\n        r.Lock()\n\tdefer r.Unlock()\n\tif len(data) == 0 {\n\t\treturn UnmarshalError\n\t}\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = UnmarshalError\n\t\t}\n\t}()\n\tidx, size := 0, 0\n\tr.CompressType = CompressType(binary.LittleEndian.Uint16(data[idx:]))\n\tidx += Uint16Size\n\n\tr.ID, size = binary.Uvarint(data[idx:])\n\tidx += size\n\n\tr.Error, size = readString(data[idx:])\n\tidx += size\n\n\tlength, size := binary.Uvarint(data[idx:])\n\tr.ResponseLen = uint32(length)\n\tidx += size\n\n\tr.Checksum = binary.LittleEndian.Uint32(data[idx:])\n\treturn\n}\n```\n\n## 头部消息对象池\n\n为了减少创建请求头部对象 **RequestHeader** 和响应头部对象 **ResponseHeader** 的次数**，**我们通过为这两个结构体建立对象池，以便可以进行复用。\n\n同时我们为 *RequestHeader* 和 *ResponseHeader* 都实现了ResetHeader方法，当每次使用完这些对象时，我们调用ResetHeader让结构体内容初始化，随后再把它们丢回对象池里。\n\n代码 [header/pool.go](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc/blob/main/header/pool.go) 如下：\n\n```go\n\npackage header\n\nimport \"sync\"\n\nvar (\n\tRequestPool  sync.Pool\n\tResponsePool sync.Pool\n)\n\nfunc init() {\n\tRequestPool = sync.Pool{New: func() any {\n\t\treturn &RequestHeader{}\n\t}}\n\tResponsePool = sync.Pool{New: func() any {\n\t\treturn &ResponseHeader{}\n\t}}\n}\n\n// ResetHeader reset request header\nfunc (h *RequestHeader) ResetHeader() {\n\th.Id = 0\n\th.Checksum = 0\n\th.Method = \"\"\n\th.CompressType = 0\n\th.RequestLen = 0\n}\n\n// ResetHeader reset response header\nfunc (h *ResponseHeader) ResetHeader() {\n\th.Error = \"\"\n\th.Id = 0\n\th.CompressType = 0\n\th.Checksum = 0\n\th.ResponseLen = 0\n}\n```\n\n## IO操作\n\nTinyRPC的IO操作函数在[codec/io.go](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc/blob/main/codec/io.go)中，其中 *sendFrame* 函数会向IO中写入**uvarint**类型的 *size* ，表示要发送数据的长度，随后将该字节slice类型的数据 *data* 写入IO流中。\n\n- 若写入数据的长度为 0 ，此时*sendFrame* 函数会向IO流写入uvarint类型的 0 值；\n- 若写入数据的长度大于 0 ，此时*sendFrame* 函数会向IO流写入uvarint类型的 *len(data)* 值，随后将该字节串的数据 data 写入IO流中。\n\n代码如下所示：\n\n```go\nfunc sendFrame(w io.Writer, data []byte) (err error) {\n\tvar size [binary.MaxVarintLen64]byte\n\n\tif data == nil || len(data) == 0 {\n\t\tn := binary.PutUvarint(size[:], uint64(0))\n\t\tif err = write(w, size[:n]); err != nil {\n\t\t\treturn\n\t\t}\n\t\treturn\n\t}\n\n\tn := binary.PutUvarint(size[:], uint64(len(data)))\n\tif err = write(w, size[:n]); err != nil {\n\t\treturn\n\t}\n\tif err = write(w, data); err != nil {\n\t\treturn\n\t}\n\treturn\n}\n\nfunc write(w io.Writer, data []byte) error {\n\tfor index := 0; index < len(data); {\n\t\tn, err := w.Write(data[index:])\n\t\tif _, ok := err.(net.Error); !ok {\n\t\t\treturn err\n\t\t}\n\t\tindex += n\n\t}\n\treturn nil\n}\n```\n\n*recvFrame* 函数与*sendFrame* 函数类似，首先会向IO中读入**uvarint**类型的 *size* ，表示要接收数据的长度，随后将该从IO流中读取该 *size* 长度字节串。\n\n> 注意，由于 codec 层会传入一个**bufio**类型的结构体，**bufio**类型实现了有缓冲的IO操作，以便减少IO在用户态与内核态拷贝的次数。\n\n- 若 *recvFrame* 函数从IO流读取**uvarint**类型的 *size* 值大于0，随后 *recvFrame* 将该从IO流中读取该 *size* 长度字节串。\n\n```go\nfunc recvFrame(r io.Reader) (data []byte, err error) {\n\tsize, err := binary.ReadUvarint(r.(io.ByteReader))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif size != 0 {\n\t\tdata = make([]byte, size)\n\t\tif err = read(r, data); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn data, nil\n}\n\nfunc read(r io.Reader, data []byte) error {\n\tfor index := 0; index < len(data); {\n\t\tn, err := r.Read(data[index:])\n\t\tif err != nil {\n\t\t\tif _, ok := err.(net.Error); !ok {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tindex += n\n\t}\n\treturn nil\n}\n```\n\n## TinyRPC的压缩器\n\nTinyRPC的压缩器代码部分很短，RawCompressor、GzipCompressor、SnappyCompressor、ZlibCompressor压缩器均实现了Compressor 接口，代码[compressor/compressor.go](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc/blob/main/compressor/compressor.go)：\n\n```go\ntype CompressType int32\n\nconst (\n\tRaw CompressType = iota\n\tGzip\n\tSnappy\n\tZlib\n)\n// Compressors 四种压缩器的实现\nvar Compressors = map[CompressType]Compressor{\n\tRaw:    RawCompressor{},\n\tGzip:   GzipCompressor{},\n\tSnappy: SnappyCompressor{},\n\tZlib:   ZlibCompressor{},\n}\n// Compressor 压缩器接口\ntype Compressor interface {\n\tZip([]byte) ([]byte, error)\n\tUnzip([]byte) ([]byte, error)\n}\n\n```\n\n## TinyRPC的序列化器\n\nTinyRPC的序列化器的代码部分也很短，ProtoSerializer实现了Serializer接口，它是基于Protocol Buffer的序列化协议，代码[serializer/serializer.go](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc/blob/main/serializer/serializer.go)：\n\n```go\ntype SerializeType int32\n\ntype Serializer interface {\n\tMarshal(message any) ([]byte, error)\n\tUnmarshal(data []byte, message any) error\n}\n```\n\n## 实现ClientCodec接口\n\n由于TinyRPC是**基于标准库net/rpc**扩展的，所以TinyRPC在codec层需要实现**net/rpc**的**ClientCodec**接口，我们先看看**ClientCodec**的代码：\n\n```go\n// 文件 src/net/rpc/server.go\n\ntype ClientCodec interface {\n\tWriteRequest(*Request, any) error\n\tReadResponseHeader(*Response) error\n\tReadResponseBody(any) error\n\n\tClose() error\n}\n// Request 标准库里的请求体结构\ntype Request struct {\n\tServiceMethod string \n\tSeq           uint64 \n\tnext          *Request \n}\n// Response 标准库里的响应结构\ntype Response struct {\n\tServiceMethod string \n\tSeq           uint64 \n\tError         string \n\tnext          *Response \n}\n```\n\n其中ClientCodec接口包括**写请求**、**读响应头部**和**读响应体**，我们建立一个clientCode的结构体用来实现ClientCodec接口：\n\n代码 [codec/client.go](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc/blob/main/codec/client.go) 如下：\n\n```go\ntype clientCodec struct {\n\tr io.Reader\n\tw io.Writer\n\tc io.Closer\n\n\tcompressor compressor.CompressType // rpc compress type(raw,gzip,snappy,zlib)\n\tserializer serializer.Serializer\n\tresponse   header.ResponseHeader // rpc response header\n\tmutex      sync.Mutex            // protect pending map\n\tpending    map[uint64]string\n}\n```\n\n其中 *compressor* 表示压缩类型，*serializer* 表示使用的序列化器，*response* 是响应的头部，*mutex* 是用于保护 *pending* 的互斥锁；\n\n```go\n// NewClientCodec Create a new client codec\nfunc NewClientCodec(conn io.ReadWriteCloser,\n\tcompressType compressor.CompressType, serializer serializer.Serializer) rpc.ClientCodec {\n\n\treturn &clientCodec{\n\t\tr:          bufio.NewReader(conn),\n\t\tw:          bufio.NewWriter(conn),\n\t\tc:          conn,\n\t\tcompressor: compressType,\n\t\tserializer: serializer,\n\t\tpending:    make(map[uint64]string),\n\t}\n}\n```\n\n这里的读写IO分别使用 *bufio.NewReader* 和 *bufio.NewWriter* 构造，通过缓冲IO来提高RPC的读写性能；\n\n首先 *clientCode* 结构体实现了 ***ClientCodec*** 接口的***WriteRequest*** 方法：\n\n```go\nfunc (c *clientCodec) WriteRequest(r *rpc.Request, param any) error {\nc.mutex.Lock()\n\tc.pending[r.Seq] = r.ServiceMethod\n\tc.mutex.Unlock()\n          // 判断压缩器是否存在\n\tif _, ok := compressor.Compressors[c.compressor]; !ok {\n\t\treturn NotFoundCompressorError\n\t}\n\treqBody, err := c.serializer.Marshal(param) // 用序列化器进行编码\n\tif err != nil {\n\t\treturn err\n\t}\n        // 压缩\n\tcompressedReqBody, err := compressor.Compressors[c.compressor].Zip(reqBody)\n\tif err != nil {\n\t\treturn err\n\t}\n        // 从请求头部对象池取出请求头\n\th := header.RequestPool.Get().(*header.RequestHeader)\n\tdefer func() {\n\t\th.ResetHeader()\n\t\theader.RequestPool.Put(h)\n\t}()\n\th.ID = r.Seq\n\th.Method = r.ServiceMethod\n\th.RequestLen = uint32(len(compressedReqBody))\n\th.CompressType = header.CompressType(c.compressor)\n\th.Checksum = crc32.ChecksumIEEE(compressedReqBody)\n        // 发送请求头\n\tif err := sendFrame(c.w, h.Marshal()); err != nil {\n\t\treturn err\n\t}\n        // 发送请求体\n\tif err := write(c.w, compressedReqBody); err != nil {\n\t\treturn err\n\t}\n\n\tc.w.(*bufio.Writer).Flush()\n\treturn nil\t\n}\n```\n\n实现 ***ClientCodec\\*** 接口的 ***ReadResponseHeader\\*** 方法：\n\n```go\nfunc (c *clientCodec) ReadResponseHeader(r *rpc.Response) error {\n        c.response.ResetHeader() // 重置clientCodec的响应头部\n\tdata, err := recvFrame(c.r) // 读取请求头字节串  \n\tif err != nil {\n\t\treturn err\n\t}\n\terr = c.response.Unmarshal(data) // 用序列化器继续解码\n\tif err != nil {\n\t\treturn err\n\t}\n\tc.mutex.Lock()\n\tr.Seq = c.response.ID // 填充 r.Seq  \n\tr.Error = c.response.Error // 填充 r.Error\n\tr.ServiceMethod = c.pending[r.Seq]  // 根据序号填充 r.ServiceMethod\n\tdelete(c.pending, r.Seq) // 删除pending里的序号  \n\tc.mutex.Unlock()\n\treturn nil\t\n}\n```\n\n实现 ***ClientCodec\\*** 接口的 ***ReadResponseBody\\*** 方法：\n\n```go\n\nfunc (c *clientCodec) ReadResponseBody(param any) error {\n        if param == nil {\n\t\tif c.response.ResponseLen != 0 {   // 废弃多余部分\n\t\t\tif err := read(c.r, make([]byte, c.response.ResponseLen)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n        // 根据响应体长度，读取该长度的字节串\n\trespBody := make([]byte, c.response.ResponseLen)\n\terr := read(c.r, respBody)\n\tif err != nil {\n\t\treturn err\n\t}\n        // 校验\n\tif c.response.Checksum != 0 {\n\t\tif crc32.ChecksumIEEE(respBody) != c.response.Checksum {\n\t\t\treturn UnexpectedChecksumError\n\t\t}\n\t}\n        // 判断压缩器是否存在\n\tif _, ok := compressor.Compressors[c.response.GetCompressType()]; !ok {\n\t\treturn NotFoundCompressorError\n\t}\n        // 解压\n\tresp, err := compressor.Compressors[c.response.GetCompressType()].Unzip(respBody)\n\tif err != nil {\n\t\treturn err\n\t}\n        // 反序列化\n\treturn c.serializer.Unmarshal(resp, param)\n}\n```\n\n## 实现ServerCodec接口\n\nTinyRPC在codec层还需要实现**net/rpc**的**ServerCodec**接口\n\n*ServerCodec* 的接口和 *ClientCodec* 接口十分类似：\n\n```go\ntype ServerCodec interface {\n\tReadRequestHeader(*Request) error\n\tReadRequestBody(any) error\n\tWriteResponse(*Response, any) error\n\tClose() error\n}\n```\n\n其中 *ServerCodec* 接口包括**写响应**、**读请求头部**和**读请求体**，我们建立一个 *serverCodec* 的结构体用来实现 *ServerCodec* 接口，代码[codec/server.go](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc/blob/main/codec/server.go)：\n\n```go\ntype serverCodec struct {\n\tr io.Reader\n\tw io.Writer\n\tc io.Closer\n\n\trequest    header.RequestHeader\n\tserializer serializer.Serializer\n\tmutex      sync.Mutex // protects seq, pending\n\tseq        uint64\n\tpending    map[uint64]uint64\n}\n\n// NewServerCodec Create a new server codec\nfunc NewServerCodec(conn io.ReadWriteCloser, serializer serializer.Serializer) rpc.ServerCodec {\n\treturn &serverCodec{\n\t\tr:          bufio.NewReader(conn),\n\t\tw:          bufio.NewWriter(conn),\n\t\tc:          conn,\n\t\tserializer: serializer,\n\t\tpending:    make(map[uint64]uint64),\n\t}\n}\n```\n\n是不是和刚才的 *clientCode* 结构体**神似**？\n\n首先， *serverCodec* 结构体实现了 *ServerCodec* 接口的 *ReadRequestHeader*方法：\n\n```go\nfunc (s *serverCodec) ReadRequestHeader(r *rpc.Request) error {\n\ts.request.ResetHeader()  // 重置serverCodec结构体的请求头部\n\tdata, err := recvFrame(s.r) // 读取请求头部字节串\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = s.request.Unmarshal(data)   //将字节串反序列化\n\tif err != nil {\n\t\treturn err\n\t}\n\ts.mutex.Lock()\n\ts.seq++  // 序号自增\n\ts.pending[s.seq] = s.request.ID // 自增序号与请求头部的ID进行绑定\n\tr.ServiceMethod = s.request.Method  // 填充 r.ServiceMethod \n\tr.Seq = s.seq// 填充 r.Seq  \n\ts.mutex.Unlock()\n\treturn nil\t\n}\n```\n\n实现 *ServerCodec* 接口的 *ReadRequestBody* 方法：\n\n```go\nfunc (s *serverCodec) ReadRequestBody(x any) error {\n       if param == nil { \n\t\tif s.request.RequestLen != 0 {  // 废弃多余部分\n\t\t\tif err := read(s.r, make([]byte, s.request.RequestLen)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\treqBody := make([]byte, s.request.RequestLen)\n        // 根据请求体的大小，读取该大小的字节串\n\terr := read(s.r, reqBody)\n\tif err != nil {\n\t\treturn err\n\t}\n        // 校验\n\tif s.request.Checksum != 0 {\n\t\tif crc32.ChecksumIEEE(reqBody) != s.request.Checksum {\n\t\t\treturn UnexpectedChecksumError\n\t\t}\n\t}\n         // 判断压缩器是否存在\n\tif _, ok := compressor.\n\t\tCompressors[s.request.GetCompressType()]; !ok {\n\t\treturn NotFoundCompressorError\n\t}\n        // 解压\n\treq, err := compressor.\n\t\tCompressors[s.request.GetCompressType()].Unzip(reqBody)\n\tif err != nil {\n\t\treturn err\n\t}\n        // 把字节串反序列化\n\treturn s.serializer.Unmarshal(req, param)\n}\n```\n\n实现 *ServerCodec* 接口的 *WriteResponse* 方法：\n\n```go\nfunc (s *serverCodec) WriteResponse(r *rpc.Response, param any) error {\n\ts.mutex.Lock()\n\tid, ok := s.pending[r.Seq]\n\tif !ok {\n\t\ts.mutex.Unlock()\n\t\treturn InvalidSequenceError\n\t}\n\tdelete(s.pending, r.Seq)\n\ts.mutex.Unlock()\n\n\tif r.Error != \"\" {   // 如果RPC调用结果有误，把param置为nil\n\t\tparam = nil\n\t}\n         // 判断压缩器是否存在\n\tif _, ok := compressor.\n\t\tCompressors[s.request.GetCompressType()]; !ok {\n\t\treturn NotFoundCompressorError\n\t}\n        \n\tvar respBody []byte\n\tvar err error\n\tif param != nil {\n\t\trespBody, err = s.serializer.Marshal(param) // 反序列化\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n        // 压缩\n\tcompressedRespBody, err := compressor.\n\t\tCompressors[s.request.GetCompressType()].Zip(respBody)\n\tif err != nil {\n\t\treturn err\n\t}\n\th := header.ResponsePool.Get().(*header.ResponseHeader)\n\tdefer func() {\n\t\th.ResetHeader()\n\t\theader.ResponsePool.Put(h)\n\t}()\n\th.ID = id\n\th.Error = r.Error\n\th.ResponseLen = uint32(len(compressedRespBody))\n\th.Checksum = crc32.ChecksumIEEE(compressedRespBody)\n\th.CompressType = s.request.CompressType\n        // 发送响应头\n\tif err = sendFrame(s.w, h.Marshal()); err != nil {\n\t\treturn err\n\t}\n        // 发送响应体\n\tif err = write(s.w, compressedRespBody); err != nil {\n\t\treturn err\n\t}\n\ts.w.(*bufio.Writer).Flush()\n\treturn nil\n}\n```\n\n## TinyRPC的Server\n\nTinyRPC的服务端非常简单，把标准库 **net/rpc** 的 **Server** 结构包装了一层，其中 *ServeCodec* 使用的是TinyRPC的编解码器，代码[server.go](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc/blob/main/server.go)：\n\n```go\ntype Server struct {\n\t*rpc.Server\n\tserializer.Serializer\n}\n\n...\n\nfunc (s *Server) Serve(lis net.Listener) {\n\tfor {\n\t\tconn, err := lis.Accept()\n\t\tif err != nil {\n\t\t\tlog.Print(\"tinyrpc.Serve: accept:\", err.Error())\n\t\t\treturn\n\t\t}\n\t\tgo s.Server.ServeCodec(codec.NewServerCodec(conn, s.Serializer)))  // 使用TinyRPC的解码器\n\t}\n}\n```\n\n## TinyRPC的Client\n\nTinyRPC的客户端也很简单，把标准库 **net/rpc** 的 **Client** 结构包装了一层，其中 *ClientCodec* 使用的是TinyRPC的编解码器，代码[client.go](https://link.zhihu.com/?target=https%3A//github.com/zehuamama/tinyrpc/blob/main/client.go)：\n\n> 注意：TinyRPC Client使用一种Go语言常用的设计模式：功能选项模式\n\n```go\n// Client rpc client based on net/rpc implementation\ntype Client struct {\n\t*rpc.Client\n}\n\n//Option provides options for rpc\ntype Option func(o *options)\n\ntype options struct {\n\tcompressType compressor.CompressType\n\tserializer   serializer.Serializer\n}\n\n// WithCompress set client compression format\nfunc WithCompress(c compressor.CompressType) Option {\n\treturn func(o *options) {\n\t\to.compressType = c\n\t}\n}\n\n// WithSerializer set client serializer\nfunc WithSerializer(serializer serializer.Serializer) Option {\n\treturn func(o *options) {\n\t\to.serializer = serializer\n\t}\n}\n\n// NewClient Create a new rpc client\nfunc NewClient(conn io.ReadWriteCloser, opts ...Option) *Client {\n\toptions := options{\n\t\tcompressType: compressor.Raw,\n\t\tserializer:   serializer.Proto,\n\t}\n\tfor _, option := range opts {\n\t\toption(&options)\n\t}\n\treturn &Client{rpc.NewClientWithCodec(\n\t\tcodec.NewClientCodec(conn, options.compressType, options.serializer))}\n}\n\n// Call synchronously calls the rpc function\nfunc (c *Client) Call(serviceMethod string, args interface{}, reply interface{}) error {\n\treturn c.Client.Call(serviceMethod, args, reply)\n}\n\n// AsyncCall asynchronously calls the rpc function and returns a channel of *rpc.Call\nfunc (c *Client) AsyncCall(serviceMethod string, args interface{}, reply interface{}) chan *rpc.Call {\n\treturn c.Go(serviceMethod, args, reply, nil).Done\n}\n```\n\n作者：马丸子\n链接：https://zhuanlan.zhihu.com/p/499098284\n来源：知乎\n","tags":["go"],"categories":["go"]},{"title":"how to fix Incorrect credentials. Request response. 401 Unauthorized","url":"/2023/03/23/how-to-fix-Incorrect-credentials-Request-response-401-Unauthorized/","content":"\n使用GitHub生成的token登录idea时，无法登录，报了如标题错误\n\n<!--more-->\n\n这是因为生成token时未勾选相应的权限\n\n请务必勾选以下权限\n\n```\nrepo - select everything\ngist - select everything\norg - select only read:org\n```\n\n勾选以后即可使用GitHub登录idea了\n","tags":["debug","git"],"categories":["debug"]},{"title":"redis实战:短信登录","url":"/2023/03/21/redis-in-action-SMS-login/","content":"\n本文介绍了如何使用session进行登录及其缺点，以及如何用redis对缺点进行改进\n\n<!--more-->\n\n## 基于session实现登录流程\n\n### **发送验证码：**\n\n用户在提交手机号后，会校验手机号是否合法，如果不合法，则要求用户重新输入手机号\n\n如果手机号合法，后台此时生成对应的验证码，同时将验证码进行保存，然后再通过短信的方式将验证码发送给用户\n\n### **短信验证码登录、注册：**\n\n用户将验证码和手机号进行输入，后台从 session 中拿到当前验证码，然后和用户输入的验证码进行校验，如果不一致，则无法通过校验，如果一致，则后台根据手机号查询用户，如果用户不存在，则为用户创建账号信息，保存到数据库，无论是否存在，都会将用户信息保存到 session 中，方便后续获得当前登录信息\n\n### **校验登录状态:**\n\n用户在请求时候，会从 cookie 中携带sessionId 到后台，后台通过 sessionId 从 session 中拿到用户信息，如果没有 session 信息，则进行拦截，如果有 session 信息，则将用户信息保存到 threadLocal 中，并且放行\n\n![登录流程](redis-in-action-SMS-login/登录流程.png)\n\n## 实现发送短信验证码功能\n\n![短信验证码](redis-in-action-SMS-login/短信验证码.png)\n\n代码如下：\n\n```\n    @Override\n    public Result sendCode(String phone, HttpSession session) {\n        // 1.校验手机号\n        if (RegexUtils.isPhoneInvalid(phone)) {\n            // 2.如果不符合，返回错误信息\n            return Result.fail(\"手机号格式错误！\");\n        }\n        // 3.符合，生成验证码\n        String code = RandomUtil.randomNumbers(6);\n \n        // 4.保存验证码到 session\n        session.setAttribute(\"code\",code);\n        // 5.发送验证码\n        log.debug(\"发送短信验证码成功，验证码：{}\", code);\n        // 返回ok\n        return Result.ok();\n    }\n```\n\n登录：\n\n```\n    @Override\n    public Result login(LoginFormDTO loginForm, HttpSession session) {\n        // 1.校验手机号\n        String phone = loginForm.getPhone();\n        if (RegexUtils.isPhoneInvalid(phone)) {\n            // 2.如果不符合，返回错误信息\n            return Result.fail(\"手机号格式错误！\");\n        }\n        // 3.校验验证码\n        Object cacheCode = session.getAttribute(\"code\");\n        String code = loginForm.getCode();\n        if(cacheCode == null || !cacheCode.toString().equals(code)){\n             //3.不一致，报错\n            return Result.fail(\"验证码错误\");\n        }\n        //一致，根据手机号查询用户\n        User user = query().eq(\"phone\", phone).one();\n \n        //5.判断用户是否存在\n        if(user == null){\n            //不存在，则创建\n            user =  createUserWithPhone(phone);\n        }\n        //7.保存用户信息到session中\n        session.setAttribute(\"user\",user);\n \n        return Result.ok();\n    }\n```\n\n## 实现登录拦截功能\n\n![登录拦截](redis-in-action-SMS-login/登录拦截.png)\n\n当用户发起请求时，会访问我们像 tomcat 注册的端口，任何程序想要运行，都需要有一个线程对当前端口号进行监听，tomcat 也不例外，当监听线程知道用户想要和 tomcat 连接连接时，那会由监听线程创建 socket 连接，socket 都是成对出现的，用户通过 socket 像互相传递数据，当 tomcat 端的 socket 接受到数据后，此时监听线程会从 tomcat 的线程池中取出一个线程执行用户请求，在我们的服务部署到 tomcat 后，线程会找到用户想要访问的工程，然后用这个线程转发到工程中的 controller，service，dao 中，并且访问对应的 DB，在用户执行完请求后，再统一返回，再找到 tomcat 端的 socket，再将数据写回到用户端的 socket，完成请求和响应\n\n通过以上讲解，我们可以得知 每个用户其实对应都是去找 tomcat 线程池中的一个线程来完成工作的， 使用完成后再进行回收，既然每个请求都是独立的，所以在每个用户去访问我们的工程时，我们可以使用 threadlocal 来做到线程隔离，每个线程操作自己的一份数据\n\n**温馨小贴士：关于 threadlocal**\n\n如果小伙伴们看过 threadLocal 的源码，你会发现在 threadLocal 中，无论是他的 put 方法和他的 get 方法， 都是先从获得当前用户的线程，然后从线程中取出线程的成员变量 map，只要线程不一样，map 就不一样，所以可以通过这种方式来做到线程隔离\n\n![拦截器处理](redis-in-action-SMS-login/拦截器处理.png)\n\n拦截器代码：\n\n```\npublic class LoginInterceptor implements HandlerInterceptor {\n \n    @Override\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {\n       //1.获取session\n        HttpSession session = request.getSession();\n        //2.获取session中的用户\n        Object user = session.getAttribute(\"user\");\n        //3.判断用户是否存在\n        if(user == null){\n              //4.不存在，拦截，返回401状态码\n              response.setStatus(401);\n              return false;\n        }\n        //5.存在，保存用户信息到Threadlocal\n        UserHolder.saveUser((User)user);\n        //6.放行\n        return true;\n    }\n}\n```\n\n使拦截器生效：\n\n```\n@Configuration\npublic class MvcConfig implements WebMvcConfigurer {\n \n    @Resource\n    private StringRedisTemplate stringRedisTemplate;\n \n    @Override\n    public void addInterceptors(InterceptorRegistry registry) {\n        // 登录拦截器\n        registry.addInterceptor(new LoginInterceptor())\n                .excludePathPatterns(\n                        \"/shop/**\",\n                        \"/voucher/**\",\n                        \"/shop-type/**\",\n                        \"/upload/**\",\n                        \"/blog/hot\",\n                        \"/user/code\",\n                        \"/user/login\"\n                ).order(1);\n        // token刷新的拦截器\n        registry.addInterceptor(new RefreshTokenInterceptor(stringRedisTemplate)).order(0);\n    }\n}\n```\n\n## 隐藏用户敏感信息\n\n我们通过浏览器观察到此时用户的全部信息都在，这样极为不靠谱，所以我们应当在返回用户信息之前，将用户的敏感信息进行隐藏，采用的核心思路就是书写一个 UserDto 对象，这个 UserDto 对象就没有敏感信息了，我们在返回前，将有用户敏感信息的 User 对象转化成没有敏感信息的 UserDto 对象，那么就能够避免这个尴尬的问题了\n\n```\n//7.保存用户信息到session中\nsession.setAttribute(\"user\", BeanUtils.copyProperties(user,UserDTO.class));\n```\n\n**在拦截器处：**\n\n```\n//5.存在，保存用户信息到Threadlocal\nUserHolder.saveUser((UserDTO) user);\n```\n\n**在 UserHolder 处：将 user 对象换成 UserDTO**\n\n```\n\tpublic class UserHolder {\n    private static final ThreadLocal<UserDTO> tl = new ThreadLocal<>();\n \n    public static void saveUser(UserDTO user){\n        tl.set(user);\n    }\n \n    public static UserDTO getUser(){\n        return tl.get();\n    }\n \n    public static void removeUser(){\n        tl.remove();\n    }\n}\n```\n\n## session 共享问题\n\n**核心思路分析：**\n\n每个 tomcat 中都有一份属于自己的 session, 假设用户第一次访问第一台 tomcat，并且把自己的信息存放到第一台服务器的 session 中，但是第二次这个用户访问到了第二台 tomcat，那么在第二台服务器上，肯定没有第一台服务器存放的 session，所以此时 整个登录拦截功能就会出现问题，我们能如何解决这个问题呢？早期的方案是 session 拷贝，就是说虽然每个 tomcat 上都有不同的 session，但是每当任意一台服务器的 session 修改时，都会同步给其他的 Tomcat 服务器的 session，这样的话，就可以实现 session 的共享了\n\n但是这种方案具有两个大问题\n\n1、每台服务器中都有完整的一份 session 数据，服务器压力过大。\n\n2、session 拷贝数据时，可能会出现延迟\n\n所以咱们后来采用的方案都是基于 redis 来完成，我们把 session 换成 redis，redis 数据本身就是共享的，就可以避免 session 共享的问题了\n\n![session共享](redis-in-action-SMS-login/session共享.png)\n\n## Redis 代替 session 的业务流程\n\n### 设计 key 的结构\n\n首先我们要思考一下利用 redis 来存储数据，那么到底使用哪种结构呢？由于存入的数据比较简单，我们可以考虑使用 String，或者是使用哈希，如下图，如果使用 String，同学们注意他的 value，用多占用一点空间，如果使用哈希，则他的 value 中只会存储他数据本身，如果不是特别在意内存，其实使用 String 就可以啦。\n\n![redis设计key](redis-in-action-SMS-login/redis设计key.png)\n\n### 设计 key 的具体细节\n\n所以我们可以使用 String 结构，就是一个简单的 key，value 键值对的方式，但是关于 key 的处理，session 他是每个用户都有自己的 session，但是 redis 的 key 是共享的，咱们就不能使用 code 了\n\n在设计这个 key 的时候，我们之前讲过需要满足两点\n\n1、key 要具有唯一性\n\n2、key 要方便携带\n\n如果我们采用 phone：手机号这个的数据来存储当然是可以的，但是如果把这样的敏感数据存储到 redis 中并且从页面中带过来毕竟不太合适，所以我们在后台生成一个随机串 token，然后让前端带来这个 token 就能完成我们的整体逻辑了\n\n### 整体访问流程\n\n当注册完成后，用户去登录会去校验用户提交的手机号和验证码，是否一致，如果一致，则根据手机号查询用户信息，不存在则新建，最后将用户数据保存到 redis，并且生成 token 作为 redis 的 key，当我们校验用户是否登录时，会去携带着 token 进行访问，从 redis 中取出 token 对应的 value，判断是否存在这个数据，如果没有则拦截，如果存在则将其保存到 threadLocal 中，并且放行。\n\n![共享session登录](redis-in-action-SMS-login/共享session登录.png)\n\n## 基于 Redis 实现短信登录\n\n### 设计 key 的结构\n\n首先我们要思考一下利用 redis 来存储数据，那么到底使用哪种结构呢？由于存入的数据比较简单，我们可以考虑使用 String，或者是使用哈希，如下图，如果使用 String，同学们注意他的 value，用多占用一点空间，如果使用哈希，则他的 value 中只会存储他数据本身，如果不是特别在意内存，其实使用 String 就可以啦。\n\n![使用string结构保存json字符串](redis-in-action-SMS-login/使用string结构保存json字符串.png)\n\n### 设计 key 的具体细节\n\n所以我们可以使用 String 结构，就是一个简单的 key，value 键值对的方式，但是关于 key 的处理，session 他是每个用户都有自己的 session，但是 redis 的 key 是共享的，咱们就不能使用 code 了\n\n在设计这个 key 的时候，我们之前讲过需要满足两点\n\n1、key 要具有唯一性\n\n2、key 要方便携带\n\n如果我们采用 phone：手机号这个的数据来存储当然是可以的，但是如果把这样的敏感数据存储到 redis 中并且从页面中带过来毕竟不太合适，所以我们在后台生成一个随机串 token，然后让前端带来这个 token 就能完成我们的整体逻辑了\n\n### 整体访问流程\n\n当注册完成后，用户去登录会去校验用户提交的手机号和验证码，是否一致，如果一致，则根据手机号查询用户信息，不存在则新建，最后将用户数据保存到 redis，并且生成 token 作为 redis 的 key，当我们校验用户是否登录时，会去携带着 token 进行访问，从 redis 中取出 token 对应的 value，判断是否存在这个数据，如果没有则拦截，如果存在则将其保存到 threadLocal 中，并且放行。\n\n![1653319474181](redis-in-action-SMS-login/1653319474181.png)\n\n## 基于 Redis 实现短信登录\n\n这里具体逻辑就不分析了，之前咱们已经重点分析过这个逻辑啦。\n\n```java\n@Override\npublic Result login(LoginFormDTO loginForm, HttpSession session) {\n    // 1.校验手机号\n    String phone = loginForm.getPhone();\n    if (RegexUtils.isPhoneInvalid(phone)) {\n        // 2.如果不符合，返回错误信息\n        return Result.fail(\"手机号格式错误！\");\n    }\n    // 3.从redis获取验证码并校验\n    String cacheCode = stringRedisTemplate.opsForValue().get(LOGIN_CODE_KEY + phone);\n    String code = loginForm.getCode();\n    if (cacheCode == null || !cacheCode.equals(code)) {\n        // 不一致，报错\n        return Result.fail(\"验证码错误\");\n    }\n \n    // 4.一致，根据手机号查询用户 select * from tb_user where phone = ?\n    User user = query().eq(\"phone\", phone).one();\n \n    // 5.判断用户是否存在\n    if (user == null) {\n        // 6.不存在，创建新用户并保存\n        user = createUserWithPhone(phone);\n    }\n \n    // 7.保存用户信息到 redis中\n    // 7.1.随机生成token，作为登录令牌\n    String token = UUID.randomUUID().toString(true);\n    // 7.2.将User对象转为HashMap存储\n    UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class);\n    Map<String, Object> userMap = BeanUtil.beanToMap(userDTO, new HashMap<>(),\n            CopyOptions.create()\n                    .setIgnoreNullValue(true)\n                    .setFieldValueEditor((fieldName, fieldValue) -> fieldValue.toString()));\n    // 7.3.存储\n    String tokenKey = LOGIN_USER_KEY + token;\n    stringRedisTemplate.opsForHash().putAll(tokenKey, userMap);\n    // 7.4.设置token有效期\n    stringRedisTemplate.expire(tokenKey, LOGIN_USER_TTL, TimeUnit.MINUTES);\n \n    // 8.返回token\n    return Result.ok(token);\n}\n```\n\n","tags":["redis"],"categories":["redis"]},{"title":"爬虫初级使用记录","url":"/2023/03/20/introduction-to-spider/","content":"\n本文记录了爬虫最基础的使用方法，如果只想最快获得网页源码从2.2浏览即可\n\n<!--more-->\n\n## 1、爬虫核心库1：requests库\n\n学习爬虫其实并不太需要了解太多的网页结构知识，作为初学者只需要知道1点：所有想要获取的内容（例如新闻标题/网址/日期/来源等）都在网页源代码里，所谓网页源代码，就是网页背后的编程代码，这一小节我们首先来讲解下如何查看网页源代码，以及通过两个案例快速体验下如何通过requests库获取网页源代码。\n\n### 1.1 如何查看网页源代码\n\n在进入正式爬虫实战前，我们首先来了解下如何查看网页源代码。\n\n网络爬虫首先得有一个浏览器，这里强烈推荐谷歌浏览器（百度搜索谷歌浏览器，然后在官网[https://www.google.cn/chrome/](https://www.google.cn/chrome/)下载，谷歌浏览器默认是谷歌搜索，直接在网址输入框里输入内容可能搜索不到内容，可以在网址栏上输入baidu.com进行访问，或者可以点击浏览器右侧的设置按钮->选择界面左侧的搜索引擎->选择百度搜索引擎）。当然用别的浏览器，比如火狐浏览器等都是可以的，只要它按F12(有的电脑要同时按住左下角的Fn键)能弹出网页源代码即可。\n\n以谷歌浏览器为例来演示下F12的强大作用，百度搜索“阿里巴巴”，然后**按一下F12（有的电脑还得同时按住Fn）**，弹出如下页面，其中点击右侧设置按钮可以切换布局样式。\n\n![image-20230320154143682](introduction-to-spider/image-20230320154143682.png)\n\n这个按住F12弹出来的东西叫做开发者工具，是进行数据挖掘的利器，对于爬虫来说，大多数情况下只需要会用下图的这两个按钮即可。\n\n![img](introduction-to-spider/v2-e64893143d82ba0f5e79621345dea1cf_r.jpg)\n\n第一个按钮箭头形状按钮为**选择**按钮，第二个Elements按钮为**元素**按钮。\n\n**(1)** **选择按钮**\n\n点击一下它，发现它会变成蓝色，然后把鼠标在页面上移动移动，会发现页面上的颜色机会发生改变。如下图所示，当**移动鼠标**的时候，会发现界面上的颜色会发生变化，并且Elements里的内容就会随之发生变化。\n\n![image-20230320154833691](introduction-to-spider/image-20230320154833691.png)\n\n下面当选择按钮处于蓝色状态的时，点击一下第一个链接的标题，这时候选择按钮再次变成灰色，而Elements里的内容也将不再变动，此时便可以观察具体的网页源代码内容了，如下图所示，我们一般只关心里面的所需要的中文内容，如果没有看到中文文本，店家下图所示的三角箭头，即可展开内容，看到中文文本。\n\n![image-20230320155030944](introduction-to-spider/image-20230320155030944.png)\n\n**(2) Elements元素按钮**：\n\nElements元素按钮里面的内容可以理解为**就是网站的源码**，最后爬虫爬到的内容大致就是长这个样子的。下面就要接着完成一些“神奇”的操作。\n\n在下图“**1688”**那个地方鼠标双击俩下，这两个字变成可编辑的格式。\n\n![image-20230320155132398](introduction-to-spider/image-20230320155132398.png)\n\n将其改成“测试”，可以看到第一个的标题发生了改变，如下图所示：\n\n![image-20230320155245952](introduction-to-spider/image-20230320155245952.png)\n\n还可以用同样的操作，修改页面上的其他信息，如股价等。\n\n通过F12启动开发者工具，我们可以对网页结构有一个初步的认识，并可以利用选择按钮和“Elements”元素按钮观察我们想获取的内容在源码中的文本格式以及所在位置。\n\n**补充知识点1：查看网页源码的另一个方式**\n\n除了F12，另一个获取网页源码的方式是在网页上右击选择“**查看网页源代码**”，就可以获取这个网址的源代码，这个基本就是Python爬取到的最终信息。用鼠标上下滚动，就能看到很多内容，同样初学者不需要关心那些英文或者网页框架，只需要知道想获取的中文在哪里即可。\n\n这个方法比F12观察源码的方式更加真实，因为F12观察到的源码可能是被网页修饰过的，通过Python获取到内容可能和F12看到的不一致。通过该方法查看到的源码就是通过Python程序能够获取到的网页源代码。实战中常将两种方法联合使用：通过F12进行初步了解，然后右击查看网页源代码，看看所需内容到底在网页源代码的什么位置，其中可以通过Ctrl + F快捷键搜索所需要的内容。\n\n此外，如果F12看到的内容和通过右击查看网页源代码看到的内容很不一样，这是因为网站做了动态渲染的处理（这是一种反爬处理），这时候就需要用到2.2节selenium库的相关知识点来获取真正的网页源代码。\n\n**补充知识点2：http与https协议**\n\n有的时候我们理解的网址是：[http://www.baidu.com]([http://www.baidu.com)，但其实在编程里或者它真实的名字其实是：[https://www.baidu.com](https://www.baidu.com)，它前面有个“https://”这个叫做https协议，是网址的固定构成形式，某种程度表明这个网址是安全的，有的网址前面则为http://。如果在Python里输入[www.baidu.com](https://link.zhihu.com/?target=http%3A//www.baidu.com/)它是不认识的，得把“https://”加上才行，如下面所示。\n\n```text\nurl = 'https://www.baidu.com/'\n```\n\n其实最简单的办法，**就是直接浏览器访问该网址，然后把链接复制下来就行**。\n\n### 1.2 爬虫初尝试 - requests库获取网页源代码\n\n了解了如何查看网页源代码后，这一小节我们讲解下如何通过requests库爬取网页源代码。这里以一个学校的招聘网站为例。\n\n**（1） 获取网页源代码**\n\n通过第一章最后介绍的requests库来尝试获取下新闻的网页源代码，代码如下：\n\n```python\nimport requests\nurl = 'https://www.163.com/dy/article/I09JUB0P051984TV.html'\nres = requests.get(url).text\nprint(res)\n```\n\n运行后报错：\n\n`requests.exceptions.SSLError: HTTPSConnectionPool`\n\n![image-20230320160811293](introduction-to-spider/image-20230320160811293.png)\n\n这是由于ssl认证失败造成的，我们并不需要知道原因，只要像以下一样禁用ssl认证就可以了。\n\n```python\nimport requests\n\ns = requests.session()\ns.trust_env = False\ns.keep_alive = False\nrequests.DEFAULT_RETRIES = 50\n\nurl = 'https://www.gdpt.edu.cn/al_8/189'\nres = s.get(url).text\nprint(res)\n```\n\n这段代码使requests的连接禁用了ssl认证，不使用keep-alive，并将重连次数增加到了50，如果出现了类似的错误只需要将以上代码复制粘贴即可。运行后得到以下结果。\n\n![image-20230320161701948](introduction-to-spider/image-20230320161701948.png)\n\n这里虽然得到了结果，但是有的网站只认可浏览器发送过去的访问，而不认可直接通过Python发送过去的访问请求，那么该如何解决该问题呢？这时就需要设置下requests.get()中的headers参数，用来模拟浏览器进行访问。\n\n```python\nimport requests\n\ns = requests.session()\ns.trust_env = False\ns.keep_alive = False\nrequests.DEFAULT_RETRIES = 50\n\nheaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 \\\n                    (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1'}\nurl = 'https://www.gdpt.edu.cn/al_8/189'\nres = s.get(url, headers=headers).text\nprint(res)\n```\n\n## 2 爬虫核心库2：selenium库\n\n对比了我想要查询的内容，发现requests爬取的信息不是完整的，这是因为Requests只能获取到HTML文本，而无法获取到JavaScript动态生成的内容。如果div盒子里的内容是通过JavaScript生成的，那么requests就无法获取到。此时就需要使用selenium库了。\n\nSelenium库这一知识点相对比较重要，如果说requests库能够爬取50%的网站的话，那么通过selenium库的话可以爬取95%的网站，大部分较为困难的网址都可以通过其来获取网页源代码。下面我们首先来分析下requests库在一些复杂爬虫中遇到的难点，然后讲解下如何安装selenium库以及如何通过selenium库获取到网页源代码。\n\n### 2.1 requests库遇到的难点\n\n在使用requests库进行爬虫实战时，有时会遇到一大难题：获取不了网页真正的源代码。例如，上海证券交易所的公开信息、新浪财经的股票行情实时数据等，用常规爬虫手段会发现获取到的网页源代码内容很少且没有用。因为这些网页上展示的信息是动态渲染出来的，而通过常规爬虫手段获取的则是未经渲染的信息，所以其中没有我们想要的信息。\n\n以新浪财经的上证综合指数（上证综合指数反映在上海证券交易所全部上市股票价格综合情况）网页（[http://finance.sina.com.cn/realstock/]([http://finance.sina.com.cn/realstock)）为例，在浏览器中按F12 键，可以在网页源代码中看到指数数值。然后用常规爬虫手段，以requests.get(url).text 的方式获取这个网页的源代码，然后按快捷键Ctrl+F，在源代码中搜索刚才看到的指数数值，会发现搜索不到，如下图所示。而且就算加上headers 参数也没有改观。\n\n面对这种动态渲染的网页，在数据挖掘时就需要使用Selenium 库，通过模拟打开一个浏览器访问网址，然后获取渲染后的网页源代码，从而完成requests库难以完成的任务。\n\n| 优点       | 缺点                |                  |\n| ---------- | ------------------- | ---------------- |\n| requests库 | 爬取速度快          | 有些网站爬取不到 |\n| selenium库 | 能爬取95%以上的网站 | 爬取速度较慢     |\n\n### 2.2  Selenium库介绍与安装\n\n正式介绍selenium库之前，得首先先安装一个网页模拟器：ChromeDriver，它的作用是给Pyhton提供一个模拟浏览器，让Python能够运行一个模拟的浏览器进行网页访问，并用selenium进行鼠标及键盘等操作获取到网页真正的源代码。\n\n**(1) 安装Chrome谷歌浏览器**\n\n安装ChromeDriver之前，得先装一下Chrome谷歌浏览器，直接百度搜索谷歌浏览器，然后在官网[https://www.google.cn/chrome/](https://www.google.cn/chrome)下载即可。\n\n**(2) 查看Chrome浏览器版本**\n\n地址栏输入`chrome://version`回车即可\n\n![image-20230320163352932](introduction-to-spider/image-20230320163352932.png)\n\n![image-20230320163501421](introduction-to-spider/image-20230320163501421.png)\n\n**(3) ChromeDriver下载**\n\nChromeDriver官方下载地址：[http://chromedriver.storage.googleapis.com/index.html](http://chromedriver.storage.googleapis.com/index.html)。进入官网后，选择对应自己谷歌浏览器版本的ChromeDriver下载即可。不过由于官网再国内经常访问不了，因此可以在百度上搜索“ChromeDriver下载”，可以找到如下一个镜像下载网站：[http://npm.taobao.org/mirrors/chromedriver/](https://registry.npmmirror.com/binary.html?path=chromedriver/)。\n\n**(4) ChromeDriver环境变量配置**\n\n解压压缩包，找到chromedriver.exe复制到chrome的安装目录（其实也可以随便放一个文件夹,关键是要加入环境变量）。复制chromedriver.exe文件的路径并加入到电脑的环境变量中去。具体的：\n\n![img](introduction-to-spider/1365470-20190316154653549-1071713064.png)进入环境变量编辑界面，添加到用户变量即可，双击PATH，将你的文件位置（C:\\Program Files (x86)\\Google\\Chrome\\Application\\)添加到后面。\n\n![img](introduction-to-spider/1365470-20190316155217109-176711956.png)\n\n完成后在按住win+R键进入cmd，输入chromedriver验证是否安装成功：\n\n![image-20230320165251568](introduction-to-spider/image-20230320165251568.png)\n\n未配置环境也可以，例如：\n\n```python\nfrom selenium import webdriver\nimport time\n\ndef main():\n    chrome_driver = 'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe'  #chromedriver的文件位置\n    b = webdriver.Chrome(executable_path = chrome_driver)\n    b.get('https://www.google.com')\n    time.sleep(5)\n    b.quit()\n\nif __name__ == '__main__':\n    main()\n```\n\n已配置环境变量时，就不需要指定位置了\n\n```python\nfrom selenium import webdriver\nimport time\n\ndef main():\n    b = webdriver.Chrome()\n    b.get('https://www.baidu.com')\n    time.sleep(5)\n    b.quit()\n\nif __name__ == '__main__':\n    main()\n```\n\n如果运行时提示\n\n![img](introduction-to-spider/1365470-20190316162023053-275348276.png)\n\n很可能是chromedriver的版本不对（不要问我怎么知道的）。\n\n### **2.3 Selenium库获取网页源代码** \n\nSelenium库的功能很强大，使用技巧却并不复杂，只要掌握了下面的几个知识点，就能较游刃有余的使用selenium库了。\n\n**(1) 访问及关闭页面 + 网页最大化**\n\n通过以下这三行代码，就可以访问网站了，它相当于模拟人打开了一个浏览器，然后输入了一串网址：\n\n```text\nfrom selenium import webdriver\nbrowser = webdriver.Chrome()\nbrowser.get(\"https://www.baidu.com/\")\n```\n\n第一行引入selenium库里的webdriver功能，第二行browser = webdriver.Chrome()声明我们用的模拟器是谷歌浏览器，第三行通过brower.get()的这个方法访问网址。\n\n关闭模拟浏览器的代码如下，在代码最后加上这么一行，能关闭模拟浏览器。\n\n```text\nbrowser.quit()\n```\n\n**(2) 获取网页真正的源代码**\n\n利用selenium的一个主要目的就是为了获取原来难以获得的网页源码，代码如下：\n\n```text\ndata = browser.page_source\n```\n\n拿这个方法来试试之前提到过的比较难以获取的动态加载的内容：\n\n```python\nfrom selenium import webdriver\nfrom selenium.webdriver import ChromeOptions\n\nurl = \"https://www.gdpt.edu.cn/al_8/189\"\n\noptions = ChromeOptions()\nbrowser = webdriver.Chrome(options=options)\nbrowser.get(url)\n\nsource = browser.page_source\n\nwith open('example.html', 'w', encoding=\"utf8\") as f:\n    f.write(source)\nbrowser.quit()\n```\n\n这样就得到了所有的内容并以一个html文件的形式写出\n\n### 2.4 selenium库加载登录信息\n\n如果一个网站要求登录才能看到信息，如何使用selenium登录呢。很简单，加载浏览器登陆过的信息即可。浏览器会将用户登录过的所有数据保存在`C:\\Users\\电脑用户名\\AppData\\Local\\Google\\Chrome\\User Data`，selenium只需要加载这个文件夹就可以了，代码如下：\n\n```python\nfrom selenium import webdriver\nfrom selenium.webdriver import ChromeOptions\n\nurl = \"https://www.gdpt.edu.cn/al_8/189\"\n\n# 加载cookies中已经保存的账号和密码\noptions = ChromeOptions()\noptions.add_argument(r'user-data-dir=C:\\Users\\电脑用户名\\AppData\\Local\\Google\\Chrome\\User Data')\nbrowser = webdriver.Chrome(options=options)\nbrowser.get(url)\n\nsource = browser.page_source\n\nwith open('example.html', 'w', encoding=\"utf8\") as f:\n    f.write(source)\nbrowser.quit()\n```\n\n","tags":["Python","spider"],"categories":["Python","spider"]},{"title":"how-to-fix-This-application-has-no-explicit-mapping-error","url":"/2023/03/17/how-to-fix-This-application-has-no-explicit-mapping-error/","content":"\n报错完整信息:This application has no explicit mapping for /error, so you are seeing this as a fallback\n\n<!--more-->\n\n翻阅了网上众多资料，主要有一下几种解决的方向：\n\n## 方向1:包的位置可能错误\n\nApplication启动类的位置不对.要将Application类放在最外侧,即包含所有子包\n\n原因:spring-boot会自动加载启动类所在包下及其子包下的所有组件.\n\n## 方向2: springboot配置文件有误\n\n在springboot的配置文件:application.yml或application.properties中关于视图解析器的配置问题:\n\n当pom文件下的spring-boot-starter-paren版本高时使用:\n\nspring.mvc.view.prefix/spring.mvc.view.suffix\n\n当pom文件下的spring-boot-starter-paren版本低时使用:\n\nspring.view.prefix/spring.view.suffix\n\n## 方向3: 映射路径有误\n\n控制器的URL路径书写问题\n\n@RequestMapping(“xxxxxxxxxxxxxx”)\n\n实际访问的路径与”xxx”不符合.\n\n然而作为刚接触SpringBoot的我一开始犯错的时候是在第四层\n\n## 方向4: 扫包出现错误\n\n你把你Controller类的@Controller给补上\n\n## 方向5: nginx是否打开并检查端口\n\n当使用nginx代理静态资源时，需要检查是否运行nginx，并注意端口不能冲突\n\n","tags":["debug","Spring Boot"],"categories":["debug","Spring Boot"]},{"title":"LeetCode题解：括号匹配算法","url":"/2023/01/21/LeetCode题解：括号匹配算法/","content":"\n## 括号匹配算法\n\n左*括号*必须用相同类型的右*括号*闭合。 左*括号*必须以正确的顺序闭合。 注意空字符串可被认为是有效字符串。\n\n<!--more-->\n\n### 1.实现目标\n\n在开发中，会出现需要判断字符串是否匹配的问题，如文本编辑器中括号不匹配会出现格式错误（如以下字符串），这就需要括号匹配算法\n\n```\ndsa(dsadsa{dhk)s})}\n```\n\n### 2.实现思路\n\n![$R5G0SIK](LeetCode题解：括号匹配算法/$R5G0SIK.jpg)\n\n由于括号是与最近的同类型括号匹配，可以利用栈的后进先出特性将右括号与最近的左括号匹配，如果不匹配，直接返回false\n\n### 3.具体实现\n\n当括号数量为奇数时直接返回false，为0直接返回true。\n\n核心逻辑：循环遍历字符串每一个字符，判断是左括号则入栈，num++，若是右括号则num--，让该括号与栈顶括号匹配，若相同则弹出栈，不同则什么都不做，这样就可以跳过普通字符而判断括号是否匹配\n\n为什么要设置num：设置变量num统计左括号的数目，当有右括号时num--，这是为了判断左右括号的数目要相同，但是还要判断是否为同类括号\n\n改进方法：判断isMatch（）时在后面加else，就不用判断num了，但是使用原方法leetcode速度更快，内存也更小\n\n代码\n\n```javascript\n<script>\t\n    let isValid = funtion(str){\n        const len = str.length\n        if(len%2===1)\n            return false\n        if(len===0)\n            return true\n        \n        str = str.split('')\n        let stack = []\n        const leftBracket = '{[('\n        cosnt rightBracket = '}])'\n        let a = str[0]\n        if(rightBracket.includes(a))\n            return false\n        \n        for(let i = 0;i<len;i++)\n            if(leftBracket.includes(str[i]))\n                stack.push(str[i])\n        \t\tnum++\n        \telse if(rightBracket.includes(str[i]))//\n                num--\n                let top = stack[stack.length-1]\n                if(isMatch(top,str[i]))\n                    stack.pop()\n        \t\t/*\n        \t\telse\n        \t\t\treturn false\t\t此时就不用num了\n        \t\t*/\n        if(num===0&&stack.length===0)\n            return true\n        else \n            return false\n        \n        \n    }\n\n\n\tfuntion isMatch(left,right){\n        if(left==='{'&&right==='}'){\n            return true\n        }else if(left==='['&&right===']'){\n            return true\n        }else if(left==='('&&right===')'){\n            return true\n        }else\n            return false\n    \n</script>\n```\n\n","tags":["Leetcode"],"categories":["Leetcode"]},{"title":"每天一个Linux命令：top","url":"/2022/07/20/one-linux-command-per-day/","content":"\n每天坚持学习Linux\n\n<!--more-->\n\ntop命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。下面详细介绍它的使用方法。top是一个动态显示过程,即可以通过用户按键来不断刷新当前状态.如果在前台执行该命令,它将独占前台,直到用户终止该程序为止.比较准确的说,top命令提供了实时的对系统处理器的状态监视.它将显示系统中CPU最“敏感”的任务列表.该命令可以按CPU使用.内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定\n\n**1．**命令格式：\n\ntop [参数]\n\n**2．** 命令功能\n\n显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等\n\n3．命令参数：\n\n-b 批处理\n\n-c 显示完整的治命令\n\n-I 忽略失效过程\n\n-s 保密模式\n\n-S 累积模式\n\n-i<时间> 设置间隔时间\n\n-u<用户名> 指定用户名\n\n-p<进程号> 指定进程\n\n-n<次数> 循环显示的次数\n\n**统计信息区**：\n\n前五行是当前系统情况整体的统计信息区。下面我们看每一行信息的具体意义。\n\n**第一行**，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下：\n\n14:06:23 — 当前系统时间\n\nup 70 days, 16:44 — 系统已经运行了70天16小时44分钟（\n\n2 users — 当前有2个用户登录系统\n\nload average: 1.15, 1.42, 1.44 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。\n\nload average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。\n\n**第二行**，Tasks — 任务（进程），具体信息说明如下：\n\n系统现在共有206个进程，其中处于运行中的有1个，205个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。\n\n**第三行**，cpu状态 信息，具体属性说明如下：\n\n5.9%us — 用户空间占用CPU的百分比。\n\n3.4% sy — 内核空间占用CPU的百分比。\n\n0.0% ni — 改变过优先级的进程占用CPU的百分比\n\n90.4% id — 空闲CPU百分比\n\n0.0% wa — IO等待占用CPU的百分比\n\n0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比\n\n0.2% si — 软中断（Software Interrupts）占用CPU的百分比\n\n**备注：**在这里CPU的使用比率和windows概念不同，需要理解linux系统用户空间和内核空间的相关知识！\n\n**第四行**,内存状态，具体信息如下：\n\n32949016k total — 物理内存总量（32GB）\n\n14411180k used — 使用中的内存总量（14GB）\n\n18537836k free — 空闲内存总量（18GB）\n\n169884k buffers — 缓存的内存量 （169M）\n\n**第五行，**swap交换分区信息，具体信息说明如下：\n\n32764556k total — 交换区总量（32GB）\n\n0k used — 使用的交换区总量（0K）\n\n32764556k free — 空闲交换区总量（32GB）\n\n3612636k cached — 缓冲的交换区总量（3.6GB）\n\n备注：第四行中使用中的内存总量（used）指的是现在系统内核控制的内存数，空闲内存总量（free）是内核还未纳入其管控范围的数量。纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到free中去，因此在linux上free内存会越来越少，但不用为此担心。\n\n如果出于习惯去计算可用内存数，这里有个近似的计算公式：第四行的free + 第四行的buffers + 第五行的cached\n\n对于内存监控，在top里我们要时刻监控第五行swap交换分区的used，如果这个数值在不断的变化，说明内核在不断进行内存和swap的数据交换，这是真正的内存不够用了\n\n**其他使用技巧：**\n\n**1.** **多U多核CPU监控**\n\n在top基本视图中，按键盘数字“1”，可监控每个逻辑CPU的状况：\n\n​\t![img](one-linux-command-per-day/top1.jpg) \n\n观察上图，服务器有16个逻辑CPU，实际上是4个物理CPU。再按数字键1，就会返回到top基本视图界面。\n\n**2.****高亮显示当前运行进程**\n\n​\t敲击键盘“b”（打开/关闭加亮效果），top的视图变化如下：\n\n​\t  ![img](one-linux-command-per-day/top2.png) \n\n我们发现进程id为2570的“top”进程被加亮了，top进程就是视图第二行显示的唯一的运行态（runing）的那个进程，可以通过敲击“y”键关闭或打开运行态进程的加亮效果。\n\n**3.****进程字段排序**\n\n默认进入top时，各进程是按照CPU的占用量来排序的,敲击键盘“x”（打开/关闭排序列的加亮效果），top默认的排序列是“%CPU”。\n\n4.通过”shift + >”或”shift + <”可以向右或左改变排序列\n\n**实例2：****显示 完整命令**\n\ntop -c\n\n**实例3：以批处理模式显示程序信息**\n\ntop -b\n\n**实例4：****以累积模式显示程序信息**\n\ntop -S\n\n**实例5：****设置信息更新次数**\n\n top -n 2\n\n表示更新两次后终止更新显示\n\n**实例6：设置信息更新时间**\n\ntop -d 3\n\n表示更新周期为3秒\n\n**实例7：显示指定的进程信息**\n\ntop -p 574\n\n **5.top交互命令**\n\n在top 命令执行过程中可以使用的一些交互命令。这些命令都是单字母的，如果在命令行中使用了s 选项， 其中一些命令可能会被屏蔽。\n\nh 显示帮助画面，给出一些简短的命令总结说明\n\nk 终止一个进程。\n\ni 忽略闲置和僵死进程。这是一个开关式命令。\n\nq 退出程序\n\nr 重新安排一个进程的优先级别\n\nS 切换到累计模式\n\ns 改变两次刷新之间的延迟时间（单位为s），如果有小数，就换算成m s。输入0值则系统将不断刷新，默认值是5 s\n\nf或者F 从当前显示中添加或者删除项目\n\no或者O 改变显示项目的顺序\n\nl 切换显示平均负载和启动时间信息\n\nm 切换显示内存信息\n\nt 切换显示进程和CPU状态信息\n\nc 切换显示命令名称和完整命令行\n\nM 根据驻留内存大小进行排序\n\nP 根据CPU使用百分比大小进行排序\n\nT 根据时间/累计时间进行排序\n","tags":["Linux"],"categories":["Linux"]}]